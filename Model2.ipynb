{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flask_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow keras-tuner numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow keras scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ mood_keywords.json generated with 30 moods.\n",
      "‚úÖ mood_keywords.json updated and saved with 30 mood labels.\n"
     ]
    }
   ],
   "source": [
    "#Moods code\n",
    "import json\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load short mood dataset\n",
    "with open(\"short_mood_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Normalize and group contexts by mood\n",
    "mood_to_contexts = defaultdict(list)\n",
    "for sample in data:\n",
    "    mood = sample.get(\"Mood\", \"\").strip().title()\n",
    "    context = sample.get(\"Context\", \"\").strip().lower()\n",
    "    if mood and context:\n",
    "        mood_to_contexts[mood].append(context)\n",
    "\n",
    "# Extract keywords per mood\n",
    "mood_keywords = {}\n",
    "for mood, contexts in mood_to_contexts.items():\n",
    "    if len(contexts) < 2:\n",
    "        continue  # skip rare moods\n",
    "    all_text = \" \".join(contexts)\n",
    "    tokens = word_tokenize(all_text)\n",
    "    cleaned = [w for w in tokens if w.isalpha() and w.lower() not in stop_words]\n",
    "    top_keywords = [word.lower() for word, _ in Counter(cleaned).most_common(15)]\n",
    "    if top_keywords:\n",
    "        mood_keywords[mood] = top_keywords\n",
    "\n",
    "# Save result\n",
    "with open(\"mood_keywords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mood_keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ mood_keywords.json generated with {len(mood_keywords)} moods.\")\n",
    "\n",
    "\n",
    "# Load the old mood_keywords.json (with Mood_0, Mood_1... keys)\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "# Define mapping from generic Mood_# labels to real mood names\n",
    "mood_map = {\n",
    "    \"Mood_0\": \"Abuse\",\n",
    "    \"Mood_1\": \"Sexual Issues\",\n",
    "    \"Mood_2\": \"Erectile Dysfunction\",\n",
    "    \"Mood_3\": \"Relationship Issues\",\n",
    "    \"Mood_4\": \"Depression\",\n",
    "    \"Mood_5\": \"Child Custody\",\n",
    "    \"Mood_6\": \"Parental Issues\",\n",
    "    \"Mood_7\": \"Infidelity\",\n",
    "    \"Mood_8\": \"Marriage Problems\",\n",
    "    \"Mood_9\": \"Motherhood\",\n",
    "    \"Mood_10\": \"Gender Identity\",\n",
    "    \"Mood_11\": \"Therapist Issues\",\n",
    "    \"Mood_12\": \"Therapy Anxiety\",\n",
    "    \"Mood_13\": \"Marital Issues\",\n",
    "    \"Mood_14\": \"Unrequited Love\",\n",
    "    \"Mood_15\": \"Therapy Nervousness\",\n",
    "    \"Mood_16\": \"Health Trauma\",\n",
    "    \"Mood_17\": \"PTSD\",\n",
    "    \"Mood_18\": \"Alcoholism\",\n",
    "    \"Mood_19\": \"Breakups\",\n",
    "    \"Mood_20\": \"Sexless Marriage\",\n",
    "    \"Mood_21\": \"Hearing Voices\",\n",
    "    \"Mood_22\": \"Eating Disorders\",\n",
    "    \"Mood_23\": \"Suicidal Thoughts\",\n",
    "    \"Mood_24\": \"Panic Attacks\",\n",
    "    \"Mood_25\": \"Toxic Relationships\",\n",
    "    \"Mood_26\": \"Anxiety And Depression\",\n",
    "    \"Mood_27\": \"Daughter Stress\",\n",
    "    \"Mood_28\": \"Work And Family Stress\",\n",
    "    \"Mood_29\": \"School Stress\"\n",
    "}\n",
    "\n",
    "# Create a new dict with renamed keys\n",
    "renamed_keywords = {}\n",
    "for mood_id, keywords in mood_keywords.items():\n",
    "    readable_label = mood_map.get(mood_id, mood_id)  # fallback to original key if no match\n",
    "    renamed_keywords[readable_label] = keywords\n",
    "\n",
    "# Save the updated file\n",
    "with open(\"mood_keywords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(renamed_keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ mood_keywords.json updated and saved with {len(renamed_keywords)} mood labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ mood_keywords_grouped_enhanced.json created with 4 main categories, including additional keywords.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download necessary NLTK data (run this once)\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    word_tokenize(\"example\")\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# ---- Step 1: Load original mood_keywords.json ----\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "# ---- Step 2: Map Mood_# to actual mood names ----\n",
    "mood_map = {\n",
    "    \"Mood_0\": \"Abuse\",\n",
    "    \"Mood_1\": \"Sexual Issues\",\n",
    "    \"Mood_2\": \"Erectile Dysfunction\",\n",
    "    \"Mood_3\": \"Relationship Issues\",\n",
    "    \"Mood_4\": \"Depression\",\n",
    "    \"Mood_5\": \"Child Custody\",\n",
    "    \"Mood_6\": \"Parental Issues\",\n",
    "    \"Mood_7\": \"Infidelity\",\n",
    "    \"Mood_8\": \"Marriage Problems\",\n",
    "    \"Mood_9\": \"Motherhood\",\n",
    "    \"Mood_10\": \"Gender Identity\",\n",
    "    \"Mood_11\": \"Therapist Issues\",\n",
    "    \"Mood_12\": \"Therapy Anxiety\",\n",
    "    \"Mood_13\": \"Marital Issues\",\n",
    "    \"Mood_14\": \"Unrequited Love\",\n",
    "    \"Mood_15\": \"Therapy Nervousness\",\n",
    "    \"Mood_16\": \"Health Trauma\",\n",
    "    \"Mood_17\": \"PTSD\",\n",
    "    \"Mood_18\": \"Alcoholism\",\n",
    "    \"Mood_19\": \"Breakups\",\n",
    "    \"Mood_20\": \"Sexless Marriage\",\n",
    "    \"Mood_21\": \"Hearing Voices\",\n",
    "    \"Mood_22\": \"Eating Disorders\",\n",
    "    \"Mood_23\": \"Suicidal Thoughts\",\n",
    "    \"Mood_24\": \"Panic Attacks\",\n",
    "    \"Mood_25\": \"Toxic Relationships\",\n",
    "    \"Mood_26\": \"Anxiety And Depression\",\n",
    "    \"Mood_27\": \"Daughter Stress\",\n",
    "    \"Mood_28\": \"Work And Family Stress\",\n",
    "    \"Mood_29\": \"School Stress\"\n",
    "}\n",
    "\n",
    "# Rename original mood keys\n",
    "renamed_keywords = {}\n",
    "for mood_id, keywords in mood_keywords.items():\n",
    "    readable_label = mood_map.get(mood_id, mood_id)\n",
    "    renamed_keywords[readable_label] = keywords\n",
    "\n",
    "# ---- Step 3: Define the 4 main categories and map moods under them ----\n",
    "category_map = {\n",
    "    \"Depression & Anxiety\": [\n",
    "        \"Depression\", \"Unrequited Love\", \"Breakups\", \"Suicidal Thoughts\",\n",
    "        \"Anxiety And Depression\", \"Therapy Anxiety\", \"Therapy Nervousness\",\n",
    "        \"Panic Attacks\", \"Hearing Voices\"\n",
    "    ],\n",
    "    \"Personality & Behaviour\": [\n",
    "        \"Gender Identity\", \"Motherhood\", \"Alcoholism\", \"Eating Disorders\"\n",
    "    ],\n",
    "    \"Stress & Coping\": [\n",
    "        \"Work And Family Stress\", \"Daughter Stress\", \"School Stress\",\n",
    "        \"Toxic Relationships\", \"Parental Issues\", \"Child Custody\",\n",
    "        \"Relationship Issues\", \"Marriage Problems\", \"Infidelity\",\n",
    "        \"Marital Issues\", \"Therapist Issues\"\n",
    "    ],\n",
    "    \"Trauma & PTSD\": [\n",
    "        \"Abuse\", \"Sexual Issues\", \"Erectile Dysfunction\", \"Health Trauma\",\n",
    "        \"PTSD\", \"Sexless Marriage\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ---- Step 4: Function to generate additional keywords using NLTK ----\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "\n",
    "def generate_keywords(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    filtered_tokens = [\n",
    "        w for w in tokens if not w in stop_words and not w in punctuation and w.isalpha()\n",
    "    ]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Example of adding more descriptive text for some moods to extract keywords\n",
    "mood_descriptions = {\n",
    "    \"Depression\": \"Feeling sad, loss of interest, fatigue, changes in appetite and sleep, feelings of worthlessness, difficulty concentrating, thoughts of death or suicide. Major depressive disorder symptoms.\",\n",
    "    \"Anxiety And Depression\": \"Experiencing both persistent worry, nervousness, and fear along with feelings of sadness, hopelessness, and loss of interest in activities.\",\n",
    "    \"Relationship Issues\": \"Problems in interpersonal connections, including communication breakdowns, conflicts, lack of trust, and emotional distance between partners, family members, or friends.\",\n",
    "    \"PTSD\": \"A condition that develops after a traumatic event, characterized by flashbacks, nightmares, severe anxiety, and uncontrollable thoughts about the event. Post-traumatic stress disorder symptoms.\",\n",
    "    \"Abuse\": \"Experiencing physical, emotional, sexual, or financial harm from another person, leading to trauma and significant distress.\",\n",
    "    \"Gender Identity\": \"A person's internal sense of being male, female, both, neither, or somewhere else along the gender spectrum.\",\n",
    "    \"Work And Family Stress\": \"Strain and pressure arising from the demands and responsibilities of both professional life and family obligations.\",\n",
    "    \"Eating Disorders\": \"Characterized by persistent disturbances of eating or eating-related behavior that results in the altered consumption or absorption of food and that significantly impairs physical health or psychosocial functioning.\"\n",
    "}\n",
    "\n",
    "# ---- Step 5: Group and merge keywords for main categories, including generated ones ----\n",
    "grouped_keywords = {}\n",
    "additional_keywords_count = 5  # Number of additional keywords to add per mood\n",
    "\n",
    "for main_category, specific_moods in category_map.items():\n",
    "    combined_keywords = []\n",
    "    for mood in specific_moods:\n",
    "        # Add original keywords\n",
    "        combined_keywords.extend(renamed_keywords.get(mood, []))\n",
    "\n",
    "        # Generate and add more keywords based on descriptions if available\n",
    "        if mood in mood_descriptions:\n",
    "            description = mood_descriptions[mood]\n",
    "            generated_keywords = generate_keywords(description)\n",
    "            combined_keywords.extend(generated_keywords)\n",
    "\n",
    "    # Count and deduplicate, keeping top 20\n",
    "    counter = Counter(combined_keywords)\n",
    "    grouped_keywords[main_category] = [kw for kw, _ in counter.most_common(20)]\n",
    "\n",
    "# ---- Step 6: Save the final grouped mood keywords ----\n",
    "with open(\"mood_keywords_grouped_enhanced.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(grouped_keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ mood_keywords_grouped_enhanced.json created with {len(grouped_keywords)} main categories, including additional keywords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading existing SBERT embeddings...\n",
      "‚úÖ Loaded existing embeddings successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\AI ML Models\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Length of X_train: 34908\n",
      "Length of X_test: 11637\n",
      "Length of Y_train_int: 34908\n",
      "Length of Y_test_int: 11637\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model,save_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Bidirectional, BatchNormalization, Layer, Add, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ---- Custom Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# ---- Load Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "contexts = [sample[\"Context\"] for sample in parsed_data]\n",
    "\n",
    "# ---- Load Mood Keywords ----\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "def detect_mood(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "X_texts, Y_labels = [], []\n",
    "for context in contexts:\n",
    "    mood = detect_mood(context)\n",
    "    if mood:\n",
    "        X_texts.append(context)\n",
    "        Y_labels.append(mood)\n",
    "\n",
    "# ---- Encode and Filter Labels ----\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y_labels)\n",
    "with open(\"mood_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "label_counts = Counter(Y_encoded)\n",
    "valid_labels = {label for label, count in label_counts.items() if count > 1}\n",
    "filtered = [(x, y) for x, y in zip(X_texts, Y_encoded) if y in valid_labels]\n",
    "X_texts_filtered = [x for x, _ in filtered]\n",
    "Y_filtered = [y for _, y in filtered]\n",
    "\n",
    "# ---- SBERT Embedding with Save/Load Logic ----\n",
    "embedding_file = \"sbert_embeddings.npy\"\n",
    "texts_file = \"sbert_embedding_texts.pkl\"\n",
    "labels_file = \"sbert_embedding_labels.pkl\"\n",
    "\n",
    "# Check if embeddings already exist\n",
    "if os.path.exists(embedding_file) and os.path.exists(texts_file) and os.path.exists(labels_file):\n",
    "    print(\"üìÇ Loading existing SBERT embeddings...\")\n",
    "    X_emb = np.load(embedding_file)\n",
    "\n",
    "    with open(texts_file, \"rb\") as f:\n",
    "        saved_texts = pickle.load(f)\n",
    "\n",
    "    with open(labels_file, \"rb\") as f:\n",
    "        saved_labels = pickle.load(f)\n",
    "\n",
    "    # Verify that the saved data matches current data\n",
    "    if len(saved_texts) == len(X_texts_filtered) and len(saved_labels) == len(Y_filtered):\n",
    "        print(\"‚úÖ Loaded existing embeddings successfully!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Saved data doesn't match current data. Regenerating embeddings...\")\n",
    "        sbert = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "        print(\"üîÑ Generating SBERT embeddings...\")\n",
    "        X_emb = sbert.encode(X_texts_filtered, show_progress_bar=True)\n",
    "\n",
    "        # Save the embeddings and corresponding data\n",
    "        print(\"üíæ Saving SBERT embeddings...\")\n",
    "        np.save(embedding_file, X_emb)\n",
    "        with open(texts_file, \"wb\") as f:\n",
    "            pickle.dump(X_texts_filtered, f)\n",
    "        with open(labels_file, \"wb\") as f:\n",
    "            pickle.dump(Y_filtered, f)\n",
    "else:\n",
    "    # Generate embeddings if they don't exist\n",
    "    sbert = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "    print(\"üîÑ Generating SBERT embeddings...\")\n",
    "    X_emb = sbert.encode(X_texts_filtered, show_progress_bar=True)\n",
    "\n",
    "    # Save the embeddings and corresponding data\n",
    "    print(\"üíæ Saving SBERT embeddings...\")\n",
    "    np.save(embedding_file, X_emb)\n",
    "    with open(texts_file, \"wb\") as f:\n",
    "        pickle.dump(X_texts_filtered, f)\n",
    "    with open(labels_file, \"wb\") as f:\n",
    "        pickle.dump(Y_filtered, f)\n",
    "\n",
    "# ---- Resampling ----\n",
    "min_samples = min(Counter(Y_filtered).values())\n",
    "if min_samples < 2:\n",
    "    print(\"‚ö†Ô∏è SMOTE skipped due to insufficient samples.\")\n",
    "    X_resampled, Y_resampled_encoded = X_emb, Y_filtered\n",
    "else:\n",
    "    smote_k = max(1, min(5, min_samples - 1))\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote = smote.fit_resample(X_emb, Y_filtered)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_encoded = smote_tomek.fit_resample(X_smote, Y_smote)\n",
    "\n",
    "Y_resampled = to_categorical(Y_resampled_encoded)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled, test_size=0.25, stratify=np.argmax(Y_resampled, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(Y_train_int), y=Y_train_int)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# ---- Model Builder ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout=0.3):\n",
    "        attention = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention = Dropout(dropout)(attention)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention]))\n",
    "        ff = Dense(ff_dim, activation='relu')(out1)\n",
    "        ff = Dense(inputs.shape[-1])(ff)\n",
    "        ff = Dropout(dropout)(ff)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff]))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_block(inputs)\n",
    "    x = transformer_block(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---- Train Model ----\n",
    "gc.collect()\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=Y_resampled.shape[1])\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, min_lr=1e-6, verbose=1)\n",
    "\n",
    "\n",
    "print(\"Length of X_train:\", len(X_train))\n",
    "print(\"Length of X_test:\", len(X_test))\n",
    "print(\"Length of Y_train_int:\", len(Y_train_int))\n",
    "print(\"Length of Y_test_int:\", len(Y_test_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training model...\n",
      "Epoch 1/20\n",
      "\u001b[1m  24/1091\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m1:59\u001b[0m 112ms/step - accuracy: 0.9900 - loss: 0.0471"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, Y_train_int,\n",
    "    validation_data=(X_test, Y_test_int),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test_int, verbose=1)\n",
    "print(f\"‚úÖ Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# ---- Plot Results ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('Accuracy'); plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Loss'); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- F1 Score ----\n",
    "Y_pred_probs = model.predict(X_test, verbose=1)\n",
    "Y_pred = np.argmax(Y_pred_probs, axis=1)\n",
    "f1 = f1_score(Y_test_int, Y_pred, average=\"weighted\")\n",
    "print(f\"üéØ Weighted F1 Score: {f1 * 100:.2f}%\")\n",
    "\n",
    "\n",
    "model_path = \"mood_classifier_model.keras\"\n",
    "\n",
    "# üßπ Remove old `.keras` model folder if it exists\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        if os.path.isfile(model_path):\n",
    "            os.remove(model_path)\n",
    "        else:\n",
    "            import shutil\n",
    "            shutil.rmtree(model_path)\n",
    "        print(f\"üßπ Removed old model: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not delete old model: {e}\")\n",
    "        raise\n",
    "\n",
    "# üíæ Save the model using native format\n",
    "try:\n",
    "    save_model(model, model_path)\n",
    "    print(f\"‚úÖ Model saved in native Keras format at: {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Confusion Matrix ----f\n",
    "cm = confusion_matrix(Y_test_int, Y_pred)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"üß© Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Optional: ROC AUC for binary/multi-class (One-hot) ----\n",
    "try:\n",
    "    from sklearn.preprocessing import label_binarize\n",
    "    Y_test_bin = label_binarize(Y_test_int, classes=list(range(len(label_encoder.classes_))))\n",
    "    roc_auc = roc_auc_score(Y_test_bin, Y_pred_probs, average=\"macro\", multi_class=\"ovr\")\n",
    "    print(f\"üöÄ ROC AUC Score (macro): {roc_auc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Skipped ROC AUC Score: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# --- Custom Attention Layer ---\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# --- Load Model and Assets ---\n",
    "model = load_model(\"mood_classifier_model.keras\", custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "sbert = SentenceTransformer(\"paraphrase-MiniLM-L12-v2\")\n",
    "\n",
    "with open(\"mood_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "with open(\"mood_keywords_grouped_enhanced.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "# --- Parse cleaned_questions.json ---\n",
    "with open(\"cleaned_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "# Structure: {category.lower(): [questions...]}\n",
    "question_data = {}\n",
    "for entry in raw_data:\n",
    "    category = entry.get(\"category\", \"\").strip().lower()\n",
    "    if not category:\n",
    "        continue\n",
    "    all_qs = []\n",
    "    for doc in entry.get(\"questions\", []):\n",
    "        all_qs.extend([q.strip() for q in doc.get(\"questions\", []) if isinstance(q, str) and len(q.strip()) > 10])\n",
    "    if all_qs:\n",
    "        question_data[category] = all_qs\n",
    "\n",
    "# --- Logging ---\n",
    "def log_entry(data):\n",
    "    path = \"chat_log.json\"\n",
    "    logs = []\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            logs = json.load(f)\n",
    "    logs.append(data)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(logs, f, indent=2)\n",
    "\n",
    "# --- Mood Detection ---\n",
    "def detect_mood_keywords(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(k in text for k in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "def predict_mood(text):\n",
    "    mood = detect_mood_keywords(text)\n",
    "    if mood:\n",
    "        print(f\"üìå Detected mood from keywords: {mood}\")\n",
    "        return mood, 1.0\n",
    "\n",
    "    emb = sbert.encode([text])\n",
    "    emb = np.expand_dims(emb, axis=1)\n",
    "    pred = model.predict(emb, verbose=0)\n",
    "    idx = np.argmax(pred)\n",
    "    conf = float(np.max(pred))\n",
    "    mood = label_encoder.inverse_transform([idx])[0]\n",
    "    print(f\"ü§ñ Predicted mood: {mood} (confidence: {conf:.2f})\")\n",
    "    return mood, conf\n",
    "\n",
    "# --- Question Fetch ---\n",
    "def fetch_questions(mood, count=3):\n",
    "    key = mood.strip().lower()\n",
    "    questions = question_data.get(key, [])\n",
    "    if not questions:\n",
    "        print(f\"‚ö†Ô∏è No questions found for mood '{mood}'.\")\n",
    "        return []\n",
    "    return list(np.random.choice(questions, size=min(count, len(questions)), replace=False))\n",
    "\n",
    "# --- Chatbot ---\n",
    "def chatbot():\n",
    "    print(\"üß† MoodBot is ready. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"üëã Take care. I'm here whenever you need support.\")\n",
    "            break\n",
    "\n",
    "        mood, confidence = predict_mood(user_input)\n",
    "        questions = fetch_questions(mood)\n",
    "\n",
    "        if not questions:\n",
    "            print(f\"üß† MoodBot: I'm here to support you. Would you like to talk more about feeling {mood.lower()}?\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß† MoodBot ({mood.title()}): It seems you're feeling {mood.lower()}. Let's explore further:\\n\")\n",
    "        for i, q in enumerate(questions, 1):\n",
    "            answer = input(f\"{i}. {q}\\nYou: \").strip()\n",
    "            log_entry({\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"user_input\": user_input,\n",
    "                \"predicted_mood\": mood,\n",
    "                \"confidence\": confidence,\n",
    "                \"question\": q,\n",
    "                \"answer\": answer\n",
    "            })\n",
    "        print(\"\\n‚úÖ Thank you for sharing. I'm here anytime you want to talk.\\n\")\n",
    "\n",
    "# --- Run Chatbot ---\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
