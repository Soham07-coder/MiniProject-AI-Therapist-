{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list | findstr tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow keras-tuner numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import (Input, LSTM, Dense, Dropout, Add, LayerNormalization, \n",
    "                                     Bidirectional, MultiHeadAttention, GlobalAveragePooling1D)\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import keras_tuner as kt\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ==============================\n",
    "# üîπ 1. Load Dataset\n",
    "# ==============================\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "X_texts = [sample[\"Context\"] for sample in parsed_data]\n",
    "Y_labels = [sample[\"Response\"] for sample in parsed_data]\n",
    "\n",
    "# ==============================\n",
    "# üîπ 2. Load BERT Tokenizer & Model\n",
    "# ==============================\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = BertModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# ==============================\n",
    "# üîπ 3. Function to Convert Text to BERT Embeddings\n",
    "# ==============================\n",
    "def get_bert_embeddings(texts, batch_size=16):\n",
    "    \"\"\"Converts input texts into BERT embeddings.\"\"\"\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i : i + batch_size]\n",
    "        tokens = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = bert_model(**tokens)\n",
    "        \n",
    "        cls_embedding = output.last_hidden_state[:, 0, :].numpy()  # Extract [CLS] token\n",
    "        embeddings.append(cls_embedding)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# ==============================\n",
    "# üîπ 4. Load or Compute BERT Embeddings\n",
    "# ==============================\n",
    "if os.path.exists(\"bert_embeddings.npy\") and os.path.exists(\"y_encoded.npy\"):\n",
    "    X_emb = np.load(\"bert_embeddings.npy\")\n",
    "    Y_encoded = np.load(\"y_encoded.npy\")\n",
    "    with open(\"response_encoder.pkl\", \"rb\") as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    print(\"‚úÖ Loaded saved BERT embeddings and labels.\")\n",
    "else:\n",
    "    X_emb = get_bert_embeddings(X_texts)\n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_encoded = label_encoder.fit_transform(Y_labels)\n",
    "\n",
    "    np.save(\"bert_embeddings.npy\", X_emb)\n",
    "    np.save(\"y_encoded.npy\", Y_encoded)\n",
    "    with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    print(\"‚úÖ Computed and saved BERT embeddings.\")\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "Y_encoded = to_categorical(Y_encoded, num_classes=num_classes)\n",
    "\n",
    "# ==============================\n",
    "# üîπ 5. Class Balancing with SMOTE\n",
    "# ==============================\n",
    "# Convert labels to integer format for SMOTE\n",
    "Y_resampled_labels = np.argmax(Y_encoded, axis=1)  # Convert one-hot to class labels\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = Counter(Y_resampled_labels)\n",
    "print(f\"üîç Class distribution before SMOTE: {class_counts}\")\n",
    "\n",
    "# Filter out classes with fewer than 2 samples\n",
    "valid_classes = {label for label, count in class_counts.items() if count > 1}\n",
    "\n",
    "# Keep only samples belonging to valid classes\n",
    "valid_indices = [i for i, label in enumerate(Y_resampled_labels) if label in valid_classes]\n",
    "\n",
    "# Update X and Y to remove rare classes\n",
    "X_filtered = X_emb[valid_indices]\n",
    "Y_filtered_labels = Y_resampled_labels[valid_indices]\n",
    "\n",
    "# Apply SMOTE only if multiple valid classes exist\n",
    "if len(set(Y_filtered_labels)) > 1:\n",
    "    min_samples_per_class = min(Counter(Y_filtered_labels).values())  # Find smallest class size\n",
    "    smote_k = min(5, min_samples_per_class - 1) if min_samples_per_class > 1 else 1  # Ensure k_neighbors >= 1\n",
    "\n",
    "    smote = SMOTE(random_state=42, k_neighbors=smote_k)\n",
    "    X_resampled, Y_resampled_labels = smote.fit_resample(X_filtered, Y_filtered_labels)\n",
    "\n",
    "    # Convert back to one-hot encoding\n",
    "    Y_resampled = to_categorical(Y_resampled_labels, num_classes=num_classes)\n",
    "    print(f\"‚úÖ SMOTE applied successfully. New class distribution: {Counter(Y_resampled_labels)}\")\n",
    "else:\n",
    "    X_resampled, Y_resampled = X_filtered, to_categorical(Y_filtered_labels, num_classes=num_classes)\n",
    "    print(\"‚ö†Ô∏è Skipping SMOTE because not enough valid classes exist.\")\n",
    "\n",
    "# ==============================\n",
    "# üîπ 6. Train-Test Split\n",
    "# ==============================\n",
    "try:\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_resampled, Y_resampled, test_size=0.2, random_state=42, stratify=Y_resampled_labels\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(\"‚ö†Ô∏è WARNING: Not enough samples for stratified split. Using a regular split.\")\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        X_resampled, Y_resampled, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "# ==============================\n",
    "# üîπ 7. Debugging & Fixing Label Issues\n",
    "# ==============================\n",
    "Y_train_labels = np.argmax(Y_train, axis=1)\n",
    "min_label, max_label = np.min(Y_train_labels), np.max(Y_train_labels)\n",
    "\n",
    "print(f\"üü¢ Min label index in Y_train: {min_label}\")\n",
    "print(f\"üü¢ Max label index in Y_train: {max_label}\")\n",
    "\n",
    "if max_label >= num_classes:\n",
    "    print(f\"üî¥ ERROR: Found label index {max_label}, but num_classes is {num_classes}!\")\n",
    "    num_classes = max_label + 1  # Fix the issue\n",
    "    print(f\"üîÑ Adjusted num_classes: {num_classes}\")\n",
    "\n",
    "# Expand dimensions for LSTM input\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# Convert labels again with updated num_classes\n",
    "Y_train = to_categorical(Y_train_labels, num_classes=num_classes)\n",
    "Y_test = to_categorical(np.argmax(Y_test, axis=1), num_classes=num_classes)\n",
    "\n",
    "# Compute Class Weights\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(Y_resampled_labels), y=Y_resampled_labels)\n",
    "class_weight_dict = {i: max(w, 0.5) for i, w in enumerate(class_weights)}\n",
    "\n",
    "# ==============================\n",
    "# üîπ 8. Print Final Data Shapes & Stats\n",
    "# ==============================\n",
    "print(\"‚úÖ Training samples:\", len(X_train), \"Test samples:\", len(X_test))\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "np.save(\"X_train.npy\", X_train)\n",
    "np.save(\"Y_train.npy\", Y_train)\n",
    "np.save(\"X_test.npy\", X_test)\n",
    "np.save(\"Y_test.npy\", Y_test)\n",
    "\n",
    "print(\"üíæ Saved X_train, Y_train, X_test, and Y_test to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, LayerNormalization, Bidirectional, LSTM,\n",
    "    GlobalAveragePooling1D, Add, MultiHeadAttention\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "# ==============================\n",
    "# üîπ 1. Custom LSTM\n",
    "# ==============================\n",
    "@register_keras_serializable()\n",
    "class CustomLSTM(LSTM):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        kwargs.pop(\"time_major\", None)\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "# ==============================\n",
    "# üîπ 2. Focal Loss\n",
    "# ==============================\n",
    "@register_keras_serializable()\n",
    "def focal_loss(alpha=0.25, gamma=2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1.0 - tf.keras.backend.epsilon())\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        weight = alpha * tf.math.pow(1 - y_pred, gamma)\n",
    "        return tf.reduce_mean(weight * cross_entropy)\n",
    "    return loss\n",
    "\n",
    "# ==============================\n",
    "# üîπ 3. Model Architecture\n",
    "# ==============================\n",
    "def build_model(input_shape=(1, 768), num_classes=2480, num_heads=4, key_dim=64, lstm_units=256, dropout_rate=0.3):\n",
    "    \"\"\"\n",
    "    Build the model with the given input shape.\n",
    "    \n",
    "    Parameters:\n",
    "    - input_shape: Tuple representing (sequence_length, feature_dim), default (1, 768)\n",
    "    - Other parameters remain the same\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # Initial projection\n",
    "    x = Dense(512, activation=\"relu\", kernel_initializer=HeNormal())(inputs)\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # Calculate the output dimension of the Bidirectional LSTM\n",
    "    bilstm_dim = lstm_units * 2  # Multiply by 2 because it's bidirectional\n",
    "    \n",
    "    # First Bidirectional LSTM - Only if sequence length > 1\n",
    "    if input_shape[0] > 1:\n",
    "        x = Bidirectional(CustomLSTM(lstm_units, return_sequences=True, dropout=dropout_rate))(x)\n",
    "    else:\n",
    "        # For sequence length=1, BiLSTM doesn't make sense, use Dense instead\n",
    "        x = Dense(bilstm_dim, activation=\"relu\")(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Multi-Head Attention - only if sequence length > 1\n",
    "    if input_shape[0] > 1:\n",
    "        attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "        attn_output = Dense(bilstm_dim)(attn_output)  # Project to match output\n",
    "        x = Add()([x, attn_output])\n",
    "        x = LayerNormalization()(x)\n",
    "\n",
    "    # Residual Dense Block\n",
    "    residual = x\n",
    "    x = Dense(bilstm_dim, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Add()([x, residual])\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # Second Bidirectional LSTM - Only if sequence length > 1\n",
    "    if input_shape[0] > 1:\n",
    "        second_lstm_units = lstm_units // 2\n",
    "        x = Bidirectional(CustomLSTM(second_lstm_units, return_sequences=True, dropout=dropout_rate))(x)\n",
    "        x = GlobalAveragePooling1D()(x)\n",
    "    else:\n",
    "        # For sequence length=1, flatten directly\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    # Final layers\n",
    "    x = Dense(256, activation=\"relu\", kernel_regularizer=l2(1e-4))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# üîπ 4. Hyperparameter Tuner\n",
    "# ==============================\n",
    "def model_builder(hp):\n",
    "    # Get actual input shape from the data\n",
    "    sequence_length = X_train.shape[1]\n",
    "    feature_dim = X_train.shape[2]\n",
    "    actual_input_shape = (sequence_length, feature_dim)\n",
    "    \n",
    "    lstm_units = hp.Int(\"lstm_units\", 128, 512, step=64)\n",
    "    num_heads = hp.Int(\"num_heads\", 2, 8, step=2)\n",
    "    key_dim = hp.Int(\"key_dim\", 32, 128, step=32)\n",
    "    dropout_rate = hp.Float(\"dropout_rate\", 0.2, 0.5, step=0.05)\n",
    "    lr = hp.Float(\"learning_rate\", 1e-5, 1e-3, sampling=\"log\")\n",
    "\n",
    "    model = build_model(\n",
    "        input_shape=actual_input_shape,\n",
    "        num_classes=Y_train.shape[1],  # Get number of classes from actual data\n",
    "        num_heads=num_heads,\n",
    "        key_dim=key_dim,\n",
    "        lstm_units=lstm_units,\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr),\n",
    "        loss=focal_loss(),\n",
    "        metrics=[\"accuracy\", TopKCategoricalAccuracy(k=5, name=\"top_5_accuracy\")]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ==============================\n",
    "# üîπ 5. Load Your Data\n",
    "# ==============================\n",
    "# Replace with actual data\n",
    "X_train, Y_train = np.load(\"X_train.npy\"), np.load(\"Y_train.npy\")\n",
    "X_test, Y_test = np.load(\"X_test.npy\"), np.load(\"Y_test.npy\")\n",
    "\n",
    "# Check and print the actual shapes of your data\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Y_test shape: {Y_test.shape}\")\n",
    "\n",
    "# ==============================\n",
    "# üîπ 6. Tuning - WITH ERROR HANDLING\n",
    "# ==============================\n",
    "try:\n",
    "    tuner = kt.Hyperband(\n",
    "        model_builder, \n",
    "        objective=\"val_accuracy\", \n",
    "        max_epochs=50, \n",
    "        factor=3, \n",
    "        directory=\"tuner_results_v3\",\n",
    "        overwrite=True  # Use this to start fresh if needed\n",
    "    )\n",
    "    \n",
    "    # Increase max_consecutive_failed_trials to be more tolerant of errors\n",
    "    tuner.oracle.max_consecutive_failed_trials = 5\n",
    "    \n",
    "    tuner.search(\n",
    "        X_train, Y_train, \n",
    "        epochs=20, \n",
    "        validation_split=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "    best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Tuning error: {str(e)}\")\n",
    "    print(\"Falling back to default hyperparameters...\")\n",
    "    \n",
    "    # Get actual shape from data\n",
    "    actual_input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    num_classes = Y_train.shape[1]\n",
    "    \n",
    "    # Fallback to default hyperparameters\n",
    "    default_hps = {\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"lstm_units\": 256,\n",
    "        \"num_heads\": 4,\n",
    "        \"key_dim\": 64,\n",
    "        \"dropout_rate\": 0.3\n",
    "    }\n",
    "    \n",
    "    best_model = build_model(\n",
    "        input_shape=actual_input_shape,\n",
    "        num_classes=num_classes,\n",
    "        num_heads=default_hps[\"num_heads\"],\n",
    "        key_dim=default_hps[\"key_dim\"],\n",
    "        lstm_units=default_hps[\"lstm_units\"],\n",
    "        dropout_rate=default_hps[\"dropout_rate\"]\n",
    "    )\n",
    "    \n",
    "    best_model.compile(\n",
    "        optimizer=Adam(learning_rate=default_hps[\"learning_rate\"]),\n",
    "        loss=focal_loss(),\n",
    "        metrics=[\"accuracy\", TopKCategoricalAccuracy(k=5, name=\"top_5_accuracy\")]\n",
    "    )\n",
    "    \n",
    "    best_hps = default_hps\n",
    "\n",
    "# Recompile best model with correct objects\n",
    "best_model.compile(\n",
    "    optimizer=Adam(learning_rate=best_hps.get(\"learning_rate\", 1e-4)),\n",
    "    loss=focal_loss(),\n",
    "    metrics=[\"accuracy\", TopKCategoricalAccuracy(k=5, name=\"top_5_accuracy\")]\n",
    ")\n",
    "\n",
    "# Save hyperparams\n",
    "with open(\"best_hyperparameters_v3.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"learning_rate\": float(best_hps.get(\"learning_rate\", 1e-4)),\n",
    "        \"lstm_units\": best_hps.get(\"lstm_units\", 256),\n",
    "        \"num_heads\": best_hps.get(\"num_heads\", 4),\n",
    "        \"dropout_rate\": best_hps.get(\"dropout_rate\", 0.3)\n",
    "    }, f)\n",
    "\n",
    "# ==============================\n",
    "# üîπ 7. Training\n",
    "# ==============================\n",
    "early_stop = EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
    "\n",
    "history = best_model.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    epochs=60,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# üîπ 8. Evaluate and Compare\n",
    "# ==============================\n",
    "test_loss, test_acc, test_top5 = best_model.evaluate(X_test, Y_test)\n",
    "print(f\"‚úÖ Test Accuracy: {test_acc:.4f}\")\n",
    "print(f\"üèÖ Top-5 Accuracy: {test_top5:.4f}\")\n",
    "print(f\"üìä Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# Compare with previous model\n",
    "model_path = \"best_transformer_lstm_model.keras\"\n",
    "prev_model_path = \"previous_transformer_lstm_model.keras\"\n",
    "prev_model_acc = 0\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"üì¶ Loading previous model...\")\n",
    "    prev_model = load_model(\n",
    "        model_path,\n",
    "        custom_objects={\"focal_loss\": focal_loss(), \"CustomLSTM\": CustomLSTM}\n",
    "    )\n",
    "    prev_model.compile(optimizer=Adam(learning_rate=3e-5), loss=focal_loss(), metrics=[\"accuracy\"])\n",
    "    _, prev_model_acc = prev_model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print(f\"üìâ Previous Model Accuracy: {prev_model_acc:.4f}\")\n",
    "\n",
    "# Save if improved\n",
    "if test_acc > prev_model_acc + 0.005:\n",
    "    if os.path.exists(model_path):\n",
    "        os.rename(model_path, prev_model_path)\n",
    "    best_model.save(model_path)\n",
    "    print(\"‚úÖ New best model saved!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No significant improvement over previous model.\")\n",
    "\n",
    "# ==============================\n",
    "# üîπ 9. Plot Training Accuracy\n",
    "# ==============================\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
