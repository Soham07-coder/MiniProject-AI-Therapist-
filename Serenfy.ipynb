{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras nltk scikit-learn transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall transformers -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers tensorflow torch scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install keras==2.13.1  transformers==4.33.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow keras transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras==3.0.5 transformers==4.38.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated complete code with enhancements, preserving old functions\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Bidirectional, Dense, Dropout, BatchNormalization,\n",
    "    Input, Attention, LayerNormalization, GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ---- Load Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "X_texts = [sample[\"Context\"] for sample in parsed_data]\n",
    "Y_labels = [sample[\"Response\"] for sample in parsed_data]\n",
    "\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = TFBertModel.from_pretrained(bert_model_name)\n",
    "try:\n",
    "    X_emb = np.load(\"x_embeddings_sbert.npy\")\n",
    "    Y_encoded = np.load(\"y_encoded.npy\")\n",
    "    with open(\"response_encoder.pkl\", \"rb\") as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    print(\"✅ Loaded saved embeddings and encoder.\")\n",
    "except FileNotFoundError:\n",
    "    X_emb = sbert_model.encode(X_texts)\n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_encoded = label_encoder.fit_transform(Y_labels)\n",
    "    np.save(\"x_embeddings_sbert.npy\", X_emb)\n",
    "    np.save(\"y_encoded.npy\", Y_encoded)\n",
    "    with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    print(\"✅ Computed and saved embeddings and encoder.\")\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "Y_encoded = to_categorical(Y_encoded, num_classes=num_classes)\n",
    "\n",
    "# ---- Filter Rare Classes ----\n",
    "class_counts = Counter(np.argmax(Y_encoded, axis=1))\n",
    "valid_classes = {cls for cls, count in class_counts.items() if count > 1}\n",
    "valid_indices = [i for i, label in enumerate(np.argmax(Y_encoded, axis=1)) if label in valid_classes]\n",
    "X_filtered = X_emb[valid_indices]\n",
    "Y_filtered = Y_encoded[valid_indices]\n",
    "filtered_class_counts = Counter(np.argmax(Y_filtered, axis=1))\n",
    "min_samples_per_class = min(filtered_class_counts.values())\n",
    "\n",
    "if min_samples_per_class < 2:\n",
    "    print(\"⚠️ Skipping SMOTETomek due to class imbalance.\")\n",
    "    X_resampled, Y_resampled = X_filtered, Y_filtered\n",
    "else:\n",
    "    smote_k = max(min(5, min_samples_per_class - 1), 1)\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote = smote.fit_resample(X_filtered, np.argmax(Y_filtered, axis=1))\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled = smote_tomek.fit_resample(X_smote, Y_smote)\n",
    "    Y_resampled = to_categorical(Y_resampled, num_classes=num_classes)\n",
    "\n",
    "# ---- Train-Test Split ----\n",
    "num_samples = len(X_resampled)\n",
    "test_size = min(int(0.2 * num_samples), num_samples - num_classes)\n",
    "test_size = max(test_size, num_classes)\n",
    "\n",
    "if len(Y_resampled.shape) > 1:  # If Y_resampled is one-hot encoded\n",
    "    Y_labels_resampled = np.argmax(Y_resampled, axis=1)  # Convert to integer labels\n",
    "else:\n",
    "    Y_labels_resampled = Y_resampled  # Already integer labels\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_labels_resampled, test_size=test_size, random_state=42, stratify=Y_labels_resampled\n",
    ")\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# ---- Ensure Y_train and Y_test are always integers ----\n",
    "if len(Y_train.shape) > 1:  # Only apply np.argmax() if needed\n",
    "    Y_train = np.argmax(Y_train, axis=1)\n",
    "    Y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "print(f\"✅ X_train shape: {X_train.shape}\")  \n",
    "print(f\"✅ Y_train shape: {Y_train.shape}\")  \n",
    "print(f\"✅ Y_test shape: {Y_test.shape}\") \n",
    "\n",
    "y_train_labels = Y_train if Y_train.ndim == 1 else np.argmax(Y_train, axis=1)  # Ensure class weights use integer labels\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train_labels), y=y_train_labels)\n",
    "class_weight_dict = {i: max(w, 0.5) for i, w in enumerate(class_weights)}  # Set min weight of 0.5 to avoid extreme imbalance\n",
    "\n",
    "# ---- Define or Load Model ----\n",
    "model_path = \"optimized_lstm_model.h5\"\n",
    "prev_model_path = \"previous_lstm_model.h5\"\n",
    "\n",
    "# ---- Define or Load Model ----\n",
    "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"✅ Loading existing model...\")\n",
    "    model = load_model(model_path)\n",
    "    optimizer = RMSprop(learning_rate=1e-3)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "else:\n",
    "    print(\"🚀 Building new model with attention...\")\n",
    "    input_layer = Input(shape=(1, X_train.shape[-1]))\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(input_layer)# Add recurrent dropout\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x    = Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LayerNormalization()(x)  # Normalize before attention\n",
    "    attention = Attention()([x, x])\n",
    "    x = GlobalAveragePooling1D()(attention)\n",
    "\n",
    "    x = Dense(256, activation=\"relu\", kernel_initializer=GlorotUniform())(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(128, activation=\"relu\", kernel_initializer=GlorotUniform())(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    output = Dense(num_classes, activation=\"softmax\")(x)    \n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "loss_function = \"sparse_categorical_crossentropy\"\n",
    "optimizer = Adam(learning_rate=5e-5)  # Lower initial learning rate\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# ---- Train ----\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\")\n",
    "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=2, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    epochs=80,  # Increase epochs for better convergence\n",
    "    batch_size=64,  # Reduce batch size to 32\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping, checkpoint, lr_scheduler]\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Evaluate ----\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print(f\"✅ Model Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Ensure Y_test is one-hot encoded before evaluating the model\n",
    "if Y_test.ndim == 1:  # Convert only if it's 1D (integer labels)\n",
    "    Y_test = to_categorical(Y_test, num_classes=num_classes)\n",
    "\n",
    "# ---- Save if better ----\n",
    "prev_model_acc = 0\n",
    "if os.path.exists(\"optimized_lstm_model.h5\"):\n",
    "    print(\"✅ Loading existing model...\")\n",
    "    prev_model = load_model(\"optimized_lstm_model.h5\")\n",
    "    \n",
    "    # Explicitly compile the model to avoid warning\n",
    "    prev_model.compile(\n",
    "        optimizer = Adam(learning_rate=5e-5),\n",
    "        loss=\"categorical_crossentropy\",  # Use categorical loss if Y_test is one-hot encoded\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    _, prev_model_acc = prev_model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "if test_acc > prev_model_acc+0.005:\n",
    "    if os.path.exists(model_path):\n",
    "        os.rename(model_path, prev_model_path)  # Keep previous model\n",
    "    model.save(model_path)\n",
    "    print(f\"✅ Improved model saved as `{model_path}`.\")\n",
    "    print(f\"📂 Previous model saved as `{prev_model_path}`.\")\n",
    "else:\n",
    "    print(f\"⚠️ No improvement. Previous accuracy: {prev_model_acc * 100:.2f}%\")\n",
    "\n",
    "# ---- Plot Accuracy ----\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Dropout, Input, LayerNormalization ,Attention, Layer\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# ---- Step 1: Load Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "parsed_data = [json.loads(line) for line in lines if line.strip()]\n",
    "\n",
    "X_texts = [sample[\"Context\"] for sample in parsed_data]\n",
    "Y_labels = [sample[\"Response\"] for sample in parsed_data]\n",
    "\n",
    "# ---- Step 2: Load Sentence-BERT Model ----\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "try:\n",
    "    X_emb = np.load(\"x_embeddings_sbert.npy\")\n",
    "    Y_int = np.load(\"y_encoded.npy\")\n",
    "    with open(\"response_encoder.pkl\", \"rb\") as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    print(\"✅ Loaded saved embeddings and encoder.\")\n",
    "except FileNotFoundError:\n",
    "    X_emb = sbert_model.encode(X_texts)\n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_int = label_encoder.fit_transform(Y_labels)\n",
    "    np.save(\"x_embeddings_sbert.npy\", X_emb)\n",
    "    np.save(\"y_encoded.npy\", Y_int)\n",
    "    with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    print(\"✅ Computed and saved embeddings and encoder.\")\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "Y_encoded = to_categorical(Y_int, num_classes=num_classes)\n",
    "\n",
    "# ---- Filter classes with more than 1 sample ----\n",
    "class_counts = Counter(Y_int)\n",
    "valid_classes = {cls for cls, count in class_counts.items() if count > 1}\n",
    "valid_indices = [i for i, y in enumerate(Y_int) if y in valid_classes]\n",
    "\n",
    "X_filtered = X_emb[valid_indices]\n",
    "Y_filtered_int = np.array(Y_int)[valid_indices]\n",
    "\n",
    "filtered_class_counts = Counter(Y_filtered_int)\n",
    "print(f\"📊 Filtered Class Distribution: {filtered_class_counts}\")\n",
    "\n",
    "# ---- SMOTE + TomekLinks ----\n",
    "min_samples_per_class = min(filtered_class_counts.values())\n",
    "if min_samples_per_class < 2:\n",
    "    print(\"⚠️ Not enough samples for SMOTE.\")\n",
    "    X_resampled, Y_resampled_int = X_filtered, Y_filtered_int\n",
    "else:\n",
    "    smote_k = max(min(5, min_samples_per_class - 1), 1)\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote_int = smote.fit_resample(X_filtered, Y_filtered_int)\n",
    "\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_int = smote_tomek.fit_resample(X_smote, Y_smote_int)\n",
    "    print(f\"📊 After SMOTETomek: {Counter(Y_resampled_int)}\")\n",
    "\n",
    "Y_resampled = to_categorical(Y_resampled_int, num_classes=num_classes)\n",
    "\n",
    "# ---- Train-Test Split ----\n",
    "test_size = max(min(int(0.2 * len(X_resampled)), len(X_resampled) - num_classes), num_classes)\n",
    "X_train, X_test, Y_train, Y_test, Y_train_int, Y_test_int = train_test_split(\n",
    "    X_resampled, Y_resampled, Y_resampled_int, test_size=test_size, stratify=Y_resampled_int, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"✅ Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
    "\n",
    "# ---- Reshape Input for LSTM ----\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "# ---- Compute Class Weights ----\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(Y_train_int), y=Y_train_int)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# ---- Define Learning Rate Schedule ----\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate=0.001, decay_steps=1500, decay_rate=0.9)\n",
    "\n",
    "# ---- Load or Define Model ----\n",
    "model_path = \"optimized_lstm_model.h5\"\n",
    "\n",
    "# ---- Define Model with Attention ----\n",
    "if os.path.exists(model_path):\n",
    "    print(\"✅ Loading existing model...\")\n",
    "    model = load_model(model_path, custom_objects={\"Attention\": Attention})\n",
    "    optimizer = AdamW(learning_rate=1e-3, weight_decay=1e-4)\n",
    "    model.compile(loss=CategoricalCrossentropy(label_smoothing=0.1), optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "else:\n",
    "    print(\"🚀 No model found. Training from scratch...\")\n",
    "    model = Sequential([\n",
    "        Input(shape=(1, X_train.shape[-1])),\n",
    "        Bidirectional(LSTM(256, return_sequences=True)),\n",
    "        LayerNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Bidirectional(LSTM(256, return_sequences=True)),\n",
    "        LayerNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        AttentionLayer(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation=\"softmax\")\n",
    "    ])\n",
    "    optimizer = AdamW(learning_rate=1e-3, weight_decay=1e-4)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=CategoricalCrossentropy(label_smoothing=0.1),\n",
    "                  metrics=[\"accuracy\"])\n",
    "\n",
    "# ---- Callbacks ----\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, verbose=1)\n",
    "\n",
    "# ---- Train ----\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# ---- Save ----\n",
    "model.save(model_path)\n",
    "\n",
    "# ---- Evaluate ----\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print(f\"✅ Final Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# ---- Step 9: Save the Best Model ----\n",
    "prev_model_acc = 0\n",
    "if os.path.exists(\"optimized_lstm_model.h5\"):\n",
    "    prev_model = load_model(\"optimized_lstm_model.h5\")\n",
    "    _, prev_model_acc = prev_model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "_, new_model_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "if new_model_acc > prev_model_acc:\n",
    "    model.save(\"optimized_lstm_model.h5\")\n",
    "    print(f\"✅ New model saved with improved accuracy: {new_model_acc * 100:.2f}%\")\n",
    "else:\n",
    "    print(f\"⚠️ New model did NOT improve accuracy. Keeping previous model: {prev_model_acc * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# Predict\n",
    "Y_pred_probs = model.predict(X_test)\n",
    "Y_pred_int = np.argmax(Y_pred_probs, axis=1)\n",
    "\n",
    "\n",
    "# ---- Step 10: Plot Training vs Validation Accuracy ----\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Model\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K  # To clear memory\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, LSTM, BatchNormalization, Bidirectional, Layer,\n",
    "                                     MultiHeadAttention, LayerNormalization, Add)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ---- Step 2: Define Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                   initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                   initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1]) # Output shape is (batch_size, embedding_dim)\n",
    "\n",
    "# ---- Step 3: Load and Prepare Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "X_texts = [sample[\"Context\"] for sample in parsed_data]\n",
    "Y_labels = [sample[\"Response\"] for sample in parsed_data]\n",
    "\n",
    "# ---- Step 4: Define SBERT Variants to Test ----\n",
    "sbert_variants = [\n",
    "    'paraphrase-MiniLM-L12-v2'\n",
    "]\n",
    "#',\n",
    "#,     'all-MiniLM-L6-v2',\n",
    "# ---- Step 5: Encode Labels ----\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded_labels = label_encoder.fit_transform(Y_labels)\n",
    "num_classes_original = len(label_encoder.classes_)\n",
    "Y_encoded = to_categorical(Y_encoded_labels, num_classes=num_classes_original)\n",
    "\n",
    "# ---- Step 6: Save Label Encoder ----\n",
    "with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# ---- Step 7: Remove Rare Classes (only 1 sample) ----\n",
    "class_counts = Counter(np.argmax(Y_encoded, axis=1))\n",
    "valid_classes = {cls for cls, count in class_counts.items() if count > 1}\n",
    "valid_indices = [i for i, label in enumerate(np.argmax(Y_encoded, axis=1)) if label in valid_classes]\n",
    "\n",
    "X_texts_filtered = [X_texts[i] for i in valid_indices]\n",
    "Y_filtered = Y_encoded[valid_indices]\n",
    "Y_filtered_encoded_labels = np.argmax(Y_filtered, axis=1) # Get integer labels after filtering\n",
    "\n",
    "filtered_class_counts = Counter(Y_filtered_encoded_labels)\n",
    "min_samples_per_class = min(filtered_class_counts.values())\n",
    "print(f\"📊 Filtered Class Distribution: {filtered_class_counts}\")\n",
    "\n",
    "# ---- Step 8: Prepare Embeddings and Split Data ----\n",
    "sbert_model_name = sbert_variants[0] # Using the first variant for this non-function block\n",
    "print(f\"\\n🔄 Preparing embeddings with SBERT model: {sbert_model_name}\")\n",
    "sbert = SentenceTransformer(sbert_model_name)\n",
    "X_emb = sbert.encode(X_texts_filtered)\n",
    "Y_filtered_encoded_labels_step8 = np.argmax(Y_filtered, axis=1) # Get integer labels after filtering\n",
    "\n",
    "# Resample using SMOTE + TomekLinks if safe\n",
    "if min_samples_per_class < 2:\n",
    "    print(\"⚠️ SMOTE skipped due to classes with <2 samples.\")\n",
    "    X_resampled, Y_resampled_encoded = X_emb, Y_filtered_encoded_labels_step8\n",
    "else:\n",
    "    smote_k = min(5, min_samples_per_class - 1)\n",
    "    smote_k = max(smote_k, 1)\n",
    "    print(f\"✅ Applying SMOTE with k={smote_k} and TomekLinks...\")\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote_encoded = smote.fit_resample(X_emb, Y_filtered_encoded_labels_step8)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_encoded = smote_tomek.fit_resample(X_smote, Y_smote_encoded)\n",
    "\n",
    "unique_resampled_labels = np.unique(Y_resampled_encoded)\n",
    "current_num_classes = len(unique_resampled_labels)\n",
    "\n",
    "# --- FIX: Ensure Y_resampled_encoded values are within the valid range ---\n",
    "Y_resampled_encoded = np.clip(Y_resampled_encoded, 0, current_num_classes - 1)\n",
    "\n",
    "Y_resampled = to_categorical(Y_resampled_encoded, num_classes=current_num_classes)\n",
    "print(f\"📊 Resampled Class Distribution: {Counter(np.argmax(Y_resampled, axis=1))}\")\n",
    "print(f\"🔢 Number of classes after resampling: {current_num_classes}\")\n",
    "\n",
    "# Train-Test Split (75% train, 25% test)\n",
    "print(f\"➡️ Before train_test_split: test_size = 0.25\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled,\n",
    "    test_size=0.25,  # 25% for the test set\n",
    "    random_state=42,\n",
    "    stratify=np.argmax(Y_resampled, axis=1)\n",
    ")\n",
    "print(f\"✅ After train_test_split: Training size = {len(X_train)}, Testing size = {len(X_test)}\")\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(Y_train_int),\n",
    "    y=Y_train_int\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"📏 Training data size: {len(X_train)}\")\n",
    "print(f\"📏 Testing data size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 9: Build Model ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout_rate=0.3):\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention_output = Dropout(dropout_rate)(attention_output)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention_output]))\n",
    "\n",
    "        ff_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "        ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "        ff_output = Dropout(dropout_rate)(ff_output)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff_output]))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_block(inputs)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=0.002,\n",
    "        decay_steps=1200,\n",
    "        decay_rate=0.97\n",
    "    )\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=current_num_classes)\n",
    "\n",
    "# ---- Step 10: Train Model ----\n",
    "print(f\"\\n🚀 Training model using: {sbert_model_name}\")\n",
    "\n",
    "# 🧹 Clear session\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, verbose=1, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train_int,  # Use integer labels for training\n",
    "    validation_data=(X_test, Y_test_int),  # Use integer labels for validation\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[reduce_lr, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---- Step 11: Evaluate and Save Model ----\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test_int, verbose=1)  # Use integer labels for evaluation\n",
    "print(f\"✅ Test Accuracy with {sbert_model_name}: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = f\"model_{sbert_model_name.replace('-', '_')}.keras\"\n",
    "if os.path.exists(model_filename):\n",
    "    os.remove(model_filename)\n",
    "model.save(model_filename)\n",
    "print(f\"💾 Model saved as {model_filename}\")\n",
    "\n",
    "# --- Optional: Plot training history ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Step 12: Save Model as Final ----\n",
    "final_model_name = \"optimized_lstm_model.keras\"\n",
    "if os.path.exists(final_model_name):\n",
    "    try:\n",
    "        previous_model = load_model(final_model_name, custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "        _, prev_acc = previous_model.evaluate(X_test, Y_test, verbose=1)\n",
    "        if test_acc > prev_acc:\n",
    "            os.replace(model_filename, final_model_name)\n",
    "            print(f\"✅ Final model updated → Accuracy improved from {prev_acc*100:.2f}% → {test_acc*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"⚠️ Existing model retained → Accuracy {prev_acc*100:.2f}% is better or equal.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Couldn't load existing model: {e}\")\n",
    "        os.replace(model_filename, final_model_name)\n",
    "        print(f\"✅ Final model saved as {final_model_name}\")\n",
    "else:\n",
    "    os.rename(model_filename, final_model_name)\n",
    "    print(f\"✅ Final model saved as {final_model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
