{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras nltk scikit-learn transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob vaderSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall transformers -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers tensorflow torch scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install keras==2.13.1  transformers==4.33.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow keras transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras==3.0.5 transformers==4.38.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mood_keywords.json\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ---- Step 1: Load original mood_keywords.json (with Mood_0, Mood_1...) ----\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "# ---- Step 2: Map Mood_# to actual mood names ----\n",
    "mood_map = {\n",
    "    \"Mood_0\": \"Abuse\",\n",
    "    \"Mood_1\": \"Sexual Issues\",\n",
    "    \"Mood_2\": \"Erectile Dysfunction\",\n",
    "    \"Mood_3\": \"Relationship Issues\",\n",
    "    \"Mood_4\": \"Depression\",\n",
    "    \"Mood_5\": \"Child Custody\",\n",
    "    \"Mood_6\": \"Parental Issues\",\n",
    "    \"Mood_7\": \"Infidelity\",\n",
    "    \"Mood_8\": \"Marriage Problems\",\n",
    "    \"Mood_9\": \"Motherhood\",\n",
    "    \"Mood_10\": \"Gender Identity\",\n",
    "    \"Mood_11\": \"Therapist Issues\",\n",
    "    \"Mood_12\": \"Therapy Anxiety\",\n",
    "    \"Mood_13\": \"Marital Issues\",\n",
    "    \"Mood_14\": \"Unrequited Love\",\n",
    "    \"Mood_15\": \"Therapy Nervousness\",\n",
    "    \"Mood_16\": \"Health Trauma\",\n",
    "    \"Mood_17\": \"PTSD\",\n",
    "    \"Mood_18\": \"Alcoholism\",\n",
    "    \"Mood_19\": \"Breakups\",\n",
    "    \"Mood_20\": \"Sexless Marriage\",\n",
    "    \"Mood_21\": \"Hearing Voices\",\n",
    "    \"Mood_22\": \"Eating Disorders\",\n",
    "    \"Mood_23\": \"Suicidal Thoughts\",\n",
    "    \"Mood_24\": \"Panic Attacks\",\n",
    "    \"Mood_25\": \"Toxic Relationships\",\n",
    "    \"Mood_26\": \"Anxiety And Depression\",\n",
    "    \"Mood_27\": \"Daughter Stress\",\n",
    "    \"Mood_28\": \"Work And Family Stress\",\n",
    "    \"Mood_29\": \"School Stress\"\n",
    "}\n",
    "\n",
    "# Rename original mood keys\n",
    "renamed_keywords = {}\n",
    "for mood_id, keywords in mood_keywords.items():\n",
    "    readable_label = mood_map.get(mood_id, mood_id)\n",
    "    renamed_keywords[readable_label] = keywords\n",
    "\n",
    "# ---- Step 3: Define broader categories and map moods under them ----\n",
    "category_map = {\n",
    "    \"Depression\": [\n",
    "        \"Depression\", \"Unrequited Love\", \"Breakups\", \"Suicidal Thoughts\", \"Anxiety And Depression\"\n",
    "    ],\n",
    "    \"Anxiety\": [\n",
    "        \"Therapy Anxiety\", \"Therapy Nervousness\", \"Panic Attacks\", \"Hearing Voices\", \"Anxiety And Depression\"\n",
    "    ],\n",
    "    \"Stress\": [\n",
    "        \"Work And Family Stress\", \"Daughter Stress\", \"School Stress\", \"Toxic Relationships\",\n",
    "        \"Parental Issues\", \"Child Custody\"\n",
    "    ],\n",
    "    \"Health Trauma\": [\n",
    "        \"Health Trauma\", \"Eating Disorders\", \"Alcoholism\", \"PTSD\"\n",
    "    ],\n",
    "    \"Panic Attack\": [\n",
    "        \"Panic Attacks\"\n",
    "    ],\n",
    "    \"Work and School Stress\": [\n",
    "        \"Work And Family Stress\", \"School Stress\"\n",
    "    ],\n",
    "    \"Relationship Issues\": [\n",
    "        \"Marriage Problems\", \"Infidelity\", \"Sexless Marriage\", \"Marital Issues\", \"Relationship Issues\"\n",
    "    ],\n",
    "    \"Sexual Health\": [\n",
    "        \"Erectile Dysfunction\", \"Sexual Issues\"\n",
    "    ],\n",
    "    \"Identity\": [\n",
    "        \"Gender Identity\", \"Motherhood\"\n",
    "    ],\n",
    "    \"Therapy Issues\": [\n",
    "        \"Therapist Issues\", \"Therapy Anxiety\", \"Therapy Nervousness\"\n",
    "    ],\n",
    "    \"Abuse\": [\n",
    "        \"Abuse\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ---- Step 4: Group and merge keywords for broader categories ----\n",
    "grouped_keywords = {}\n",
    "\n",
    "for broad_mood, specific_moods in category_map.items():\n",
    "    combined_keywords = []\n",
    "    for mood in specific_moods:\n",
    "        combined_keywords.extend(renamed_keywords.get(mood, []))\n",
    "    \n",
    "    # Count and deduplicate\n",
    "    counter = Counter(combined_keywords)\n",
    "    grouped_keywords[broad_mood] = [kw for kw, _ in counter.most_common(20)]\n",
    "\n",
    "# ---- Step 5: Save the final grouped mood keywords ----\n",
    "with open(\"mood_keywords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(grouped_keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ mood_keywords.json created with {len(grouped_keywords)} broader categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Code uses Mood_keywords.json file\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model,save_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Bidirectional, BatchNormalization, Layer, Add, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ---- Custom Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# ---- Load Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "contexts = [sample[\"Context\"] for sample in parsed_data]\n",
    "\n",
    "# ---- Load Mood Keywords ----\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "def detect_mood(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "X_texts, Y_labels = [], []\n",
    "for context in contexts:\n",
    "    mood = detect_mood(context)\n",
    "    if mood:\n",
    "        X_texts.append(context)\n",
    "        Y_labels.append(mood)\n",
    "\n",
    "# ---- Encode and Filter Labels ----\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y_labels)\n",
    "with open(\"mood_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "label_counts = Counter(Y_encoded)\n",
    "valid_labels = {label for label, count in label_counts.items() if count > 1}\n",
    "filtered = [(x, y) for x, y in zip(X_texts, Y_encoded) if y in valid_labels]\n",
    "X_texts_filtered = [x for x, _ in filtered]\n",
    "Y_filtered = [y for _, y in filtered]\n",
    "\n",
    "# ---- SBERT Embedding with Save/Load Logic ----\n",
    "embedding_file = \"sbert_embeddings.npy\"\n",
    "texts_file = \"sbert_embedding_texts.pkl\"\n",
    "labels_file = \"sbert_embedding_labels.pkl\"\n",
    "\n",
    "# Check if embeddings already exist\n",
    "if os.path.exists(embedding_file) and os.path.exists(texts_file) and os.path.exists(labels_file):\n",
    "    print(\"üìÇ Loading existing SBERT embeddings...\")\n",
    "    X_emb = np.load(embedding_file)\n",
    "    \n",
    "    with open(texts_file, \"rb\") as f:\n",
    "        saved_texts = pickle.load(f)\n",
    "    \n",
    "    with open(labels_file, \"rb\") as f:\n",
    "        saved_labels = pickle.load(f)\n",
    "    \n",
    "    # Verify that the saved data matches current data\n",
    "    if len(saved_texts) == len(X_texts_filtered) and len(saved_labels) == len(Y_filtered):\n",
    "        print(\"‚úÖ Loaded existing embeddings successfully!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Saved data doesn't match current data. Regenerating embeddings...\")\n",
    "        sbert = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "        print(\"üîÑ Generating SBERT embeddings...\")\n",
    "        X_emb = sbert.encode(X_texts_filtered, show_progress_bar=True)\n",
    "        \n",
    "        # Save the embeddings and corresponding data\n",
    "        print(\"üíæ Saving SBERT embeddings...\")\n",
    "        np.save(embedding_file, X_emb)\n",
    "        with open(texts_file, \"wb\") as f:\n",
    "            pickle.dump(X_texts_filtered, f)\n",
    "        with open(labels_file, \"wb\") as f:\n",
    "            pickle.dump(Y_filtered, f)\n",
    "else:\n",
    "    # Generate embeddings if they don't exist\n",
    "    sbert = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "    print(\"üîÑ Generating SBERT embeddings...\")\n",
    "    X_emb = sbert.encode(X_texts_filtered, show_progress_bar=True)\n",
    "    \n",
    "    # Save the embeddings and corresponding data\n",
    "    print(\"üíæ Saving SBERT embeddings...\")\n",
    "    np.save(embedding_file, X_emb)\n",
    "    with open(texts_file, \"wb\") as f:\n",
    "        pickle.dump(X_texts_filtered, f)\n",
    "    with open(labels_file, \"wb\") as f:\n",
    "        pickle.dump(Y_filtered, f)\n",
    "\n",
    "# ---- Resampling ----\n",
    "min_samples = min(Counter(Y_filtered).values())\n",
    "if min_samples < 2:\n",
    "    print(\"‚ö†Ô∏è SMOTE skipped due to insufficient samples.\")\n",
    "    X_resampled, Y_resampled_encoded = X_emb, Y_filtered\n",
    "else:\n",
    "    smote_k = max(1, min(5, min_samples - 1))\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote = smote.fit_resample(X_emb, Y_filtered)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_encoded = smote_tomek.fit_resample(X_smote, Y_smote)\n",
    "\n",
    "Y_resampled = to_categorical(Y_resampled_encoded)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled, test_size=0.25, stratify=np.argmax(Y_resampled, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(Y_train_int), y=Y_train_int)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# ---- Model Builder ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout=0.3):\n",
    "        attention = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention = Dropout(dropout)(attention)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention]))\n",
    "        ff = Dense(ff_dim, activation='relu')(out1)\n",
    "        ff = Dense(inputs.shape[-1])(ff)\n",
    "        ff = Dropout(dropout)(ff)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff]))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_block(inputs)\n",
    "    x = transformer_block(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---- Train Model ----\n",
    "gc.collect()\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=Y_resampled.shape[1])\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, min_lr=1e-6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, Y_train_int,\n",
    "    validation_data=(X_test, Y_test_int),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test_int, verbose=1)\n",
    "print(f\"‚úÖ Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# ---- Plot Results ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('Accuracy'); plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Loss'); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- F1 Score ----\n",
    "Y_pred_probs = model.predict(X_test, verbose=1)\n",
    "Y_pred = np.argmax(Y_pred_probs, axis=1)\n",
    "f1 = f1_score(Y_test_int, Y_pred, average=\"weighted\")\n",
    "print(f\"üéØ Weighted F1 Score: {f1 * 100:.2f}%\")\n",
    "\n",
    "\n",
    "model_path = \"mood_classifier_model.keras\"\n",
    "\n",
    "# üßπ Remove old `.keras` model folder if it exists\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        if os.path.isfile(model_path):\n",
    "            os.remove(model_path)\n",
    "        else:\n",
    "            import shutil\n",
    "            shutil.rmtree(model_path)\n",
    "        print(f\"üßπ Removed old model: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not delete old model: {e}\")\n",
    "        raise\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# ---- Custom Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# ---- Load Model (.keras) ----\n",
    "try:\n",
    "    model = load_model(\"mood_classifier_model.keras\", custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "    print(\"‚úÖ Model loaded from .keras format!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load .keras model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ---- Load SBERT and Resources ----\n",
    "sbert = SentenceTransformer(\"paraphrase-MiniLM-L12-v2\")\n",
    "\n",
    "with open(\"mood_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "try:\n",
    "    with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        mood_keywords = json.load(f)\n",
    "except:\n",
    "    mood_keywords = {}\n",
    "\n",
    "# ‚úÖ Load questions from cleaned_questions.json\n",
    "with open(\"cleaned_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_question_data = json.load(f)\n",
    "\n",
    "question_data = {}\n",
    "for entry in raw_question_data:\n",
    "    category = entry.get(\"category\", \"\").strip().lower()\n",
    "    documents = entry.get(\"questions\", [])\n",
    "    all_questions = []\n",
    "    for doc in documents:\n",
    "        q_list = doc.get(\"questions\", [])\n",
    "        if isinstance(q_list, list):\n",
    "            clean_qs = [q.strip() for q in q_list if isinstance(q, str) and len(q.strip()) > 10]\n",
    "            all_questions.extend(clean_qs)\n",
    "    if all_questions:\n",
    "        question_data[category] = all_questions\n",
    "\n",
    "# ---- Logging ----\n",
    "LOG_FILE = \"chat_log.json\"\n",
    "\n",
    "def log_user_query(user_input, mood, question, confidence, answer=None):\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"user_input\": user_input,\n",
    "        \"predicted_mood\": mood,\n",
    "        \"model_confidence\": confidence,\n",
    "        \"response_question\": question,\n",
    "        \"user_answer\": answer\n",
    "    }\n",
    "    try:\n",
    "        if os.path.exists(LOG_FILE):\n",
    "            with open(LOG_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                logs = json.load(f)\n",
    "        else:\n",
    "            logs = []\n",
    "        logs.append(entry)\n",
    "        with open(LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(logs, f, indent=2)\n",
    "        print(\"üìù Query logged.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to log query: {e}\")\n",
    "\n",
    "# ---- Mood Detection ----\n",
    "def detect_mood_keywords(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood.title()\n",
    "    return None\n",
    "\n",
    "def predict_mood(text):\n",
    "    mood = detect_mood_keywords(text) if mood_keywords else None\n",
    "    if mood:\n",
    "        print(f\"üìå Detected mood from keywords: {mood}\")\n",
    "        return mood, 1.0\n",
    "\n",
    "    embedding = sbert.encode([text])\n",
    "    embedding = np.expand_dims(embedding, axis=1)  # (1, 1, 384)\n",
    "    prediction = model.predict(embedding, verbose=0)\n",
    "    mood_index = np.argmax(prediction)\n",
    "    confidence = float(np.max(prediction))\n",
    "    mood_label = label_encoder.inverse_transform([mood_index])[0]\n",
    "    print(f\"ü§ñ Predicted mood: {mood_label} (confidence: {confidence:.2f})\")\n",
    "    return mood_label, confidence\n",
    "\n",
    "# ---- Fetch Questions by Mood ----\n",
    "def fetch_questions(mood, count=3):\n",
    "    mood_key = mood.strip().lower()\n",
    "    questions = question_data.get(mood_key, [])\n",
    "    if not questions:\n",
    "        print(f\"‚ö†Ô∏è No questions found for mood '{mood}'.\")\n",
    "        return []\n",
    "    return list(np.random.choice(questions, size=min(count, len(questions)), replace=False))\n",
    "\n",
    "# ---- Chatbot Loop ----\n",
    "def chatbot(num_questions=3):\n",
    "    print(\"üß† MoodBot is ready. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if not user_input:\n",
    "            continue\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"üëã Take care. I'm here whenever you need support.\")\n",
    "            break\n",
    "\n",
    "        mood, confidence = predict_mood(user_input)\n",
    "        questions = fetch_questions(mood, count=num_questions)\n",
    "\n",
    "        if not questions:\n",
    "            print(f\"üß† MoodBot: I'm here for you. Would you like to talk more about feeling {mood.lower()}?\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß† MoodBot ({mood}): It sounds like you're feeling {mood.lower()}. Let's go through a few questions:\\n\")\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            answer = input(f\"{i}. {question}\\nYou: \").strip()\n",
    "            log_user_query(user_input, mood, question, confidence, answer)\n",
    "\n",
    "        print(\"\\n‚úÖ Thank you for sharing. You can continue chatting or type 'exit' to leave.\\n\")\n",
    "\n",
    "# ---- Run Chatbot ----\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n",
    "\n",
    "# üíæ Save the model using native format\n",
    "try:\n",
    "    save_model(model, model_path)\n",
    "    print(f\"‚úÖ Model saved in native Keras format at: {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# ---- Custom Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# ---- Load Model (.keras) ----\n",
    "try:\n",
    "    model = load_model(\"mood_classifier_model.keras\", custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "    print(\"‚úÖ Model loaded from .keras format!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load .keras model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ---- Load SBERT and Resources ----\n",
    "sbert = SentenceTransformer(\"paraphrase-MiniLM-L12-v2\")\n",
    "\n",
    "with open(\"mood_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "try:\n",
    "    with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        mood_keywords = json.load(f)\n",
    "except:\n",
    "    mood_keywords = {}\n",
    "\n",
    "# ‚úÖ Load and normalize questions from uploaded list-style JSON\n",
    "with open(\"questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_question_data = json.load(f)\n",
    "\n",
    "# Normalize: Convert list of moods to dict: mood ‚Üí list of valid questions\n",
    "question_data = {}\n",
    "for entry in raw_question_data:\n",
    "    mood = entry.get(\"mood\", \"\").strip().lower()\n",
    "    questions = entry.get(\"questions\", [])\n",
    "    if mood and isinstance(questions, list):\n",
    "        valid = [q.strip() for q in questions if isinstance(q, str) and len(q.strip()) > 10]\n",
    "        if valid:\n",
    "            question_data.setdefault(mood, []).extend(valid)\n",
    "\n",
    "# ---- Logging ----\n",
    "LOG_FILE = \"chat_log.json\"\n",
    "\n",
    "def log_user_query(user_input, mood, question, confidence, answer=None):\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"user_input\": user_input,\n",
    "        \"predicted_mood\": mood,\n",
    "        \"model_confidence\": confidence,\n",
    "        \"response_question\": question,\n",
    "        \"user_answer\": answer\n",
    "    }\n",
    "    if os.path.exists(LOG_FILE):\n",
    "        with open(LOG_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            logs = json.load(f)\n",
    "    else:\n",
    "        logs = []\n",
    "    logs.append(entry)\n",
    "    with open(LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(logs, f, indent=2)\n",
    "    print(\"üìù Query logged.\")\n",
    "\n",
    "# ---- Mood Detection ----\n",
    "def detect_mood_keywords(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "def predict_mood(text):\n",
    "    mood = detect_mood_keywords(text) if mood_keywords else None\n",
    "    if mood:\n",
    "        print(f\"üìå Detected mood from keywords: {mood}\")\n",
    "        return mood, 1.0\n",
    "\n",
    "    embedding = sbert.encode([text])\n",
    "    embedding = np.expand_dims(embedding, axis=1)  # (1, 1, 384)\n",
    "    prediction = model.predict(embedding, verbose=0)\n",
    "    mood_index = np.argmax(prediction)\n",
    "    confidence = float(np.max(prediction))\n",
    "    mood_label = label_encoder.inverse_transform([mood_index])[0]\n",
    "    print(f\"ü§ñ Predicted mood: {mood_label} (confidence: {confidence:.2f})\")\n",
    "    return mood_label, confidence\n",
    "\n",
    "# ---- Fetch Multiple Questions ----\n",
    "def fetch_questions(mood, count=3):\n",
    "    mood_key = mood.strip().lower()\n",
    "    all_questions = question_data.get(mood_key, [])\n",
    "    if not all_questions:\n",
    "        print(f\"‚ö†Ô∏è No questions found for mood '{mood}'.\")\n",
    "        return []\n",
    "    return list(np.random.choice(all_questions, size=min(count, len(all_questions)), replace=False))\n",
    "\n",
    "# ---- Chatbot Main ----\n",
    "def chatbot(num_questions=3):\n",
    "    print(\"üß† ChatBot is ready. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"üëã Take care. I'm here whenever you need support.\")\n",
    "            break\n",
    "\n",
    "        mood, confidence = predict_mood(user_input)\n",
    "        questions = fetch_questions(mood, count=num_questions)\n",
    "\n",
    "        if not questions:\n",
    "            print(f\"üß† MoodBot: I'm here to support you. Would you like to talk more about feeling {mood.lower()}?\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß† MoodBot ({mood}): I understand you're feeling {mood.lower()}. Let's go through a few questions:\\n\")\n",
    "\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            answer = input(f\"{i}. {question}\\nYou: \").strip()\n",
    "            log_user_query(user_input, mood, question, confidence, answer)\n",
    "\n",
    "        print(\"\\n‚úÖ Thank you for sharing. If you‚Äôd like to talk more, just type again or type 'exit' to leave.\\n\")\n",
    "\n",
    "# ---- Run ----\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moods_keywords.json\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"combined_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def infer_mood(text):\n",
    "    # VADER sentiment\n",
    "    vader_scores = vader.polarity_scores(text)\n",
    "    compound = vader_scores[\"compound\"]\n",
    "\n",
    "    # TextBlob for polarity/subjectivity\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "    # Rule-based mood inference\n",
    "    if compound <= -0.5:\n",
    "        if subjectivity > 0.6:\n",
    "            return [\"Depressed\"]\n",
    "        elif polarity < -0.3:\n",
    "            return [\"Hopeless\"]\n",
    "        else:\n",
    "            return [\"Struggling\"]\n",
    "    elif -0.5 < compound < 0:\n",
    "        return [\"Anxious\"]\n",
    "    elif 0 <= compound < 0.3:\n",
    "        return [\"Neutral\"]\n",
    "    elif 0.3 <= compound <= 0.6:\n",
    "        return [\"Hopeful\"]\n",
    "    else:\n",
    "        return [\"Motivated\"]\n",
    "\n",
    "# Analyze and build output\n",
    "output_data = []\n",
    "for entry in data:\n",
    "    context = entry.get(\"Context\", \"\")\n",
    "    response = entry.get(\"Response\", \"\")\n",
    "    full_text = f\"{context} {response}\"\n",
    "\n",
    "    moods = infer_mood(full_text)\n",
    "\n",
    "    output_data.append({\n",
    "        \"Context\": context.strip(),\n",
    "        \"Detected_Moods\": moods,\n",
    "        \"Tags\": []\n",
    "    })\n",
    "\n",
    "# Save result\n",
    "with open(\"moods_keywords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ moods_keywords.json generated successfully using auto NLP analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Code uses Moods_keywords.json file\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model,save_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Bidirectional, BatchNormalization, Layer, Add, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ---- Custom Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# ---- Load Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "contexts = [sample[\"Context\"] for sample in parsed_data]\n",
    "\n",
    "# ---- Load Mood Keywords ----\n",
    "# ---- Load mood samples with Detected_Moods from JSON ----\n",
    "with open(\"moods_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_samples = json.load(f)\n",
    "\n",
    "X_texts, Y_labels = [], []\n",
    "for item in mood_samples:\n",
    "    context = item.get(\"Context\", \"\").strip()\n",
    "    detected_moods = item.get(\"Detected_Moods\", [])\n",
    "    if context and detected_moods:\n",
    "        for mood in detected_moods:\n",
    "            mood_clean = mood.strip().title()\n",
    "            X_texts.append(context)\n",
    "            Y_labels.append(mood_clean)\n",
    "\n",
    "print(f\"üìä Loaded {len(X_texts)} mood-labeled samples from moods_keywords.json\")\n",
    "\n",
    "# ---- Encode and Filter Labels ----\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y_labels)\n",
    "with open(\"mood_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "label_counts = Counter(Y_encoded)\n",
    "valid_labels = {label for label, count in label_counts.items() if count > 1}\n",
    "filtered = [(x, y) for x, y in zip(X_texts, Y_encoded) if y in valid_labels]\n",
    "X_texts_filtered = [x for x, _ in filtered]\n",
    "Y_filtered = [y for _, y in filtered]\n",
    "\n",
    "# ---- SBERT Embedding with Save/Load Logic ----\n",
    "embedding_file = \"msbert_embeddings.npy\"\n",
    "texts_file = \"msbert_embedding_texts.pkl\"\n",
    "labels_file = \"msbert_embedding_labels.pkl\"\n",
    "\n",
    "# Check if embeddings already exist\n",
    "if os.path.exists(embedding_file) and os.path.exists(texts_file) and os.path.exists(labels_file):\n",
    "    print(\"üìÇ Loading existing SBERT embeddings...\")\n",
    "    X_emb = np.load(embedding_file)\n",
    "    \n",
    "    with open(texts_file, \"rb\") as f:\n",
    "        saved_texts = pickle.load(f)\n",
    "    \n",
    "    with open(labels_file, \"rb\") as f:\n",
    "        saved_labels = pickle.load(f)\n",
    "    \n",
    "    # Verify that the saved data matches current data\n",
    "    if len(saved_texts) == len(X_texts_filtered) and len(saved_labels) == len(Y_filtered):\n",
    "        print(\"‚úÖ Loaded existing embeddings successfully!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Saved data doesn't match current data. Regenerating embeddings...\")\n",
    "        sbert = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "        print(\"üîÑ Generating SBERT embeddings...\")\n",
    "        X_emb = sbert.encode(X_texts_filtered, show_progress_bar=True)\n",
    "        \n",
    "        # Save the embeddings and corresponding data\n",
    "        print(\"üíæ Saving SBERT embeddings...\")\n",
    "        np.save(embedding_file, X_emb)\n",
    "        with open(texts_file, \"wb\") as f:\n",
    "            pickle.dump(X_texts_filtered, f)\n",
    "        with open(labels_file, \"wb\") as f:\n",
    "            pickle.dump(Y_filtered, f)\n",
    "else:\n",
    "    # Generate embeddings if they don't exist\n",
    "    sbert = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "    print(\"üîÑ Generating SBERT embeddings...\")\n",
    "    X_emb = sbert.encode(X_texts_filtered, show_progress_bar=True)\n",
    "    \n",
    "    # Save the embeddings and corresponding data\n",
    "    print(\"üíæ Saving SBERT embeddings...\")\n",
    "    np.save(embedding_file, X_emb)\n",
    "    with open(texts_file, \"wb\") as f:\n",
    "        pickle.dump(X_texts_filtered, f)\n",
    "    with open(labels_file, \"wb\") as f:\n",
    "        pickle.dump(Y_filtered, f)\n",
    "\n",
    "# ---- Resampling ----\n",
    "min_samples = min(Counter(Y_filtered).values())\n",
    "if min_samples < 2:\n",
    "    print(\"‚ö†Ô∏è SMOTE skipped due to insufficient samples.\")\n",
    "    X_resampled, Y_resampled_encoded = X_emb, Y_filtered\n",
    "else:\n",
    "    smote_k = max(1, min(5, min_samples - 1))\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote = smote.fit_resample(X_emb, Y_filtered)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_encoded = smote_tomek.fit_resample(X_smote, Y_smote)\n",
    "\n",
    "Y_resampled = to_categorical(Y_resampled_encoded)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled, test_size=0.25, stratify=np.argmax(Y_resampled, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(Y_train_int), y=Y_train_int)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# ---- Model Builder ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout=0.3):\n",
    "        attention = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention = Dropout(dropout)(attention)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention]))\n",
    "        ff = Dense(ff_dim, activation='relu')(out1)\n",
    "        ff = Dense(inputs.shape[-1])(ff)\n",
    "        ff = Dropout(dropout)(ff)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff]))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_block(inputs)\n",
    "    x = transformer_block(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---- Train Model ----\n",
    "gc.collect()\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=Y_resampled.shape[1])\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, min_lr=1e-6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, Y_train_int,\n",
    "    validation_data=(X_test, Y_test_int),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test_int, verbose=1)\n",
    "print(f\"‚úÖ Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# ---- Plot Results ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('Accuracy'); plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Loss'); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- F1 Score ----\n",
    "Y_pred_probs = model.predict(X_test, verbose=1)\n",
    "Y_pred = np.argmax(Y_pred_probs, axis=1)\n",
    "f1 = f1_score(Y_test_int, Y_pred, average=\"weighted\")\n",
    "print(f\"üéØ Weighted F1 Score: {f1 * 100:.2f}%\")\n",
    "\n",
    "\n",
    "model_path = \"moods_classifier_model.keras\"\n",
    "\n",
    "# üßπ Remove old `.keras` model folder if it exists\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        if os.path.isfile(model_path):\n",
    "            os.remove(model_path)\n",
    "        else:\n",
    "            import shutil\n",
    "            shutil.rmtree(model_path)\n",
    "        print(f\"üßπ Removed old model: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not delete old model: {e}\")\n",
    "        raise\n",
    "\n",
    "# üíæ Save the model using native format\n",
    "try:\n",
    "    save_model(model, model_path)\n",
    "    print(f\"‚úÖ Model saved in native Keras format at: {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# --- Custom Attention Layer ---\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# --- Load model & assets ---\n",
    "try:\n",
    "    model = load_model(\"moods_classifier_model.keras\", custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model load failed: {e}\")\n",
    "    exit()\n",
    "\n",
    "sbert = SentenceTransformer(\"paraphrase-MiniLM-L12-v2\")\n",
    "\n",
    "with open(\"mood_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "with open(\"cleaned_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_question_data = json.load(f)\n",
    "\n",
    "# --- Build question bank from cleaned_questions.json ---\n",
    "question_data = {}\n",
    "for entry in raw_question_data:\n",
    "    category = entry.get(\"category\", \"\").strip().lower()\n",
    "    question_data[category] = []\n",
    "    for doc in entry.get(\"questions\", []):\n",
    "        for q in doc.get(\"questions\", []):\n",
    "            if isinstance(q, str) and len(q.strip()) > 10:\n",
    "                question_data[category].append(q.strip())\n",
    "\n",
    "# --- Logging ---\n",
    "def log_response(entry):\n",
    "    LOG_FILE = \"chat_log.json\"\n",
    "    logs = []\n",
    "    if os.path.exists(LOG_FILE):\n",
    "        with open(LOG_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            logs = json.load(f)\n",
    "    logs.append(entry)\n",
    "    with open(LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(logs, f, indent=2)\n",
    "\n",
    "# --- Mood Detection ---\n",
    "def detect_mood_keywords(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "def predict_mood(text):\n",
    "    mood = detect_mood_keywords(text)\n",
    "    if mood:\n",
    "        print(f\"üìå Detected mood from keywords: {mood}\")\n",
    "        return mood, 1.0\n",
    "    emb = sbert.encode([text])\n",
    "    emb = np.expand_dims(emb, axis=1)\n",
    "    pred = model.predict(emb, verbose=0)\n",
    "    mood_idx = np.argmax(pred)\n",
    "    confidence = float(np.max(pred))\n",
    "    mood = label_encoder.inverse_transform([mood_idx])[0]\n",
    "    print(f\"ü§ñ Predicted mood: {mood} (confidence: {confidence:.2f})\")\n",
    "    return mood, confidence\n",
    "\n",
    "# --- Fetch Questions ---\n",
    "def fetch_questions(mood, count=3):\n",
    "    mood_key = mood.strip().lower()\n",
    "    available_categories = {k.lower(): k for k in question_data}\n",
    "    matched_key = available_categories.get(mood_key)\n",
    "\n",
    "    if not matched_key:\n",
    "        print(f\"‚ö†Ô∏è No questions found for mood '{mood}'.\")\n",
    "        return []\n",
    "\n",
    "    all_qs = question_data[matched_key]\n",
    "    return list(np.random.choice(all_qs, size=min(count, len(all_qs)), replace=False))\n",
    "\n",
    "# --- Main Chatbot ---\n",
    "def chatbot():\n",
    "    print(\"üß† MoodBot is ready. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if not user_input or user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"üëã Take care. I'm here whenever you need support.\")\n",
    "            break\n",
    "\n",
    "        mood, confidence = predict_mood(user_input)\n",
    "        questions = fetch_questions(mood)\n",
    "\n",
    "        if not questions:\n",
    "            print(f\"üß† MoodBot: I'm here to support you. Would you like to share more about how you're feeling?\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nüß† MoodBot ({mood.title()}): It sounds like you're feeling {mood.lower()}. Let's go through a few questions:\\n\")\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            answer = input(f\"{i}. {question}\\nYou: \").strip()\n",
    "            log_response({\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"user_input\": user_input,\n",
    "                \"predicted_mood\": mood,\n",
    "                \"confidence\": confidence,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer\n",
    "            })\n",
    "\n",
    "        print(\"\\n‚úÖ Thank you for sharing. You can continue chatting or type 'exit' to leave.\\n\")\n",
    "\n",
    "# --- Run ---\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
