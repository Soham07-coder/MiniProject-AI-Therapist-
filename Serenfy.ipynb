{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras nltk scikit-learn transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall transformers -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers tensorflow torch scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install keras==2.13.1  transformers==4.33.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow keras transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras==3.0.5 transformers==4.38.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Model\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import backend as K  # To clear memory\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, LSTM, BatchNormalization, Bidirectional, Layer,\n",
    "                                     MultiHeadAttention, LayerNormalization, Add , Input)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ---- Step 2: Define Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                   initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                   initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1]) # Output shape is (batch_size, embedding_dim)\n",
    "\n",
    "# ---- Step 3: Load and Prepare Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "X_texts = [sample[\"Context\"] for sample in parsed_data]\n",
    "Y_labels = [sample[\"Response\"] for sample in parsed_data]\n",
    "\n",
    "# ---- Step 4: Define SBERT Variants to Test ----\n",
    "sbert_variants = [\n",
    "    'paraphrase-MiniLM-L12-v2'\n",
    "]\n",
    "#',\n",
    "#,     'all-MiniLM-L6-v2',\n",
    "embedding_path = \"X_emb.npy\"\n",
    "label_path = \"Y_encoded.npy\"\n",
    "# ---- Step 5: Encode Labels ----\n",
    "if os.path.exists(embedding_path) and os.path.exists(label_path):\n",
    "    print(\"‚úÖ Loading saved embeddings...\")\n",
    "    X_emb = np.load(embedding_path)\n",
    "    Y_encoded = np.load(label_path)\n",
    "    with open(\"response_encoder.pkl\", \"rb\") as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "else:\n",
    "    print(\"üîÑ Generating SBERT embeddings...\")\n",
    "    sbert = SentenceTransformer(sbert_model_name)\n",
    "    X_emb = sbert.encode(augmented_inputs, show_progress_bar=True)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_encoded_labels = label_encoder.fit_transform(Y_labels)\n",
    "    num_classes_original = len(label_encoder.classes_)\n",
    "    Y_encoded = to_categorical(Y_encoded_labels, num_classes=num_classes_original)\n",
    "    np.save(embedding_path, X_emb)\n",
    "    np.save(label_path, Y_encoded)\n",
    "    with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "\n",
    "\n",
    "# ---- Step 6: Save Label Encoder ----\n",
    "with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# ---- Step 7: Remove Rare Classes (only 1 sample) ----\n",
    "class_counts = Counter(np.argmax(Y_encoded, axis=1))\n",
    "valid_classes = {cls for cls, count in class_counts.items() if count > 1}\n",
    "valid_indices = [i for i, label in enumerate(np.argmax(Y_encoded, axis=1)) if label in valid_classes]\n",
    "\n",
    "X_texts_filtered = [X_texts[i] for i in valid_indices]\n",
    "# Re-extract filtered string labels\n",
    "Y_labels_filtered = [Y_labels[i] for i in valid_indices]\n",
    "\n",
    "# Re-encode using a new LabelEncoder for filtered classes\n",
    "label_encoder = LabelEncoder()\n",
    "Y_filtered_int = label_encoder.fit_transform(Y_labels_filtered)\n",
    "\n",
    "# Save new encoder\n",
    "with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Now one-hot encode with correct number of classes\n",
    "current_num_classes = len(label_encoder.classes_)\n",
    "Y_filtered = to_categorical(Y_filtered_int, num_classes=current_num_classes)\n",
    "\n",
    "# ---- Step 8: Prepare Embeddings and Split Data ----\n",
    "sbert_model_name = sbert_variants[0] # Using the first variant for this non-function block\n",
    "print(f\"\\nüîÑ Preparing embeddings with SBERT model: {sbert_model_name}\")\n",
    "sbert = SentenceTransformer(sbert_model_name)\n",
    "X_emb = sbert.encode(X_texts_filtered)\n",
    "Y_filtered_encoded_labels_step8 = np.argmax(Y_filtered, axis=1) # Get integer labels after filtering\n",
    "\n",
    "# Resample using SMOTE + TomekLinks if safe\n",
    "if min_samples_per_class < 2:\n",
    "    print(\"‚ö†Ô∏è SMOTE skipped due to classes with <2 samples.\")\n",
    "    X_resampled, Y_resampled_encoded = X_emb, Y_filtered_encoded_labels_step8\n",
    "else:\n",
    "    smote_k = min(5, min_samples_per_class - 1)\n",
    "    smote_k = max(smote_k, 1)\n",
    "    print(f\"‚úÖ Applying SMOTE with k={smote_k} and TomekLinks...\")\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote_encoded = smote.fit_resample(X_emb, Y_filtered_encoded_labels_step8)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_encoded = smote_tomek.fit_resample(X_smote, Y_smote_encoded)\n",
    "\n",
    "unique_resampled_labels = np.unique(Y_resampled_encoded)\n",
    "current_num_classes = len(unique_resampled_labels)\n",
    "\n",
    "# --- FIX: Ensure Y_resampled_encoded values are within the valid range ---\n",
    "Y_resampled_encoded = np.clip(Y_resampled_encoded, 0, current_num_classes - 1)\n",
    "\n",
    "Y_resampled = to_categorical(Y_resampled_encoded, num_classes=current_num_classes)\n",
    "print(f\"üìä Resampled Class Distribution: {Counter(np.argmax(Y_resampled, axis=1))}\")\n",
    "print(f\"üî¢ Number of classes after resampling: {current_num_classes}\")\n",
    "\n",
    "# Train-Test Split (75% train, 25% test)\n",
    "print(f\"‚û°Ô∏è Before train_test_split: test_size = 0.25\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled,\n",
    "    test_size=0.25,  # 25% for the test set\n",
    "    random_state=42,\n",
    "    stratify=np.argmax(Y_resampled, axis=1)\n",
    ")\n",
    "print(f\"‚úÖ After train_test_split: Training size = {len(X_train)}, Testing size = {len(X_test)}\")\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(Y_train_int),\n",
    "    y=Y_train_int\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"üìè Training data size: {len(X_train)}\")\n",
    "print(f\"üìè Testing data size: {len(X_test)}\")\n",
    "# ---- Step 9: Build Model ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout_rate=0.3):\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention_output = Dropout(dropout_rate)(attention_output)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention_output]))\n",
    "\n",
    "        ff_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "        ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "        ff_output = Dropout(dropout_rate)(ff_output)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff_output]))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_block(inputs)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=0.0005,\n",
    "        decay_steps=1100,\n",
    "        decay_rate=0.95\n",
    "    )\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=AdamW(learning_rate=lr_schedule), metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=current_num_classes)\n",
    "\n",
    "# ---- Step 10: Train Model ----\n",
    "print(f\"\\nüöÄ Training model using: {sbert_model_name}\")\n",
    "\n",
    "# üßπ Clear session\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, verbose=1, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train_int,  # Use integer labels for training\n",
    "    validation_data=(X_test, Y_test_int),  # Use integer labels for validation\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---- Step 11: Evaluate and Save Model ----\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test_int, verbose=1)  # Use integer labels for evaluation\n",
    "print(f\"‚úÖ Test Accuracy with {sbert_model_name}: {test_acc * 100:.2f}%\")\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# ---- Custom Attention Layer (required for model loading) ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# ---- File Paths ----\n",
    "MODEL_PATH = \"optimized_lstm_model.keras\"\n",
    "LABEL_ENCODER_PATH = \"response_encoder.pkl\"\n",
    "QUESTIONNAIRE_PATH = \"generated_questionnaire.json\"\n",
    "MOOD_KEYWORDS_PATH = \"mood_keywords.json\"\n",
    "SBERT_MODEL_NAME = \"paraphrase-MiniLM-L12-v2\"\n",
    "\n",
    "# ---- Load Assets ----\n",
    "model = load_model(MODEL_PATH, custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "\n",
    "with open(LABEL_ENCODER_PATH, \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "with open(QUESTIONNAIRE_PATH, \"r\") as f:\n",
    "    questionnaire = json.load(f)\n",
    "\n",
    "with open(MOOD_KEYWORDS_PATH, \"r\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "sbert = SentenceTransformer(SBERT_MODEL_NAME)\n",
    "\n",
    "# ---- Helper Functions ----\n",
    "def augment_input_with_mood_keywords(user_input):\n",
    "    tokens = []\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(word.lower() in user_input.lower() for word in keywords):\n",
    "            tokens.append(f\"intent:{mood.lower()}\")\n",
    "    return user_input + \" \" + \" \".join(tokens)\n",
    "\n",
    "def predict_response(user_input):\n",
    "    \"\"\"Return full response from the model\"\"\"\n",
    "    augmented = augment_input_with_mood_keywords(user_input)\n",
    "    embedding = sbert.encode([augmented])\n",
    "    embedding = np.expand_dims(embedding, axis=1)\n",
    "    prediction = model.predict(embedding, verbose=0)\n",
    "    pred_index = np.argmax(prediction, axis=1)[0]\n",
    "    return label_encoder.inverse_transform([pred_index])[0]\n",
    "\n",
    "def detect_mood_from_response(response_text):\n",
    "    \"\"\"Infer mood category from keywords in the model response\"\"\"\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(word.lower() in response_text.lower() for word in keywords):\n",
    "            return mood\n",
    "    return \"Unknown\"\n",
    "\n",
    "def get_question_for_mood(mood):\n",
    "    \"\"\"Select a question based on detected mood\"\"\"\n",
    "    for category, questions in questionnaire.items():\n",
    "        if mood.lower() in category.lower():\n",
    "            return random.choice(questions)\n",
    "    return \"Sorry, I couldn't find a relevant question for you.\"\n",
    "\n",
    "# ---- Chat Loop ----\n",
    "def chatbot():\n",
    "    print(\"üß† Mental Health Bot: Hello! I'm here to support you. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"üß† Bot: Take care and stay safe! üíô\")\n",
    "            break\n",
    "\n",
    "        full_response = predict_response(user_input)\n",
    "        detected_mood = detect_mood_from_response(full_response)\n",
    "        question = get_question_for_mood(detected_mood)\n",
    "\n",
    "        print(f\"\\nüß† Bot (Mood Detected: {detected_mood})\")\n",
    "        print(f\"‚Üí Response: {full_response}\")\n",
    "        print(f\"‚Üí Follow-up Question: {question}\\n\")\n",
    "\n",
    "# ---- Run ----\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Bidirectional, BatchNormalization, Layer, Add, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ---- Custom Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# ---- Load Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "contexts = [sample[\"Context\"] for sample in parsed_data]\n",
    "\n",
    "# ---- Load Mood Keywords ----\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "def detect_mood(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "X_texts, Y_labels = [], []\n",
    "for context in contexts:\n",
    "    mood = detect_mood(context)\n",
    "    if mood:\n",
    "        X_texts.append(context)\n",
    "        Y_labels.append(mood)\n",
    "\n",
    "# ---- Encode and Filter Labels ----\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y_labels)\n",
    "with open(\"mood_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "label_counts = Counter(Y_encoded)\n",
    "valid_labels = {label for label, count in label_counts.items() if count > 1}\n",
    "filtered = [(x, y) for x, y in zip(X_texts, Y_encoded) if y in valid_labels]\n",
    "X_texts_filtered = [x for x, _ in filtered]\n",
    "Y_filtered = [y for _, y in filtered]\n",
    "\n",
    "# ---- SBERT Embedding ----\n",
    "sbert = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "print(\"üîÑ Generating SBERT embeddings...\")\n",
    "X_emb = sbert.encode(X_texts_filtered, show_progress_bar=True)\n",
    "\n",
    "# ---- Resampling ----\n",
    "min_samples = min(Counter(Y_filtered).values())\n",
    "if min_samples < 2:\n",
    "    print(\"‚ö†Ô∏è SMOTE skipped due to insufficient samples.\")\n",
    "    X_resampled, Y_resampled_encoded = X_emb, Y_filtered\n",
    "else:\n",
    "    smote_k = max(1, min(5, min_samples - 1))\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote = smote.fit_resample(X_emb, Y_filtered)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_encoded = smote_tomek.fit_resample(X_smote, Y_smote)\n",
    "\n",
    "Y_resampled = to_categorical(Y_resampled_encoded)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled, test_size=0.25, stratify=np.argmax(Y_resampled, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(Y_train_int), y=Y_train_int)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# ---- Model Builder ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout=0.3):\n",
    "        attention = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention = Dropout(dropout)(attention)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention]))\n",
    "        ff = Dense(ff_dim, activation='relu')(out1)\n",
    "        ff = Dense(inputs.shape[-1])(ff)\n",
    "        ff = Dropout(dropout)(ff)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff]))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_block(inputs)\n",
    "    x = transformer_block(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---- Train Model ----\n",
    "gc.collect()\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=Y_resampled.shape[1])\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, min_lr=1e-6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, Y_train_int,\n",
    "    validation_data=(X_test, Y_test_int),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test_int, verbose=1)\n",
    "print(f\"‚úÖ Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "model.save(\"mood_classifier_model.keras\", save_format=\"keras\")\n",
    "print(\"üíæ Model saved as mood_classifier_model.keras\")\n",
    "\n",
    "# ---- Plot Results ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('Accuracy'); plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Loss'); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- F1 Score ----\n",
    "Y_pred_probs = model.predict(X_test, verbose=0)\n",
    "Y_pred = np.argmax(Y_pred_probs, axis=1)\n",
    "f1 = f1_score(Y_test_int, Y_pred, average=\"weighted\")\n",
    "print(f\"üéØ Weighted F1 Score: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# ---- Custom Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# ---- Load Model ----\n",
    "try:\n",
    "    model = load_model(\"mood_classifier_model.keras\", custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "    print(\"‚úÖ Model loaded cleanly.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ---- Load SBERT and Resources ----\n",
    "sbert = SentenceTransformer(\"paraphrase-MiniLM-L12-v2\")\n",
    "\n",
    "with open(\"mood_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "with open(\"extracted_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    question_data = json.load(f)\n",
    "\n",
    "# ---- Logging ----\n",
    "LOG_FILE = \"chat_log.json\"\n",
    "\n",
    "def log_user_query(user_input, mood, question):\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"user_input\": user_input,\n",
    "        \"predicted_mood\": mood,\n",
    "        \"response_question\": question\n",
    "    }\n",
    "    if os.path.exists(LOG_FILE):\n",
    "        with open(LOG_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            logs = json.load(f)\n",
    "    else:\n",
    "        logs = []\n",
    "    logs.append(entry)\n",
    "    with open(LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(logs, f, indent=2)\n",
    "    print(\"üìù Query logged.\")\n",
    "\n",
    "# ---- Mood Detection ----\n",
    "def detect_mood_keywords(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "def predict_mood(text):\n",
    "    mood = detect_mood_keywords(text)\n",
    "    if mood:\n",
    "        print(f\"üìå Detected mood from keywords: {mood}\")\n",
    "        return mood\n",
    "\n",
    "    embedding = sbert.encode([text])\n",
    "    embedding = np.expand_dims(embedding, axis=1)  # Shape: (1, 1, 384)\n",
    "    prediction = model.predict(embedding, verbose=0)\n",
    "    mood_index = np.argmax(prediction)\n",
    "    mood_label = label_encoder.inverse_transform([mood_index])[0]\n",
    "    print(f\"ü§ñ Predicted mood from model: {mood_label}\")\n",
    "    return mood_label\n",
    "\n",
    "# ---- Fetch Question ----\n",
    "def fetch_question(mood):\n",
    "    mood_sections = question_data.get(mood, [])\n",
    "    all_questions = []\n",
    "\n",
    "    for section in mood_sections:\n",
    "        general = section.get(\"General\", {})\n",
    "        questions = general.get(\"questions\", [])\n",
    "        if isinstance(questions, list):\n",
    "            all_questions.extend(questions)\n",
    "\n",
    "    if not all_questions:\n",
    "        return \"I'm here to support you. Can you tell me more about how you're feeling?\"\n",
    "    return np.random.choice(all_questions)\n",
    "\n",
    "# ---- Chatbot Main ----\n",
    "def chatbot():\n",
    "    print(\"üß† ChatBot is ready. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"üëã Take care. I'm here whenever you need support.\")\n",
    "            break\n",
    "\n",
    "        mood = predict_mood(user_input)\n",
    "        question = fetch_question(mood)\n",
    "        print(f\"MoodBot ({mood}): {question}\\n\")\n",
    "        log_user_query(user_input, mood, question)\n",
    "\n",
    "# ---- Run ----\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is Another version OF the code , \n",
    "import json\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load your clustered short mood dataset\n",
    "with open(\"short_mood_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Group context texts by mood\n",
    "mood_to_contexts = defaultdict(list)\n",
    "for sample in data:\n",
    "    mood = sample[\"Mood\"]\n",
    "    context = sample[\"Context\"]\n",
    "    if mood and context:\n",
    "        mood_to_contexts[mood].append(context.lower())\n",
    "\n",
    "# Extract top keywords for each mood label\n",
    "mood_keywords = {}\n",
    "for mood, contexts in mood_to_contexts.items():\n",
    "    all_text = \" \".join(contexts)\n",
    "    words = word_tokenize(all_text)\n",
    "    cleaned_words = [\n",
    "        w.lower() for w in words\n",
    "        if w.isalpha() and w.lower() not in stop_words\n",
    "    ]\n",
    "    word_freq = Counter(cleaned_words)\n",
    "    top_keywords = [word for word, _ in word_freq.most_common(15)]\n",
    "    if top_keywords:\n",
    "        mood_keywords[mood] = top_keywords\n",
    "\n",
    "# Save as mood_keywords.json\n",
    "with open(\"mood_keywords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mood_keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ mood_keywords.json generated with {len(mood_keywords)} mood labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K  # To clear memory\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, LSTM, BatchNormalization, Bidirectional, Layer,\n",
    "                                     MultiHeadAttention, LayerNormalization, Add)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# ---- Step 2: Define Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                   initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                   initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1]) # Output shape is (batch_size, embedding_dim)\n",
    "    \n",
    "# ---- Step 3: Load and Prepare Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "X_texts = [sample[\"Context\"] for sample in parsed_data]\n",
    "Y_labels = [sample[\"Response\"] for sample in parsed_data]\n",
    "\n",
    "# Group \"Context\" texts by \"Response\" label\n",
    "label_to_contexts = defaultdict(list)\n",
    "for item in parsed_data:\n",
    "    context = item.get(\"Context\", \"\")\n",
    "    response = item.get(\"Response\", \"\")\n",
    "    if context and response:\n",
    "        label_to_contexts[response].append(context.lower())\n",
    "\n",
    "# Extract keywords for each label\n",
    "mood_keywords = {}\n",
    "for label, contexts in label_to_contexts.items():\n",
    "    all_text = \" \".join(contexts)\n",
    "    words = word_tokenize(all_text)\n",
    "    cleaned_words = [\n",
    "        w.lower() for w in words\n",
    "        if w.isalpha() and w.lower() not in stop_words\n",
    "    ]\n",
    "    word_freq = Counter(cleaned_words)\n",
    "    top_keywords = [word for word, _ in word_freq.most_common(20)]  # More keywords for broader intent detection\n",
    "    mood_keywords[label] = top_keywords\n",
    "\n",
    "# Save to mood_keywords.json\n",
    "with open(\"mood_keywords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mood_keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ mood_keywords.json generated and saved successfully!\")\n",
    "\n",
    "# ---- Step 4: Define SBERT Variants to Test ----\n",
    "sbert_variants = [\n",
    "    'paraphrase-MiniLM-L12-v2'\n",
    "]\n",
    "#',\n",
    "#,     'all-MiniLM-L6-v2',\n",
    "# ---- Step 5: Encode Labels ----\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded_labels = label_encoder.fit_transform(Y_labels)\n",
    "num_classes_original = len(label_encoder.classes_)\n",
    "Y_encoded = to_categorical(Y_encoded_labels, num_classes=num_classes_original)\n",
    "\n",
    "# ---- Step 6: Save Label Encoder ----\n",
    "with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# ---- Step 7: Remove Rare Classes (only 1 sample) ----\n",
    "class_counts = Counter(np.argmax(Y_encoded, axis=1))\n",
    "valid_classes = {cls for cls, count in class_counts.items() if count > 1}\n",
    "valid_indices = [i for i, label in enumerate(np.argmax(Y_encoded, axis=1)) if label in valid_classes]\n",
    "\n",
    "X_texts_filtered = [X_texts[i] for i in valid_indices]\n",
    "Y_filtered = Y_encoded[valid_indices]\n",
    "Y_filtered_encoded_labels = np.argmax(Y_filtered, axis=1) # Get integer labels after filtering\n",
    "\n",
    "filtered_class_counts = Counter(Y_filtered_encoded_labels)\n",
    "min_samples_per_class = min(filtered_class_counts.values())\n",
    "print(f\"üìä Filtered Class Distribution: {filtered_class_counts}\")\n",
    "\n",
    "# ---- Step 8: Prepare Embeddings and Split Data ----\n",
    "sbert_model_name = sbert_variants[0] # Using the first variant for this non-function block\n",
    "print(f\"\\nüîÑ Preparing embeddings with SBERT model: {sbert_model_name}\")\n",
    "sbert = SentenceTransformer(sbert_model_name)\n",
    "X_emb = sbert.encode(X_texts_filtered)\n",
    "Y_filtered_encoded_labels_step8 = np.argmax(Y_filtered, axis=1) # Get integer labels after filtering\n",
    "\n",
    "# Resample using SMOTE + TomekLinks if safe\n",
    "if min_samples_per_class < 2:\n",
    "    print(\"‚ö†Ô∏è SMOTE skipped due to classes with <2 samples.\")\n",
    "    X_resampled, Y_resampled_encoded = X_emb, Y_filtered_encoded_labels_step8\n",
    "else:\n",
    "    smote_k = min(5, min_samples_per_class - 1)\n",
    "    smote_k = max(smote_k, 1)\n",
    "    print(f\"‚úÖ Applying SMOTE with k={smote_k} and TomekLinks...\")\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote_encoded = smote.fit_resample(X_emb, Y_filtered_encoded_labels_step8)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_encoded = smote_tomek.fit_resample(X_smote, Y_smote_encoded)\n",
    "\n",
    "unique_resampled_labels = np.unique(Y_resampled_encoded)\n",
    "current_num_classes = len(unique_resampled_labels)\n",
    "\n",
    "# --- FIX: Ensure Y_resampled_encoded values are within the valid range ---\n",
    "Y_resampled_encoded = np.clip(Y_resampled_encoded, 0, current_num_classes - 1)\n",
    "\n",
    "Y_resampled = to_categorical(Y_resampled_encoded, num_classes=current_num_classes)\n",
    "print(f\"üìä Resampled Class Distribution: {Counter(np.argmax(Y_resampled, axis=1))}\")\n",
    "print(f\"üî¢ Number of classes after resampling: {current_num_classes}\")\n",
    "\n",
    "# Train-Test Split (75% train, 25% test)\n",
    "print(f\"‚û°Ô∏è Before train_test_split: test_size = 0.25\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled,\n",
    "    test_size=0.25,  # 25% for the test set\n",
    "    random_state=42,\n",
    "    stratify=np.argmax(Y_resampled, axis=1)\n",
    ")\n",
    "print(f\"‚úÖ After train_test_split: Training size = {len(X_train)}, Testing size = {len(X_test)}\")\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(Y_train_int),\n",
    "    y=Y_train_int\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"üìè Training data size: {len(X_train)}\")\n",
    "print(f\"üìè Testing data size: {len(X_test)}\")\n",
    "# ---- Step 9: Build Model ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout_rate=0.3):\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention_output = Dropout(dropout_rate)(attention_output)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention_output]))\n",
    "\n",
    "        ff_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "        ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "        ff_output = Dropout(dropout_rate)(ff_output)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff_output]))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_block(inputs)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=0.002,\n",
    "        decay_steps=1200,\n",
    "        decay_rate=0.97\n",
    "    )\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=current_num_classes)\n",
    "\n",
    "# ---- Step 10: Train Model ----\n",
    "print(f\"\\nüöÄ Training model using: {sbert_model_name}\")\n",
    "\n",
    "# üßπ Clear session\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, verbose=1, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train_int,  # Use integer labels for training\n",
    "    validation_data=(X_test, Y_test_int),  # Use integer labels for validation\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[reduce_lr, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---- Step 11: Evaluate and Save Model ----\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test_int, verbose=1)  # Use integer labels for evaluation\n",
    "Y_pred_probs = model.predict(X_test, verbose=0)\n",
    "Y_pred = np.argmax(Y_pred_probs, axis=1)\n",
    "print(f\"‚úÖ Test Accuracy with {sbert_model_name}: {test_acc * 100:.2f}%\")\n",
    "model_filename = f\"model_{sbert_model_name.replace('-', '_')}.keras\"\n",
    "if os.path.exists(model_filename):\n",
    "    os.remove(model_filename)\n",
    "model.save(model_filename)\n",
    "print(f\"üíæ Model saved as {model_filename}\")\n",
    "\n",
    "# --- Optional: Plot training history ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Step 12: Save Model as Final ----\n",
    "final_model_name = \"optimized_lstm_model.keras\"\n",
    "if os.path.exists(final_model_name):\n",
    "    try:\n",
    "        previous_model = load_model(final_model_name, custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "        _, prev_acc = previous_model.evaluate(X_test, Y_test_int, verbose=1)\n",
    "        if test_acc > prev_acc:\n",
    "            os.replace(model_filename, final_model_name)\n",
    "            print(f\"‚úÖ Final model updated ‚Üí Accuracy improved from {prev_acc*100:.2f}% ‚Üí {test_acc*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Final model not updated (existing model has higher accuracy: {prev_acc*100:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load existing model. Replacing with new one. Error: {e}\")\n",
    "        os.replace(model_filename, final_model_name)\n",
    "else:\n",
    "    os.replace(model_filename, final_model_name)\n",
    "    print(f\"‚úÖ Final model saved as {final_model_name}\")\n",
    "\n",
    "f1 = f1_score(Y_test_int, Y_pred, average=\"weighted\")\n",
    "print(f\"‚úÖ Test Accuracy with {sbert_model_name}: {test_acc * 100:.2f}%\")\n",
    "print(f\"üéØ Weighted F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# NLTK setup\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load combined_dataset.json\n",
    "with open(\"combined_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Load previously generated mood keywords (from context analysis)\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    context_mood_keywords = json.load(f)\n",
    "\n",
    "# Helper: Detect mood from context using context-based keywords\n",
    "def detect_mood(text, mood_keywords):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "# Step 1: Collect grouped text by mood\n",
    "mood_to_contexts = defaultdict(list)\n",
    "mood_to_responses = defaultdict(list)\n",
    "\n",
    "for sample in parsed_data:\n",
    "    context = sample.get(\"Context\", \"\").lower()\n",
    "    response = sample.get(\"Response\", \"\").lower()\n",
    "    detected_mood = detect_mood(context, context_mood_keywords)\n",
    "    if detected_mood:\n",
    "        if context:  mood_to_contexts[detected_mood].append(context)\n",
    "        if response: mood_to_responses[detected_mood].append(response)\n",
    "\n",
    "# Step 2: Extract keywords from both context & response\n",
    "def extract_keywords(texts, top_n=15):\n",
    "    all_text = \" \".join(texts)\n",
    "    tokens = word_tokenize(all_text)\n",
    "    words = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
    "    freq = Counter(words)\n",
    "    return [word for word, _ in freq.most_common(top_n)]\n",
    "\n",
    "combined_keywords = {}\n",
    "\n",
    "all_moods = set(mood_to_contexts) | set(mood_to_responses)\n",
    "\n",
    "for mood in all_moods:\n",
    "    context_keywords = extract_keywords(mood_to_contexts[mood]) if mood in mood_to_contexts else []\n",
    "    response_keywords = extract_keywords(mood_to_responses[mood]) if mood in mood_to_responses else []\n",
    "    combined_keywords[mood] = list(set(context_keywords + response_keywords))\n",
    "\n",
    "# Step 3: Save combined mood keyword file\n",
    "with open(\"mood_keywords_combined.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(combined_keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ mood_keywords_combined.json generated successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pickle, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.nn import softmax\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "from datetime import datetime\n",
    "\n",
    "# ---- Custom Attention Layer for loading the model ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        super().build(input_shape)\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# ---- Paths and Setup ----\n",
    "model_path = \"mood_model_combined.keras\"\n",
    "encoder_path = \"mood_encoder.pkl\"\n",
    "sbert_model_name = \"paraphrase-MiniLM-L12-v2\"\n",
    "log_path = \"chat_log.json\"\n",
    "\n",
    "# ---- Load Model and Encoder ----\n",
    "print(\"üîÑ Loading mood classifier model...\")\n",
    "model = load_model(model_path, custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "with open(encoder_path, \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "sbert = SentenceTransformer(sbert_model_name)\n",
    "\n",
    "# ---- Fallback Prompts ----\n",
    "fallback_responses = [\n",
    "    \"I'm not sure I understood. Could you tell me more about how you feel?\",\n",
    "    \"That sounds important. Could you elaborate a bit?\",\n",
    "    \"Thanks for sharing. Could you explain it a little more?\"\n",
    "]\n",
    "\n",
    "# ---- Ensure Chat Log Exists ----\n",
    "if not os.path.exists(log_path):\n",
    "    with open(log_path, \"w\") as f:\n",
    "        json.dump([], f)\n",
    "\n",
    "# ---- Prediction Function ----\n",
    "def predict_mood(text):\n",
    "    emb = sbert.encode([text])\n",
    "    emb = np.expand_dims(emb, axis=1)  # Reshape for model input\n",
    "    probs = softmax(model.predict(emb, verbose=0)[0]).numpy()\n",
    "    top_idx = np.argmax(probs)\n",
    "    top_label = label_encoder.inverse_transform([top_idx])[0]\n",
    "    confidence = probs[top_idx]\n",
    "    return top_label, confidence, probs\n",
    "\n",
    "# ---- Log Interaction ----\n",
    "def log_interaction(user_input, mood, confidence):\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"input\": user_input,\n",
    "        \"predicted_mood\": mood,\n",
    "        \"confidence\": float(confidence)\n",
    "    }\n",
    "    with open(log_path, \"r+\") as f:\n",
    "        logs = json.load(f)\n",
    "        logs.append(entry)\n",
    "        f.seek(0)\n",
    "        json.dump(logs, f, indent=2)\n",
    "\n",
    "# ---- Chat Loop ----\n",
    "print(\"üí¨ MoodBot is ready. Type 'exit' to quit.\")\n",
    "while True:\n",
    "    user_input = input(\"üß† You: \").strip()\n",
    "    if user_input.lower() in {\"exit\", \"quit\"}:\n",
    "        print(\"üëã Goodbye! Take care.\")\n",
    "        break\n",
    "\n",
    "    predicted_mood, conf, prob = predict_mood(user_input)\n",
    "    if conf < 0.45:\n",
    "        response = np.random.choice(fallback_responses)\n",
    "        print(f\"ü§ñ Bot: {response}\")\n",
    "    else:\n",
    "        print(f\"ü§ñ Detected Mood: {predicted_mood} ({conf:.2f} confidence)\")\n",
    "        print(f\"üì© Prompt: How does this mood affect your day?\")\n",
    "\n",
    "    log_interaction(user_input, predicted_mood, conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
