{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras nltk scikit-learn transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall transformers -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers tensorflow torch scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install keras==2.13.1  transformers==4.33.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow keras transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras==3.0.5 transformers==4.38.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Model\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras import backend as K  # To clear memory\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, LSTM, BatchNormalization, Bidirectional, Layer,\n",
    "                                     MultiHeadAttention, LayerNormalization, Add , Input)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ---- Step 2: Define Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                   initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                   initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1]) # Output shape is (batch_size, embedding_dim)\n",
    "\n",
    "# ---- Step 3: Load and Prepare Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "X_texts = [sample[\"Context\"] for sample in parsed_data]\n",
    "Y_labels = [sample[\"Response\"] for sample in parsed_data]\n",
    "\n",
    "# ---- Step 4: Define SBERT Variants to Test ----\n",
    "sbert_variants = [\n",
    "    'paraphrase-MiniLM-L12-v2'\n",
    "]\n",
    "#',\n",
    "#,     'all-MiniLM-L6-v2',\n",
    "embedding_path = \"X_emb.npy\"\n",
    "label_path = \"Y_encoded.npy\"\n",
    "# ---- Step 5: Encode Labels ----\n",
    "if os.path.exists(embedding_path) and os.path.exists(label_path):\n",
    "    print(\"✅ Loading saved embeddings...\")\n",
    "    X_emb = np.load(embedding_path)\n",
    "    Y_encoded = np.load(label_path)\n",
    "    with open(\"response_encoder.pkl\", \"rb\") as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "else:\n",
    "    print(\"🔄 Generating SBERT embeddings...\")\n",
    "    sbert = SentenceTransformer(sbert_model_name)\n",
    "    X_emb = sbert.encode(augmented_inputs, show_progress_bar=True)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_encoded_labels = label_encoder.fit_transform(Y_labels)\n",
    "    num_classes_original = len(label_encoder.classes_)\n",
    "    Y_encoded = to_categorical(Y_encoded_labels, num_classes=num_classes_original)\n",
    "    np.save(embedding_path, X_emb)\n",
    "    np.save(label_path, Y_encoded)\n",
    "    with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "\n",
    "\n",
    "# ---- Step 6: Save Label Encoder ----\n",
    "with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# ---- Step 7: Remove Rare Classes (only 1 sample) ----\n",
    "class_counts = Counter(np.argmax(Y_encoded, axis=1))\n",
    "valid_classes = {cls for cls, count in class_counts.items() if count > 1}\n",
    "valid_indices = [i for i, label in enumerate(np.argmax(Y_encoded, axis=1)) if label in valid_classes]\n",
    "\n",
    "X_texts_filtered = [X_texts[i] for i in valid_indices]\n",
    "# Re-extract filtered string labels\n",
    "Y_labels_filtered = [Y_labels[i] for i in valid_indices]\n",
    "\n",
    "# Re-encode using a new LabelEncoder for filtered classes\n",
    "label_encoder = LabelEncoder()\n",
    "Y_filtered_int = label_encoder.fit_transform(Y_labels_filtered)\n",
    "\n",
    "# Save new encoder\n",
    "with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Now one-hot encode with correct number of classes\n",
    "current_num_classes = len(label_encoder.classes_)\n",
    "Y_filtered = to_categorical(Y_filtered_int, num_classes=current_num_classes)\n",
    "\n",
    "# ---- Step 8: Prepare Embeddings and Split Data ----\n",
    "sbert_model_name = sbert_variants[0] # Using the first variant for this non-function block\n",
    "print(f\"\\n🔄 Preparing embeddings with SBERT model: {sbert_model_name}\")\n",
    "sbert = SentenceTransformer(sbert_model_name)\n",
    "X_emb = sbert.encode(X_texts_filtered)\n",
    "Y_filtered_encoded_labels_step8 = np.argmax(Y_filtered, axis=1) # Get integer labels after filtering\n",
    "\n",
    "# Resample using SMOTE + TomekLinks if safe\n",
    "if min_samples_per_class < 2:\n",
    "    print(\"⚠️ SMOTE skipped due to classes with <2 samples.\")\n",
    "    X_resampled, Y_resampled_encoded = X_emb, Y_filtered_encoded_labels_step8\n",
    "else:\n",
    "    smote_k = min(5, min_samples_per_class - 1)\n",
    "    smote_k = max(smote_k, 1)\n",
    "    print(f\"✅ Applying SMOTE with k={smote_k} and TomekLinks...\")\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote_encoded = smote.fit_resample(X_emb, Y_filtered_encoded_labels_step8)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_encoded = smote_tomek.fit_resample(X_smote, Y_smote_encoded)\n",
    "\n",
    "unique_resampled_labels = np.unique(Y_resampled_encoded)\n",
    "current_num_classes = len(unique_resampled_labels)\n",
    "\n",
    "# --- FIX: Ensure Y_resampled_encoded values are within the valid range ---\n",
    "Y_resampled_encoded = np.clip(Y_resampled_encoded, 0, current_num_classes - 1)\n",
    "\n",
    "Y_resampled = to_categorical(Y_resampled_encoded, num_classes=current_num_classes)\n",
    "print(f\"📊 Resampled Class Distribution: {Counter(np.argmax(Y_resampled, axis=1))}\")\n",
    "print(f\"🔢 Number of classes after resampling: {current_num_classes}\")\n",
    "\n",
    "# Train-Test Split (75% train, 25% test)\n",
    "print(f\"➡️ Before train_test_split: test_size = 0.25\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled,\n",
    "    test_size=0.25,  # 25% for the test set\n",
    "    random_state=42,\n",
    "    stratify=np.argmax(Y_resampled, axis=1)\n",
    ")\n",
    "print(f\"✅ After train_test_split: Training size = {len(X_train)}, Testing size = {len(X_test)}\")\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(Y_train_int),\n",
    "    y=Y_train_int\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"📏 Training data size: {len(X_train)}\")\n",
    "print(f\"📏 Testing data size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 9: Build Model ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout_rate=0.3):\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention_output = Dropout(dropout_rate)(attention_output)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention_output]))\n",
    "\n",
    "        ff_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "        ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "        ff_output = Dropout(dropout_rate)(ff_output)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff_output]))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_block(inputs)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=0.0005,\n",
    "        decay_steps=1100,\n",
    "        decay_rate=0.95\n",
    "    )\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=AdamW(learning_rate=lr_schedule), metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=current_num_classes)\n",
    "\n",
    "# ---- Step 10: Train Model ----\n",
    "print(f\"\\n🚀 Training model using: {sbert_model_name}\")\n",
    "\n",
    "# 🧹 Clear session\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, verbose=1, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train_int,  # Use integer labels for training\n",
    "    validation_data=(X_test, Y_test_int),  # Use integer labels for validation\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---- Step 11: Evaluate and Save Model ----\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test_int, verbose=1)  # Use integer labels for evaluation\n",
    "print(f\"✅ Test Accuracy with {sbert_model_name}: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# ---- Custom Attention Layer (required for model loading) ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# ---- File Paths ----\n",
    "MODEL_PATH = \"optimized_lstm_model.keras\"\n",
    "LABEL_ENCODER_PATH = \"response_encoder.pkl\"\n",
    "QUESTIONNAIRE_PATH = \"generated_questionnaire.json\"\n",
    "MOOD_KEYWORDS_PATH = \"mood_keywords.json\"\n",
    "SBERT_MODEL_NAME = \"paraphrase-MiniLM-L12-v2\"\n",
    "\n",
    "# ---- Load Assets ----\n",
    "model = load_model(MODEL_PATH, custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "\n",
    "with open(LABEL_ENCODER_PATH, \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "with open(QUESTIONNAIRE_PATH, \"r\") as f:\n",
    "    questionnaire = json.load(f)\n",
    "\n",
    "with open(MOOD_KEYWORDS_PATH, \"r\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "sbert = SentenceTransformer(SBERT_MODEL_NAME)\n",
    "\n",
    "# ---- Helper Functions ----\n",
    "def augment_input_with_mood_keywords(user_input):\n",
    "    tokens = []\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(word.lower() in user_input.lower() for word in keywords):\n",
    "            tokens.append(f\"intent:{mood.lower()}\")\n",
    "    return user_input + \" \" + \" \".join(tokens)\n",
    "\n",
    "def predict_response(user_input):\n",
    "    \"\"\"Return full response from the model\"\"\"\n",
    "    augmented = augment_input_with_mood_keywords(user_input)\n",
    "    embedding = sbert.encode([augmented])\n",
    "    embedding = np.expand_dims(embedding, axis=1)\n",
    "    prediction = model.predict(embedding, verbose=0)\n",
    "    pred_index = np.argmax(prediction, axis=1)[0]\n",
    "    return label_encoder.inverse_transform([pred_index])[0]\n",
    "\n",
    "def detect_mood_from_response(response_text):\n",
    "    \"\"\"Infer mood category from keywords in the model response\"\"\"\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(word.lower() in response_text.lower() for word in keywords):\n",
    "            return mood\n",
    "    return \"Unknown\"\n",
    "\n",
    "def get_question_for_mood(mood):\n",
    "    \"\"\"Select a question based on detected mood\"\"\"\n",
    "    for category, questions in questionnaire.items():\n",
    "        if mood.lower() in category.lower():\n",
    "            return random.choice(questions)\n",
    "    return \"Sorry, I couldn't find a relevant question for you.\"\n",
    "\n",
    "# ---- Chat Loop ----\n",
    "def chatbot():\n",
    "    print(\"🧠 Mental Health Bot: Hello! I'm here to support you. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in {\"exit\", \"quit\"}:\n",
    "            print(\"🧠 Bot: Take care and stay safe! 💙\")\n",
    "            break\n",
    "\n",
    "        full_response = predict_response(user_input)\n",
    "        detected_mood = detect_mood_from_response(full_response)\n",
    "        question = get_question_for_mood(detected_mood)\n",
    "\n",
    "        print(f\"\\n🧠 Bot (Mood Detected: {detected_mood})\")\n",
    "        print(f\"→ Response: {full_response}\")\n",
    "        print(f\"→ Follow-up Question: {question}\\n\")\n",
    "\n",
    "# ---- Run ----\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Imports ----\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, LSTM, BatchNormalization,\n",
    "                                     Bidirectional, MultiHeadAttention, LayerNormalization,\n",
    "                                     Add, GaussianNoise, Layer)\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# ---- Step 1: Define Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "\n",
    "# ---- Step 2: Load and Prepare Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "X_texts = [sample[\"Context\"] for sample in parsed_data]\n",
    "Y_labels = [sample[\"Response\"] for sample in parsed_data]\n",
    "\n",
    "\n",
    "# # ---- Step 3: Label Encoding ----\n",
    "# label_encoder = LabelEncoder()\n",
    "# Y_encoded_labels = label_encoder.fit_transform(Y_labels)\n",
    "# num_classes_original = len(label_encoder.classes_)\n",
    "# Y_encoded = to_categorical(Y_encoded_labels, num_classes=num_classes_original)\n",
    "\n",
    "# # Save label encoder\n",
    "# with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(label_encoder, f)\n",
    "\n",
    "\n",
    "# # ---- Step 4: Remove Rare Classes ----\n",
    "# class_counts = Counter(np.argmax(Y_encoded, axis=1))\n",
    "# valid_classes = {cls for cls, count in class_counts.items() if count > 1}\n",
    "# valid_indices = [i for i, y in enumerate(np.argmax(Y_encoded, axis=1)) if y in valid_classes]\n",
    "\n",
    "# X_texts_filtered = [X_texts[i] for i in valid_indices]\n",
    "# Y_filtered = Y_encoded[valid_indices]\n",
    "# Y_filtered_encoded_labels = np.argmax(Y_filtered, axis=1)\n",
    "# min_samples_per_class = min(Counter(Y_filtered_encoded_labels).values())\n",
    "\n",
    "# print(f\"📊 Filtered Class Distribution: {Counter(Y_filtered_encoded_labels)}\")\n",
    "\n",
    "\n",
    "label_counts = Counter(Y_labels)\n",
    "valid_labels = {label for label, count in label_counts.items() if count > 1}\n",
    "filtered_data = [(x, y) for x, y in zip(X_texts, Y_labels) if y in valid_labels]\n",
    "\n",
    "X_texts_filtered = [x for x, _ in filtered_data]\n",
    "Y_labels_filtered = [y for _, y in filtered_data]\n",
    "\n",
    "# Refit label encoder ONLY on filtered labels\n",
    "label_encoder = LabelEncoder()\n",
    "Y_filtered_encoded = label_encoder.fit_transform(Y_labels_filtered)\n",
    "current_num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Save updated encoder\n",
    "with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "min_samples_per_class = min(Counter(Y_filtered_encoded))\n",
    "# ---- Step 5: SBERT Embeddings ----\n",
    "sbert_model_name = 'paraphrase-MiniLM-L12-v2'\n",
    "print(f\"\\n🔄 Generating SBERT embeddings with {sbert_model_name}\")\n",
    "sbert = SentenceTransformer(sbert_model_name)\n",
    "X_emb = sbert.encode(X_texts_filtered, show_progress_bar=True)\n",
    "\n",
    "\n",
    "# ---- Step 6: Resampling with SMOTE + TomekLinks ----\n",
    "if min_samples_per_class < 2:\n",
    "    print(\"⚠️ SMOTE skipped due to classes with <2 samples.\")\n",
    "    X_resampled, Y_resampled_encoded = X_emb, Y_filtered_encoded\n",
    "else:\n",
    "    smote_k = max(min(5, min_samples_per_class - 1), 1)\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote_encoded = smote.fit_resample(X_emb, Y_filtered_encoded)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_encoded = smote_tomek.fit_resample(X_smote, Y_smote_encoded)\n",
    "\n",
    "Y_resampled_encoded = np.clip(Y_resampled_encoded, 0, current_num_classes - 1)\n",
    "Y_resampled = to_categorical(Y_resampled_encoded, num_classes=current_num_classes)\n",
    "\n",
    "print(f\"📊 Resampled Class Distribution: {Counter(np.argmax(Y_resampled, axis=1))}\")\n",
    "print(f\"🔢 Classes after resampling: {current_num_classes}\")\n",
    "\n",
    "if len(Y_resampled) * 0.25 >= current_num_classes:\n",
    "    stratify_option = np.argmax(Y_resampled, axis=1)\n",
    "else:\n",
    "    print(f\"⚠️ Skipping stratify: test size too small for {current_num_classes} classes.\")\n",
    "    stratify_option = None\n",
    "\n",
    "#Training \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled,\n",
    "    test_size=0.25,\n",
    "    random_state=42,\n",
    "    stratify=stratify_option\n",
    ")\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(Y_train_int), y=Y_train_int)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"📏 Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "\n",
    "# ---- Step 8: Build Model ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout_rate=0.3):\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention_output = Dropout(dropout_rate)(attention_output)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention_output]))\n",
    "        ff_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "        ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "        ff_output = Dropout(dropout_rate)(ff_output)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff_output]))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_block(inputs)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    lr_schedule = ExponentialDecay(initial_learning_rate=0.002, decay_steps=1200, decay_rate=0.97)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---- Step 9: Train Model ----\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=current_num_classes)\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, verbose=1, min_lr=1e-6)\n",
    "\n",
    "print(f\"\\n🚀 Training model with: {sbert_model_name}\")\n",
    "history = model.fit(\n",
    "    X_train, Y_train_int,\n",
    "    validation_data=(X_test, Y_test_int),\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Step 10: Evaluate and Save Model ----\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test_int, verbose=1)\n",
    "print(f\"✅ Test Accuracy with {sbert_model_name}: {test_acc * 100:.2f}%\")\n",
    "\n",
    "model_filename = f\"model_{sbert_model_name.replace('-', '_')}.keras\"\n",
    "if os.path.exists(model_filename):\n",
    "    os.remove(model_filename)\n",
    "model.save(model_filename)\n",
    "print(f\"💾 Model saved as {model_filename}\")\n",
    "\n",
    "\n",
    "# ---- Step 11: Plot Training Curves ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---- Step 12: Save Final Model Based on Accuracy ----\n",
    "final_model_name = \"optimized_lstm_model.keras\"\n",
    "if os.path.exists(final_model_name):\n",
    "    try:\n",
    "        prev_model = load_model(final_model_name, custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "        _, prev_acc = prev_model.evaluate(X_test, Y_test, verbose=1)\n",
    "        if test_acc > prev_acc:\n",
    "            os.replace(model_filename, final_model_name)\n",
    "            print(f\"✅ Final model updated: Accuracy improved from {prev_acc:.4f} → {test_acc:.4f}\")\n",
    "        else:\n",
    "            print(f\"⚠️ Existing model retained: Accuracy {prev_acc:.4f} is better or equal.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Couldn't load previous model: {e}\")\n",
    "        os.replace(model_filename, final_model_name)\n",
    "        print(f\"✅ Final model saved.\")\n",
    "else:\n",
    "    os.rename(model_filename, final_model_name)\n",
    "    print(f\"✅ Final model saved as {final_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Filtered Class Distribution: Counter({0: 4, 583: 3, 2089: 3, 1887: 3, 1697: 3, 1316: 3, 614: 3, 1171: 3, 1953: 3, 1404: 3, 1954: 3, 2301: 3, 1263: 3, 2304: 3, 2183: 3, 1440: 3, 522: 3, 1299: 3, 1635: 3, 749: 3, 1502: 3, 1647: 3, 1646: 3, 1989: 3, 625: 3, 827: 3, 429: 3, 413: 3, 1203: 3, 1032: 3, 1986: 3, 250: 3, 1219: 3, 1212: 3, 764: 3, 2060: 3, 1473: 2, 1284: 2, 2366: 2, 67: 2, 64: 2, 2051: 2, 778: 2, 66: 2, 1531: 2, 1463: 2, 2006: 2, 797: 2, 2288: 2, 1251: 2, 1567: 2, 411: 2, 62: 2, 1562: 2, 1912: 2, 1441: 2, 52: 2, 2007: 2, 1581: 2, 2357: 2, 2322: 2, 1806: 2, 1981: 2, 1980: 2, 2351: 2, 1936: 2, 2154: 2, 1584: 2, 1302: 2, 63: 2, 518: 2, 1352: 2, 529: 2, 2361: 2, 1805: 2, 1572: 2, 2321: 2, 952: 2, 1948: 2, 1008: 2, 539: 2, 1571: 2, 340: 2, 1609: 2, 1432: 2, 396: 2, 2228: 2, 422: 2, 1191: 2, 1688: 2, 2030: 2, 2084: 2, 1361: 2, 1486: 2, 1607: 2, 1719: 2, 208: 2, 1683: 2, 1990: 2, 450: 2, 1160: 2, 1807: 2, 217: 2, 1323: 2, 2409: 2, 1638: 2, 1371: 2, 1290: 2, 1321: 2, 434: 2, 420: 2, 374: 2, 968: 2, 106: 2, 473: 2, 469: 2, 1158: 2, 1053: 2, 1862: 2, 1753: 2, 2208: 2, 837: 2, 2451: 2, 603: 2, 2141: 2, 329: 2, 1497: 2, 1241: 2, 600: 2, 1228: 2, 1101: 2, 620: 2, 1373: 2, 1280: 2, 134: 2, 372: 2, 1485: 2, 1605: 2, 410: 2, 33: 2, 2446: 2, 1376: 2, 607: 2, 1327: 2, 242: 2, 937: 2, 586: 2, 2065: 2, 1137: 2, 921: 2, 31: 2, 2329: 2, 1721: 2, 1731: 2, 682: 2, 1229: 2, 1112: 2, 421: 2, 1105: 2, 83: 2, 2235: 2, 1184: 2, 1888: 2, 720: 2, 559: 2, 339: 2, 1170: 2, 2211: 2, 1161: 2, 180: 2, 1963: 2, 1094: 2, 1330: 2, 1380: 2, 1860: 2, 2132: 2, 1143: 2, 1037: 2, 1173: 2, 1296: 2, 1610: 2, 1643: 2, 1350: 2, 1353: 2, 1268: 2, 2204: 2, 409: 2, 2088: 2, 124: 2, 1974: 2, 2207: 2, 1654: 2, 978: 2, 866: 2, 1446: 2, 111: 2, 1517: 2, 701: 2, 2375: 2, 657: 2, 668: 2, 1573: 2, 461: 2, 1093: 2, 2433: 2, 232: 2, 271: 2, 348: 2, 1372: 2, 1348: 2, 1354: 2, 685: 2, 853: 2, 505: 2, 1165: 2, 1418: 2, 1857: 2, 237: 2, 113: 2, 24: 2, 288: 2, 426: 2, 1120: 2, 666: 2, 2384: 2, 1504: 2, 957: 2, 2016: 2, 1506: 2, 1717: 2, 982: 2, 1716: 2, 2243: 2, 1698: 2, 397: 2, 1408: 2, 1244: 2, 762: 2, 1812: 2, 1782: 2, 2218: 2, 1453: 2, 1252: 2, 418: 2, 202: 2, 2461: 2, 1261: 2, 14: 2, 337: 2, 1044: 2, 495: 2, 1356: 2, 241: 2, 105: 2, 193: 2, 1293: 2, 104: 2, 1968: 2, 2264: 2, 2281: 2, 498: 2, 929: 2, 1591: 2, 261: 2, 1292: 2, 2303: 2, 1283: 2, 2232: 2, 1492: 2, 1248: 2, 239: 2, 948: 2, 1383: 2, 1656: 2, 1295: 2, 2023: 2, 1062: 2, 2455: 2, 402: 2, 1746: 2, 1893: 2, 554: 2, 1601: 2, 347: 2, 2427: 2, 346: 2, 2027: 2, 187: 2, 582: 2, 843: 2, 406: 2, 2087: 2, 443: 2, 615: 2, 899: 2, 2426: 2, 890: 2, 2216: 2, 598: 2, 943: 2, 845: 2, 304: 2, 1770: 2, 1204: 2, 1064: 2, 1519: 2, 78: 2, 2234: 2, 175: 2, 307: 2, 206: 2, 1351: 2, 2025: 2, 2464: 2, 1696: 2, 584: 2, 1630: 2, 2063: 2, 1865: 2, 163: 2, 2240: 2, 2005: 2, 1066: 2, 1757: 2, 1220: 2, 1734: 2, 1011: 2, 1078: 2, 1225: 2, 1631: 2, 1326: 2, 702: 2, 1702: 2, 1465: 2, 842: 2, 713: 2, 656: 2, 1849: 2, 2129: 2, 1110: 2, 747: 2, 1305: 2, 1602: 2, 1322: 2, 1362: 2, 1382: 2, 1760: 2, 1163: 2, 1741: 2, 811: 2, 1444: 2, 2271: 2, 1629: 2, 35: 2, 1804: 2, 1459: 2, 2360: 2, 1451: 2, 1583: 2, 277: 2, 1424: 2, 458: 2, 2350: 2, 1944: 2, 2202: 2, 875: 2, 1169: 2, 2404: 2, 855: 2, 1107: 2, 229: 2, 2180: 2, 1309: 2, 1590: 2, 1095: 2, 336: 2, 2317: 2, 1947: 2, 65: 2, 781: 2, 613: 2, 2146: 2, 618: 2, 2399: 2, 1178: 2, 118: 2, 2302: 2, 2000: 2, 1738: 2, 218: 2, 2124: 2, 1439: 2, 1150: 2, 1490: 2, 1748: 2, 427: 2, 980: 2, 852: 2, 1858: 2, 135: 2, 658: 2, 648: 2, 680: 2, 475: 2, 655: 2, 694: 2, 2408: 2, 2158: 2, 933: 2, 1890: 2, 1343: 2, 532: 2, 2040: 2, 2036: 2, 681: 2, 1334: 2, 119: 2, 642: 2, 215: 2, 619: 2, 133: 2, 335: 2, 1729: 2, 379: 2, 958: 2, 2168: 2, 1384: 2, 216: 2, 1023: 2, 1253: 2, 1544: 2, 810: 2, 179: 2, 562: 2, 47: 2, 721: 2, 1931: 2, 1084: 2, 1067: 2, 669: 2, 1347: 2, 1198: 2, 1882: 2, 1747: 2, 2248: 2, 704: 2, 1988: 2, 1108: 2, 1104: 2, 257: 2, 2463: 2, 983: 2, 981: 2, 643: 2, 979: 2, 1495: 2, 1414: 2, 1388: 2, 1339: 2, 636: 2, 1673: 2, 1477: 2, 2147: 2, 1324: 2, 1997: 2, 617: 2, 552: 2, 370: 2, 1370: 2, 2359: 2, 1196: 2, 1218: 2, 1737: 2, 1499: 2, 205: 2, 1209: 2, 1369: 2, 1669: 2, 623: 2, 2379: 2, 578: 2, 441: 2, 846: 2, 936: 2, 839: 2, 2203: 2, 1958: 2, 1766: 2, 169: 2, 2440: 2, 1606: 2, 1964: 2, 1949: 2, 501: 2, 2195: 2, 2365: 2, 1797: 2, 369: 2, 1711: 2, 2407: 2, 1653: 2, 1705: 2, 759: 2, 1086: 2, 955: 2, 1815: 2, 1781: 2, 373: 2, 1022: 2, 1780: 2, 1471: 2, 1735: 2, 928: 2, 889: 2, 2265: 2, 879: 2, 330: 2, 1379: 2, 1613: 2, 430: 2, 2415: 2, 1081: 2, 1092: 2, 2231: 2, 1739: 2, 698: 2, 1660: 2, 449: 2, 1140: 2, 1612: 2, 455: 2, 2291: 2, 1558: 2, 1331: 2, 741: 2, 653: 2, 453: 2, 1358: 2, 1984: 2, 161: 2, 88: 2, 2292: 2, 946: 2, 107: 2, 90: 2, 89: 2, 976: 2, 1185: 2, 184: 2, 1467: 2, 238: 2, 667: 2, 2184: 2, 1658: 2, 2210: 2, 48: 2, 392: 2, 1129: 2, 712: 2, 1730: 2, 116: 2, 1091: 2, 380: 2, 1640: 2, 316: 2, 123: 2, 1616: 2, 2441: 2, 1645: 2, 1136: 2, 1103: 2, 69: 2, 2214: 2, 934: 2, 144: 2, 168: 2, 2385: 2, 519: 2, 221: 2, 1310: 2, 1971: 2, 1415: 2, 1920: 2, 1959: 2, 1038: 2, 1867: 2, 273: 2, 359: 2, 1445: 2, 1007: 2, 1515: 2, 1083: 2, 1036: 2, 591: 2, 1059: 2, 291: 2, 1162: 2, 2118: 2, 654: 2, 2331: 2, 2042: 2, 1063: 2, 2335: 2, 2396: 2, 1818: 2, 1464: 2, 132: 2, 1246: 2, 1117: 2, 388: 2, 2041: 2, 290: 2, 989: 2, 1994: 2, 120: 2, 306: 2, 1217: 2, 1230: 2, 122: 2, 1611: 2, 1071: 2, 34: 2, 2039: 2, 489: 2, 1034: 2, 1312: 2, 1545: 2, 1788: 2, 1689: 2, 1661: 2, 1035: 2, 401: 2, 2127: 2, 841: 2, 1073: 2, 1554: 2, 2022: 2, 136: 2, 951: 2, 1550: 2, 2316: 2, 1175: 2, 705: 2, 233: 2, 725: 2, 308: 2, 733: 2, 2450: 2, 706: 2, 1232: 2, 1033: 2, 234: 2, 986: 2, 487: 2, 734: 2, 1176: 2, 673: 2, 2448: 2, 297: 2, 1733: 2, 728: 2, 46: 2, 79: 2, 312: 2, 610: 2, 1224: 2, 684: 2, 689: 2, 700: 2, 121: 2, 683: 2, 722: 2, 1085: 2, 2229: 2, 1458: 2, 985: 2, 1113: 2, 888: 2, 2155: 2, 812: 2, 1304: 2, 1665: 2, 2431: 2, 1123: 2, 1817: 2, 1859: 2, 2199: 2, 2376: 2, 108: 2, 488: 2, 2297: 2, 459: 2, 1151: 2, 693: 2, 832: 2, 1300: 2, 1951: 2, 2101: 2, 199: 2, 2095: 2, 971: 2, 2024: 2, 2083: 2, 376: 2, 969: 2, 1449: 2, 820: 2, 2215: 2, 311: 2, 1513: 2, 326: 2, 400: 2, 2206: 2, 51: 2, 1114: 2, 1115: 2, 854: 2, 485: 2, 1425: 2, 2356: 2, 196: 2, 1355: 2, 942: 2, 1315: 2, 412: 2, 2346: 2, 87: 2, 590: 2, 2393: 2, 744: 2, 235: 2, 711: 2, 847: 2, 748: 2, 1265: 2, 1349: 2, 1019: 2, 695: 2, 1939: 2, 1825: 2, 813: 2, 809: 2, 1814: 2, 994: 2, 1333: 2, 2049: 2, 923: 2, 1821: 2, 313: 2, 2145: 2, 1827: 2, 479: 2, 154: 2, 384: 2, 2123: 2, 1272: 2, 278: 2, 1385: 2, 2213: 2, 1707: 2, 941: 2, 1468: 2, 2135: 2, 954: 2, 1423: 2, 1116: 2, 1902: 2, 1282: 2, 699: 2, 840: 2, 1884: 2, 925: 2, 525: 2, 1993: 2, 1962: 2, 1291: 2, 626: 2, 1145: 2, 1834: 2, 1413: 2, 2136: 2, 2225: 2, 1832: 2, 2035: 2, 2198: 2, 1828: 2, 1869: 2, 973: 2, 1891: 2, 1079: 2, 474: 2, 1866: 2, 707: 2, 1130: 2, 1048: 2, 16: 2, 431: 2, 1670: 2, 2163: 2, 84: 2, 2349: 2, 302: 2, 2140: 2, 225: 2, 2134: 2, 624: 2, 112: 2, 859: 2, 991: 2, 1870: 2, 1462: 2, 1177: 2, 631: 2, 1498: 2, 2411: 2, 2410: 2, 53: 2, 2167: 2, 303: 2, 1082: 2, 1637: 2, 1938: 2, 1516: 2, 167: 2, 158: 2, 1967: 2, 1960: 2, 835: 2, 806: 2, 573: 2, 433: 2, 1720: 2, 2398: 2, 1390: 2, 2258: 2, 442: 2, 2414: 2, 2093: 2, 1756: 2, 646: 2, 1494: 2, 703: 2, 445: 2, 1557: 2, 387: 2, 1496: 2, 692: 2, 1736: 2, 334: 2, 1277: 2, 1587: 2, 2370: 2, 1181: 2, 793: 2, 2128: 2, 1940: 2, 792: 2, 2131: 2, 844: 2, 2295: 2, 231: 2, 2196: 2, 2190: 2, 1106: 2, 538: 2, 1543: 2, 2261: 2, 126: 2, 2212: 2, 209: 2, 470: 2, 1141: 2, 1197: 2, 1024: 2, 953: 2, 1693: 2, 880: 2, 1180: 2, 860: 2, 1294: 2, 114: 2, 611: 2, 1476: 2, 1168: 2, 2267: 2, 856: 2, 1256: 2, 1561: 2, 775: 2, 178: 2, 659: 2, 2241: 2, 1794: 2, 1056: 2, 2156: 2, 117: 2, 1431: 2, 2283: 2, 1027: 2, 758: 2, 1133: 2, 1699: 2, 769: 2, 553: 2, 1202: 2, 2372: 2, 1903: 2, 44: 2, 1972: 2, 188: 2, 972: 2, 183: 2, 2001: 2, 1381: 2, 1075: 2, 1650: 2, 907: 2, 688: 2, 1344: 2, 2358: 2, 1099: 2, 2096: 2, 1900: 2, 1551: 2, 2320: 2, 2423: 2, 1965: 2, 417: 2, 2116: 2, 1301: 2, 130: 2, 1146: 2, 463: 2, 305: 2, 1410: 2, 2058: 2, 1240: 2, 647: 2, 1199: 2, 1634: 2, 1715: 2, 1628: 2, 1469: 2, 1723: 2, 708: 2, 2284: 2, 865: 2, 2107: 2, 2378: 2, 1021: 2, 1111: 2, 1740: 2, 1608: 2, 1226: 2, 1389: 2, 836: 2, 670: 2, 1785: 2, 1395: 2, 1853: 2, 2117: 2, 2387: 2, 300: 2, 125: 2, 1850: 2, 1505: 2, 1484: 2, 2110: 2, 1718: 2, 1340: 2, 1950: 2, 322: 2, 252: 2, 2299: 2, 249: 2, 753: 2, 2298: 2, 341: 2, 974: 2, 630: 2, 246: 2, 248: 2, 45: 2, 1532: 2, 774: 2, 612: 2, 2153: 2, 405: 2, 869: 2, 1883: 2, 1898: 2, 1644: 2, 1985: 2, 74: 2, 465: 2})\n",
      "\n",
      "🔄 Preparing embeddings with SBERT model: paraphrase-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Applying SMOTE with k=1 and TomekLinks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Resampled Class Distribution: Counter({994: 2412, 67: 4, 64: 4, 778: 4, 66: 4, 797: 4, 411: 4, 62: 4, 52: 4, 63: 4, 518: 4, 529: 4, 952: 4, 539: 4, 340: 4, 396: 4, 422: 4, 208: 4, 450: 4, 217: 4, 434: 4, 420: 4, 374: 4, 968: 4, 106: 4, 583: 4, 473: 4, 469: 4, 837: 4, 603: 4, 329: 4, 600: 4, 620: 4, 134: 4, 372: 4, 410: 4, 33: 4, 607: 4, 242: 4, 937: 4, 586: 4, 921: 4, 31: 4, 682: 4, 421: 4, 83: 4, 720: 4, 559: 4, 339: 4, 180: 4, 409: 4, 124: 4, 978: 4, 866: 4, 111: 4, 701: 4, 657: 4, 668: 4, 461: 4, 232: 4, 271: 4, 348: 4, 685: 4, 853: 4, 505: 4, 237: 4, 113: 4, 24: 4, 288: 4, 426: 4, 666: 4, 957: 4, 982: 4, 397: 4, 614: 4, 762: 4, 418: 4, 202: 4, 522: 4, 14: 4, 337: 4, 495: 4, 241: 4, 105: 4, 193: 4, 104: 4, 498: 4, 929: 4, 261: 4, 749: 4, 239: 4, 948: 4, 402: 4, 554: 4, 347: 4, 346: 4, 187: 4, 582: 4, 843: 4, 406: 4, 443: 4, 615: 4, 899: 4, 890: 4, 598: 4, 943: 4, 845: 4, 304: 4, 78: 4, 175: 4, 307: 4, 206: 4, 584: 4, 163: 4, 702: 4, 842: 4, 713: 4, 656: 4, 747: 4, 811: 4, 35: 4, 277: 4, 458: 4, 875: 4, 855: 4, 229: 4, 336: 4, 65: 4, 781: 4, 613: 4, 618: 4, 118: 4, 218: 4, 427: 4, 980: 4, 852: 4, 135: 4, 658: 4, 648: 4, 680: 4, 475: 4, 655: 4, 694: 4, 933: 4, 532: 4, 681: 4, 119: 4, 642: 4, 215: 4, 619: 4, 133: 4, 335: 4, 379: 4, 958: 4, 216: 4, 810: 4, 179: 4, 562: 4, 47: 4, 721: 4, 669: 4, 704: 4, 257: 4, 983: 4, 981: 4, 643: 4, 979: 4, 636: 4, 617: 4, 552: 4, 370: 4, 205: 4, 623: 4, 578: 4, 441: 4, 846: 4, 936: 4, 839: 4, 169: 4, 501: 4, 369: 4, 759: 4, 955: 4, 373: 4, 928: 4, 889: 4, 879: 4, 330: 4, 430: 4, 698: 4, 449: 4, 455: 4, 741: 4, 653: 4, 453: 4, 161: 4, 88: 4, 946: 4, 107: 4, 90: 4, 89: 4, 976: 4, 184: 4, 238: 4, 667: 4, 48: 4, 392: 4, 712: 4, 116: 4, 380: 4, 316: 4, 123: 4, 69: 4, 934: 4, 144: 4, 168: 4, 519: 4, 221: 4, 273: 4, 359: 4, 591: 4, 291: 4, 654: 4, 132: 4, 388: 4, 290: 4, 989: 4, 120: 4, 306: 4, 122: 4, 34: 4, 489: 4, 401: 4, 841: 4, 136: 4, 951: 4, 705: 4, 233: 4, 725: 4, 308: 4, 733: 4, 706: 4, 234: 4, 986: 4, 487: 4, 734: 4, 673: 4, 297: 4, 728: 4, 46: 4, 79: 4, 312: 4, 610: 4, 684: 4, 689: 4, 700: 4, 121: 4, 683: 4, 722: 4, 985: 4, 888: 4, 812: 4, 108: 4, 488: 4, 459: 4, 693: 4, 832: 4, 199: 4, 971: 4, 376: 4, 969: 4, 820: 4, 311: 4, 326: 4, 400: 4, 51: 4, 854: 4, 485: 4, 196: 4, 942: 4, 412: 4, 87: 4, 590: 4, 744: 4, 235: 4, 711: 4, 847: 4, 748: 4, 695: 4, 813: 4, 809: 4, 923: 4, 313: 4, 479: 4, 154: 4, 384: 4, 278: 4, 941: 4, 954: 4, 699: 4, 840: 4, 925: 4, 525: 4, 626: 4, 973: 4, 474: 4, 707: 4, 16: 4, 431: 4, 84: 4, 302: 4, 225: 4, 624: 4, 112: 4, 859: 4, 991: 4, 631: 4, 53: 4, 303: 4, 167: 4, 158: 4, 835: 4, 806: 4, 573: 4, 433: 4, 442: 4, 646: 4, 703: 4, 445: 4, 387: 4, 692: 4, 334: 4, 793: 4, 792: 4, 844: 4, 231: 4, 538: 4, 126: 4, 209: 4, 470: 4, 953: 4, 880: 4, 860: 4, 114: 4, 611: 4, 856: 4, 775: 4, 178: 4, 659: 4, 117: 4, 758: 4, 769: 4, 553: 4, 44: 4, 188: 4, 972: 4, 183: 4, 907: 4, 688: 4, 625: 4, 827: 4, 429: 4, 417: 4, 130: 4, 413: 4, 250: 4, 463: 4, 764: 4, 305: 4, 647: 4, 708: 4, 865: 4, 836: 4, 670: 4, 300: 4, 125: 4, 322: 4, 252: 4, 249: 4, 753: 4, 341: 4, 974: 4, 630: 4, 246: 4, 248: 4, 45: 4, 774: 4, 612: 4, 405: 4, 869: 4, 74: 4, 465: 4, 0: 4})\n",
      "🔢 Number of classes after resampling: 995\n",
      "➡️ Before train_test_split: test_size = 0.25\n",
      "✅ After train_test_split: Training size = 2985, Testing size = 995\n",
      "📏 Training data size: 2985\n",
      "📏 Testing data size: 995\n",
      "\n",
      "🚀 Training model using: paraphrase-MiniLM-L12-v2\n",
      "Epoch 1/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 238ms/step - accuracy: 0.1447 - loss: 8.0189 - val_accuracy: 0.6060 - val_loss: 6.4964 - learning_rate: 5.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 216ms/step - accuracy: 0.5955 - loss: 5.1105 - val_accuracy: 0.6060 - val_loss: 5.9764 - learning_rate: 5.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - accuracy: 0.6055 - loss: 4.6204 - val_accuracy: 0.6060 - val_loss: 5.6402 - learning_rate: 5.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 237ms/step - accuracy: 0.6071 - loss: 4.2231 - val_accuracy: 0.6060 - val_loss: 4.9182 - learning_rate: 5.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 211ms/step - accuracy: 0.6046 - loss: 3.9210 - val_accuracy: 0.6050 - val_loss: 4.1594 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 232ms/step - accuracy: 0.6188 - loss: 3.4557 - val_accuracy: 0.6121 - val_loss: 3.3104 - learning_rate: 5.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 229ms/step - accuracy: 0.5979 - loss: 3.3669 - val_accuracy: 0.6060 - val_loss: 2.6117 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 228ms/step - accuracy: 0.6097 - loss: 3.0622 - val_accuracy: 0.6291 - val_loss: 2.1465 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 217ms/step - accuracy: 0.6009 - loss: 2.7783 - val_accuracy: 0.6332 - val_loss: 1.8024 - learning_rate: 5.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 215ms/step - accuracy: 0.6028 - loss: 2.6332 - val_accuracy: 0.6191 - val_loss: 1.6169 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 226ms/step - accuracy: 0.5978 - loss: 2.5150 - val_accuracy: 0.6322 - val_loss: 1.4736 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 218ms/step - accuracy: 0.6249 - loss: 2.2422 - val_accuracy: 0.6382 - val_loss: 1.3953 - learning_rate: 5.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 207ms/step - accuracy: 0.6227 - loss: 2.1081 - val_accuracy: 0.6643 - val_loss: 1.2539 - learning_rate: 5.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 206ms/step - accuracy: 0.6142 - loss: 2.1487 - val_accuracy: 0.6653 - val_loss: 1.1503 - learning_rate: 5.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 239ms/step - accuracy: 0.6011 - loss: 2.0361 - val_accuracy: 0.6754 - val_loss: 1.1231 - learning_rate: 5.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 237ms/step - accuracy: 0.6367 - loss: 1.7834 - val_accuracy: 0.6925 - val_loss: 1.0245 - learning_rate: 5.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 237ms/step - accuracy: 0.6409 - loss: 1.7594 - val_accuracy: 0.6874 - val_loss: 1.0118 - learning_rate: 5.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 216ms/step - accuracy: 0.6457 - loss: 1.7168 - val_accuracy: 0.6905 - val_loss: 1.0437 - learning_rate: 5.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 229ms/step - accuracy: 0.6747 - loss: 1.5529 - val_accuracy: 0.7045 - val_loss: 1.0552 - learning_rate: 5.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m20/47\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 224ms/step - accuracy: 0.6742 - loss: 1.5329"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K  # To clear memory\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, LSTM, BatchNormalization, Bidirectional, Layer,\n",
    "                                     MultiHeadAttention, LayerNormalization, Add)\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import GaussianNoise\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# ---- Step 2: Define Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                   initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                   initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1]) # Output shape is (batch_size, embedding_dim)\n",
    "\n",
    "# ---- Step 3: Load and Prepare Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "X_texts = [sample[\"Context\"] for sample in parsed_data]\n",
    "Y_labels = [sample[\"Response\"] for sample in parsed_data]\n",
    "\n",
    "# ---- Step 4: Define SBERT Variants to Test ----\n",
    "sbert_variants = [\n",
    "    'paraphrase-MiniLM-L12-v2'\n",
    "]\n",
    "#',\n",
    "#,     'all-MiniLM-L6-v2',\n",
    "# ---- Step 5: Encode Labels ----\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded_labels = label_encoder.fit_transform(Y_labels)\n",
    "num_classes_original = len(label_encoder.classes_)\n",
    "Y_encoded = to_categorical(Y_encoded_labels, num_classes=num_classes_original)\n",
    "\n",
    "# ---- Step 6: Save Label Encoder ----\n",
    "with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# ---- Step 7: Remove Rare Classes (only 1 sample) ----\n",
    "class_counts = Counter(np.argmax(Y_encoded, axis=1))\n",
    "valid_classes = {cls for cls, count in class_counts.items() if count > 1}\n",
    "valid_indices = [i for i, label in enumerate(np.argmax(Y_encoded, axis=1)) if label in valid_classes]\n",
    "\n",
    "X_texts_filtered = [X_texts[i] for i in valid_indices]\n",
    "Y_filtered = Y_encoded[valid_indices]\n",
    "Y_filtered_encoded_labels = np.argmax(Y_filtered, axis=1) # Get integer labels after filtering\n",
    "\n",
    "filtered_class_counts = Counter(Y_filtered_encoded_labels)\n",
    "min_samples_per_class = min(filtered_class_counts.values())\n",
    "print(f\"📊 Filtered Class Distribution: {filtered_class_counts}\")\n",
    "\n",
    "# ---- Step 8: Prepare Embeddings and Split Data ----\n",
    "sbert_model_name = sbert_variants[0] # Using the first variant for this non-function block\n",
    "print(f\"\\n🔄 Preparing embeddings with SBERT model: {sbert_model_name}\")\n",
    "sbert = SentenceTransformer(sbert_model_name)\n",
    "X_emb = sbert.encode(X_texts_filtered)\n",
    "Y_filtered_encoded_labels_step8 = np.argmax(Y_filtered, axis=1) # Get integer labels after filtering\n",
    "\n",
    "# Resample using SMOTE + TomekLinks if safe\n",
    "if min_samples_per_class < 2:\n",
    "    print(\"⚠️ SMOTE skipped due to classes with <2 samples.\")\n",
    "    X_resampled, Y_resampled_encoded = X_emb, Y_filtered_encoded_labels_step8\n",
    "else:\n",
    "    smote_k = min(5, min_samples_per_class - 1)\n",
    "    smote_k = max(smote_k, 1)\n",
    "    print(f\"✅ Applying SMOTE with k={smote_k} and TomekLinks...\")\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote_encoded = smote.fit_resample(X_emb, Y_filtered_encoded_labels_step8)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_encoded = smote_tomek.fit_resample(X_smote, Y_smote_encoded)\n",
    "\n",
    "unique_resampled_labels = np.unique(Y_resampled_encoded)\n",
    "current_num_classes = len(unique_resampled_labels)\n",
    "\n",
    "# --- FIX: Ensure Y_resampled_encoded values are within the valid range ---\n",
    "Y_resampled_encoded = np.clip(Y_resampled_encoded, 0, current_num_classes - 1)\n",
    "\n",
    "Y_resampled = to_categorical(Y_resampled_encoded, num_classes=current_num_classes)\n",
    "print(f\"📊 Resampled Class Distribution: {Counter(np.argmax(Y_resampled, axis=1))}\")\n",
    "print(f\"🔢 Number of classes after resampling: {current_num_classes}\")\n",
    "\n",
    "# Train-Test Split (75% train, 25% test)\n",
    "print(f\"➡️ Before train_test_split: test_size = 0.25\")\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled,\n",
    "    test_size=0.25,  # 25% for the test set\n",
    "    random_state=42,\n",
    "    stratify=np.argmax(Y_resampled, axis=1)\n",
    ")\n",
    "print(f\"✅ After train_test_split: Training size = {len(X_train)}, Testing size = {len(X_test)}\")\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(Y_train_int),\n",
    "    y=Y_train_int\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(f\"📏 Training data size: {len(X_train)}\")\n",
    "print(f\"📏 Testing data size: {len(X_test)}\")\n",
    "# ---- Step 9: Build Model ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout_rate=0.3):\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention_output = Dropout(dropout_rate)(attention_output)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention_output]))\n",
    "\n",
    "        ff_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "        ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "        ff_output = Dropout(dropout_rate)(ff_output)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff_output]))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_block(inputs)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=0.002,\n",
    "        decay_steps=1200,\n",
    "        decay_rate=0.97\n",
    "    )\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0005),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=current_num_classes)\n",
    "\n",
    "# ---- Step 10: Train Model ----\n",
    "print(f\"\\n🚀 Training model using: {sbert_model_name}\")\n",
    "\n",
    "# 🧹 Clear session\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, verbose=1, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train_int,  # Use integer labels for training\n",
    "    validation_data=(X_test, Y_test_int),  # Use integer labels for validation\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[reduce_lr, early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---- Step 11: Evaluate and Save Model ----\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test_int, verbose=1)  # Use integer labels for evaluation\n",
    "Y_pred_probs = model.predict(X_test, verbose=0)\n",
    "Y_pred = np.argmax(Y_pred_probs, axis=1)\n",
    "print(f\"✅ Test Accuracy with {sbert_model_name}: {test_acc * 100:.2f}%\")\n",
    "model_filename = f\"model_{sbert_model_name.replace('-', '_')}.keras\"\n",
    "if os.path.exists(model_filename):\n",
    "    os.remove(model_filename)\n",
    "model.save(model_filename)\n",
    "print(f\"💾 Model saved as {model_filename}\")\n",
    "\n",
    "# --- Optional: Plot training history ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Step 12: Save Model as Final ----\n",
    "final_model_name = \"optimized_lstm_model.keras\"\n",
    "if os.path.exists(final_model_name):\n",
    "    try:\n",
    "        previous_model = load_model(final_model_name, custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "        _, prev_acc = previous_model.evaluate(X_test, Y_test, verbose=1)\n",
    "        if test_acc > prev_acc:\n",
    "            os.replace(model_filename, final_model_name)\n",
    "            print(f\"✅ Final model updated → Accuracy improved from {prev_acc*100:.2f}% → {test_acc*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"⚠️ Existing model retained → Accuracy {prev_acc*100:.2f}% is better or equal.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Couldn't load existing model: {e}\")\n",
    "        os.replace(model_filename, final_model_name)\n",
    "        print(f\"✅ Final model saved as {final_model_name}\")\n",
    "else:\n",
    "    os.rename(model_filename, final_model_name)\n",
    "    print(f\"✅ Final model saved as {final_model_name}\")\n",
    "\n",
    "\n",
    "f1 = f1_score(Y_test_int, Y_pred, average=\"weighted\")\n",
    "print(f\"✅ Test Accuracy with {sbert_model_name}: {test_acc * 100:.2f}%\")\n",
    "print(f\"🎯 Weighted F1 Score: {f1 * 100:.2f}%\")\n",
    "\n",
    "# Optional: detailed class-level report\n",
    "print(\"\\n📋 Classification Report:\\n\")\n",
    "print(classification_report(Y_test_int, Y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---- Step 1: Load SBERT, Model, and Encoder ----\n",
    "sbert_model_name = \"paraphrase-MiniLM-L12-v2\"\n",
    "print(\"🔄 Loading SBERT and trained model...\")\n",
    "\n",
    "sbert = SentenceTransformer(sbert_model_name)\n",
    "model = load_model(\"optimized_lstm_model.keras\", custom_objects={\"AttentionLayer\": tf.keras.layers.Layer})\n",
    "with open(\"response_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# ---- Step 2: Prediction Function ----\n",
    "def predict_response(user_input):\n",
    "    # SBERT embedding\n",
    "    embedding = sbert.encode([user_input])\n",
    "    embedding = np.expand_dims(embedding, axis=1)  # Add sequence dimension\n",
    "\n",
    "    # Model prediction\n",
    "    probs = model.predict(embedding)\n",
    "    pred_index = np.argmax(probs, axis=1)[0]\n",
    "    confidence = probs[0][pred_index]\n",
    "\n",
    "    # Decode label\n",
    "    predicted_response = label_encoder.inverse_transform([pred_index])[0]\n",
    "    return predicted_response, confidence\n",
    "\n",
    "# ---- Step 3: Run Chatbot Loop ----\n",
    "print(\"\\n🧠 Chatbot is ready! Type your message (or type 'exit' to quit):\\n\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"👋 Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response, confidence = predict_response(user_input)\n",
    "    print(f\"Bot: {response} (confidence: {confidence:.2f})\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
