{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras nltk scikit-learn transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall transformers -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers tensorflow torch scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install keras==2.13.1  transformers==4.33.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow keras transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras==3.0.5 transformers==4.38.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Generating SBERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 110/110 [00:59<00:00,  1.86it/s]\n",
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Bidirectional, BatchNormalization, Layer, Add, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ---- Custom Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# ---- Load Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "contexts = [sample[\"Context\"] for sample in parsed_data]\n",
    "\n",
    "# ---- Load Mood Keywords ----\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "def detect_mood(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "X_texts, Y_labels = [], []\n",
    "for context in contexts:\n",
    "    mood = detect_mood(context)\n",
    "    if mood:\n",
    "        X_texts.append(context)\n",
    "        Y_labels.append(mood)\n",
    "\n",
    "# ---- Encode and Filter Labels ----\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y_labels)\n",
    "with open(\"mood_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "label_counts = Counter(Y_encoded)\n",
    "valid_labels = {label for label, count in label_counts.items() if count > 1}\n",
    "filtered = [(x, y) for x, y in zip(X_texts, Y_encoded) if y in valid_labels]\n",
    "X_texts_filtered = [x for x, _ in filtered]\n",
    "Y_filtered = [y for _, y in filtered]\n",
    "\n",
    "# ---- SBERT Embedding ----\n",
    "sbert = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "print(\"🔄 Generating SBERT embeddings...\")\n",
    "X_emb = sbert.encode(X_texts_filtered, show_progress_bar=True)\n",
    "\n",
    "# ---- Resampling ----\n",
    "min_samples = min(Counter(Y_filtered).values())\n",
    "if min_samples < 2:\n",
    "    print(\"⚠️ SMOTE skipped due to insufficient samples.\")\n",
    "    X_resampled, Y_resampled_encoded = X_emb, Y_filtered\n",
    "else:\n",
    "    smote_k = max(1, min(5, min_samples - 1))\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote = smote.fit_resample(X_emb, Y_filtered)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_encoded = smote_tomek.fit_resample(X_smote, Y_smote)\n",
    "\n",
    "Y_resampled = to_categorical(Y_resampled_encoded)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled, test_size=0.25, stratify=np.argmax(Y_resampled, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(Y_train_int), y=Y_train_int)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# ---- Model Builder ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout=0.3):\n",
    "        attention = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention = Dropout(dropout)(attention)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention]))\n",
    "        ff = Dense(ff_dim, activation='relu')(out1)\n",
    "        ff = Dense(inputs.shape[-1])(ff)\n",
    "        ff = Dropout(dropout)(ff)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff]))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_block(inputs)\n",
    "    x = transformer_block(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---- Train Model ----\n",
    "gc.collect()\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=Y_resampled.shape[1])\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, min_lr=1e-6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🚀 Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, Y_train_int,\n",
    "    validation_data=(X_test, Y_test_int),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test_int, verbose=1)\n",
    "print(f\"✅ Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "model.save(\"mood_classifier_model.keras\", save_format=\"keras\")\n",
    "print(\"💾 Model saved as mood_classifier_model.keras\")\n",
    "\n",
    "# ---- Plot Results ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('Accuracy'); plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Loss'); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- F1 Score ----\n",
    "Y_pred_probs = model.predict(X_test, verbose=1)\n",
    "Y_pred = np.argmax(Y_pred_probs, axis=1)\n",
    "f1 = f1_score(Y_test_int, Y_pred, average=\"weighted\")\n",
    "print(f\"🎯 Weighted F1 Score: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 122 variables whereas the saved optimizer has 110 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to load model: A total of 2 objects could not be loaded. Example error message for object <LSTMCell name=lstm_cell, built=True>:\n",
      "\n",
      "Layer 'lstm_cell' expected 3 variables, but received 0 variables during loading. Expected: ['kernel', 'recurrent_kernel', 'bias']\n",
      "\n",
      "List of objects that could not be loaded:\n",
      "[<LSTMCell name=lstm_cell, built=True>, <LSTMCell name=lstm_cell, built=True>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 ChatBot is ready. Type 'exit' to quit.\n",
      "\n",
      "📌 Detected mood from keywords: Depression\n",
      "MoodBot (Depression): I am not particularly discouraged about the future.\n",
      "\n",
      "📝 Query logged.\n",
      "🤖 Predicted mood from model: Mood_19\n",
      "MoodBot (Mood_19): I'm here to support you. Can you tell me more about how you're feeling?\n",
      "\n",
      "📝 Query logged.\n",
      "👋 Take care. I'm here whenever you need support.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# ---- Custom Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# ---- Load Model ----\n",
    "try:\n",
    "    model = load_model(\"mood_classifier_model.keras\", custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "    print(\"✅ Model loaded cleanly.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to load model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ---- Load SBERT and Resources ----\n",
    "sbert = SentenceTransformer(\"paraphrase-MiniLM-L12-v2\")\n",
    "\n",
    "with open(\"mood_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "with open(\"extracted_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    question_data = json.load(f)\n",
    "\n",
    "# ---- Logging ----\n",
    "LOG_FILE = \"chat_log.json\"\n",
    "\n",
    "def log_user_query(user_input, mood, question):\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"user_input\": user_input,\n",
    "        \"predicted_mood\": mood,\n",
    "        \"response_question\": question\n",
    "    }\n",
    "    if os.path.exists(LOG_FILE):\n",
    "        with open(LOG_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            logs = json.load(f)\n",
    "    else:\n",
    "        logs = []\n",
    "    logs.append(entry)\n",
    "    with open(LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(logs, f, indent=2)\n",
    "    print(\"📝 Query logged.\")\n",
    "\n",
    "# ---- Mood Detection ----\n",
    "def detect_mood_keywords(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "def predict_mood(text):\n",
    "    mood = detect_mood_keywords(text)\n",
    "    if mood:\n",
    "        print(f\"📌 Detected mood from keywords: {mood}\")\n",
    "        return mood\n",
    "\n",
    "    embedding = sbert.encode([text])\n",
    "    embedding = np.expand_dims(embedding, axis=1)  # Shape: (1, 1, 384)\n",
    "    prediction = model.predict(embedding, verbose=0)\n",
    "    mood_index = np.argmax(prediction)\n",
    "    mood_label = label_encoder.inverse_transform([mood_index])[0]\n",
    "    print(f\"🤖 Predicted mood from model: {mood_label}\")\n",
    "    return mood_label\n",
    "\n",
    "# ---- Fetch Question ----\n",
    "def fetch_question(mood):\n",
    "    mood_sections = question_data.get(mood, [])\n",
    "    all_questions = []\n",
    "\n",
    "    for section in mood_sections:\n",
    "        general = section.get(\"General\", {})\n",
    "        questions = general.get(\"questions\", [])\n",
    "        if isinstance(questions, list):\n",
    "            all_questions.extend(questions)\n",
    "\n",
    "    if not all_questions:\n",
    "        return \"I'm here to support you. Can you tell me more about how you're feeling?\"\n",
    "    return np.random.choice(all_questions)\n",
    "\n",
    "# ---- Chatbot Main ----\n",
    "def chatbot():\n",
    "    print(\"🧠 ChatBot is ready. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"👋 Take care. I'm here whenever you need support.\")\n",
    "            break\n",
    "\n",
    "        mood = predict_mood(user_input)\n",
    "        question = fetch_question(mood)\n",
    "        print(f\"MoodBot ({mood}): {question}\\n\")\n",
    "        log_user_query(user_input, mood, question)\n",
    "\n",
    "# ---- Run ----\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\soham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ mood_keywords.json generated with 30 moods.\n",
      "✅ mood_keywords.json updated and saved with 30 mood labels.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load short mood dataset\n",
    "with open(\"short_mood_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Normalize and group contexts by mood\n",
    "mood_to_contexts = defaultdict(list)\n",
    "for sample in data:\n",
    "    mood = sample.get(\"Mood\", \"\").strip().title()\n",
    "    context = sample.get(\"Context\", \"\").strip().lower()\n",
    "    if mood and context:\n",
    "        mood_to_contexts[mood].append(context)\n",
    "\n",
    "# Extract keywords per mood\n",
    "mood_keywords = {}\n",
    "for mood, contexts in mood_to_contexts.items():\n",
    "    if len(contexts) < 2:\n",
    "        continue  # skip rare moods\n",
    "    all_text = \" \".join(contexts)\n",
    "    tokens = word_tokenize(all_text)\n",
    "    cleaned = [w for w in tokens if w.isalpha() and w.lower() not in stop_words]\n",
    "    top_keywords = [word.lower() for word, _ in Counter(cleaned).most_common(15)]\n",
    "    if top_keywords:\n",
    "        mood_keywords[mood] = top_keywords\n",
    "\n",
    "# Save result\n",
    "with open(\"mood_keywords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mood_keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ mood_keywords.json generated with {len(mood_keywords)} moods.\")\n",
    "\n",
    "\n",
    "# Load the old mood_keywords.json (with Mood_0, Mood_1... keys)\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "# Define mapping from generic Mood_# labels to real mood names\n",
    "mood_map = {\n",
    "    \"Mood_0\": \"Abuse\",\n",
    "    \"Mood_1\": \"Sexual Issues\",\n",
    "    \"Mood_2\": \"Erectile Dysfunction\",\n",
    "    \"Mood_3\": \"Relationship Issues\",\n",
    "    \"Mood_4\": \"Depression\",\n",
    "    \"Mood_5\": \"Child Custody\",\n",
    "    \"Mood_6\": \"Parental Issues\",\n",
    "    \"Mood_7\": \"Infidelity\",\n",
    "    \"Mood_8\": \"Marriage Problems\",\n",
    "    \"Mood_9\": \"Motherhood\",\n",
    "    \"Mood_10\": \"Gender Identity\",\n",
    "    \"Mood_11\": \"Therapist Issues\",\n",
    "    \"Mood_12\": \"Therapy Anxiety\",\n",
    "    \"Mood_13\": \"Marital Issues\",\n",
    "    \"Mood_14\": \"Unrequited Love\",\n",
    "    \"Mood_15\": \"Therapy Nervousness\",\n",
    "    \"Mood_16\": \"Health Trauma\",\n",
    "    \"Mood_17\": \"PTSD\",\n",
    "    \"Mood_18\": \"Alcoholism\",\n",
    "    \"Mood_19\": \"Breakups\",\n",
    "    \"Mood_20\": \"Sexless Marriage\",\n",
    "    \"Mood_21\": \"Hearing Voices\",\n",
    "    \"Mood_22\": \"Eating Disorders\",\n",
    "    \"Mood_23\": \"Suicidal Thoughts\",\n",
    "    \"Mood_24\": \"Panic Attacks\",\n",
    "    \"Mood_25\": \"Toxic Relationships\",\n",
    "    \"Mood_26\": \"Anxiety And Depression\",\n",
    "    \"Mood_27\": \"Daughter Stress\",\n",
    "    \"Mood_28\": \"Work And Family Stress\",\n",
    "    \"Mood_29\": \"School Stress\"\n",
    "}\n",
    "\n",
    "# Create a new dict with renamed keys\n",
    "renamed_keywords = {}\n",
    "for mood_id, keywords in mood_keywords.items():\n",
    "    readable_label = mood_map.get(mood_id, mood_id)  # fallback to original key if no match\n",
    "    renamed_keywords[readable_label] = keywords\n",
    "\n",
    "# Save the updated file\n",
    "with open(\"mood_keywords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(renamed_keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ mood_keywords.json updated and saved with {len(renamed_keywords)} mood labels.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
