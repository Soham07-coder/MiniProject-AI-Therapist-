{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow keras nltk scikit-learn transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob vaderSentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall transformers -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers tensorflow torch scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip cache purge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pip install keras==2.13.1  transformers==4.33.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow keras transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras==3.0.5 transformers==4.38.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow-addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model,save_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Bidirectional, BatchNormalization, Layer, Add, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ---- Custom Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# ---- Load Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "contexts = [sample[\"Context\"] for sample in parsed_data]\n",
    "\n",
    "# ---- Load Mood Keywords ----\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "def detect_mood(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "X_texts, Y_labels = [], []\n",
    "for context in contexts:\n",
    "    mood = detect_mood(context)\n",
    "    if mood:\n",
    "        X_texts.append(context)\n",
    "        Y_labels.append(mood)\n",
    "\n",
    "# ---- Encode and Filter Labels ----\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y_labels)\n",
    "with open(\"mood_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "label_counts = Counter(Y_encoded)\n",
    "valid_labels = {label for label, count in label_counts.items() if count > 1}\n",
    "filtered = [(x, y) for x, y in zip(X_texts, Y_encoded) if y in valid_labels]\n",
    "X_texts_filtered = [x for x, _ in filtered]\n",
    "Y_filtered = [y for _, y in filtered]\n",
    "\n",
    "# ---- SBERT Embedding ----\n",
    "sbert = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "print(\"üîÑ Generating SBERT embeddings...\")\n",
    "X_emb = sbert.encode(X_texts_filtered, show_progress_bar=True)\n",
    "\n",
    "# ---- Resampling ----\n",
    "min_samples = min(Counter(Y_filtered).values())\n",
    "if min_samples < 2:\n",
    "    print(\"‚ö†Ô∏è SMOTE skipped due to insufficient samples.\")\n",
    "    X_resampled, Y_resampled_encoded = X_emb, Y_filtered\n",
    "else:\n",
    "    smote_k = max(1, min(5, min_samples - 1))\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote = smote.fit_resample(X_emb, Y_filtered)\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled_encoded = smote_tomek.fit_resample(X_smote, Y_smote)\n",
    "\n",
    "Y_resampled = to_categorical(Y_resampled_encoded)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled, test_size=0.25, stratify=np.argmax(Y_resampled, axis=1), random_state=42\n",
    ")\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=1)\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(Y_train_int), y=Y_train_int)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# ---- Model Builder ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout=0.3):\n",
    "        attention = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention = Dropout(dropout)(attention)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention]))\n",
    "        ff = Dense(ff_dim, activation='relu')(out1)\n",
    "        ff = Dense(inputs.shape[-1])(ff)\n",
    "        ff = Dropout(dropout)(ff)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff]))\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = transformer_block(inputs)\n",
    "    x = transformer_block(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---- Train Model ----\n",
    "gc.collect()\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=Y_resampled.shape[1])\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, min_lr=1e-6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Training model...\")\n",
    "history = model.fit(\n",
    "    X_train, Y_train_int,\n",
    "    validation_data=(X_test, Y_test_int),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test_int, verbose=1)\n",
    "print(f\"‚úÖ Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# ---- Plot Results ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('Accuracy'); plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Loss'); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- F1 Score ----\n",
    "Y_pred_probs = model.predict(X_test, verbose=1)\n",
    "Y_pred = np.argmax(Y_pred_probs, axis=1)\n",
    "f1 = f1_score(Y_test_int, Y_pred, average=\"weighted\")\n",
    "print(f\"üéØ Weighted F1 Score: {f1 * 100:.2f}%\")\n",
    "\n",
    "\n",
    "model_path = \"mood_classifier_model.keras\"\n",
    "\n",
    "# üßπ Remove old `.keras` model folder if it exists\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        if os.path.isfile(model_path):\n",
    "            os.remove(model_path)\n",
    "        else:\n",
    "            import shutil\n",
    "            shutil.rmtree(model_path)\n",
    "        print(f\"üßπ Removed old model: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not delete old model: {e}\")\n",
    "        raise\n",
    "\n",
    "# üíæ Save the model using native format\n",
    "try:\n",
    "    save_model(model, model_path)\n",
    "    print(f\"‚úÖ Model saved in native Keras format at: {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# ---- Custom Attention Layer ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        return K.sum(x * a, axis=1)\n",
    "\n",
    "# ---- Load Model ----\n",
    "try:\n",
    "    model = load_model(\"mood_classifier_model.keras\", custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "    print(\"‚úÖ Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ---- Load SBERT and Resources ----\n",
    "sbert = SentenceTransformer(\"paraphrase-MiniLM-L12-v2\")\n",
    "\n",
    "with open(\"mood_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# ---- Optional: Mood keywords\n",
    "try:\n",
    "    with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        mood_keywords = json.load(f)\n",
    "except:\n",
    "    mood_keywords = {}\n",
    "\n",
    "# ---- Load Cleaned Questions ----\n",
    "with open(\"cleaned_questions.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_question_data = json.load(f)\n",
    "\n",
    "# Convert cleaned data into a lookup map: mood ‚Üí list of questions\n",
    "question_data = {}\n",
    "for entry in raw_question_data:\n",
    "    mood = entry.get(\"category\", \"\").strip().lower()\n",
    "    all_qs = []\n",
    "    for section in entry.get(\"questions\", []):\n",
    "        for q in section.get(\"questions\", []):\n",
    "            if isinstance(q, str) and len(q.strip()) > 10:\n",
    "                all_qs.append(q.strip())\n",
    "    if all_qs:\n",
    "        question_data[mood] = all_qs\n",
    "\n",
    "# ---- Logging ----\n",
    "LOG_FILE = \"chat_log.json\"\n",
    "\n",
    "def log_user_query(user_input, mood, question, confidence):\n",
    "    entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"user_input\": user_input,\n",
    "        \"predicted_mood\": mood,\n",
    "        \"model_confidence\": confidence,\n",
    "        \"response_question\": question\n",
    "    }\n",
    "    logs = []\n",
    "    if os.path.exists(LOG_FILE):\n",
    "        with open(LOG_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            logs = json.load(f)\n",
    "    logs.append(entry)\n",
    "    with open(LOG_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(logs, f, indent=2)\n",
    "\n",
    "# ---- Mood Prediction ----\n",
    "def detect_mood_keywords(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "def predict_mood(text):\n",
    "    mood = detect_mood_keywords(text) if mood_keywords else None\n",
    "    if mood:\n",
    "        print(f\"üìå Detected mood from keywords: {mood}\")\n",
    "        return mood, 1.0\n",
    "\n",
    "    embedding = sbert.encode([text])\n",
    "    embedding = np.expand_dims(embedding, axis=1)  # (1, 1, 384)\n",
    "    prediction = model.predict(embedding, verbose=0)\n",
    "    mood_index = np.argmax(prediction)\n",
    "    confidence = float(np.max(prediction))\n",
    "    mood_label = label_encoder.inverse_transform([mood_index])[0]\n",
    "    print(f\"ü§ñ Predicted mood: {mood_label} (confidence: {confidence:.2f})\")\n",
    "    return mood_label, confidence\n",
    "\n",
    "# ---- Fetch Question ----\n",
    "def fetch_question(mood):\n",
    "    mood_key = mood.strip().lower()\n",
    "    questions = question_data.get(mood_key, [])\n",
    "\n",
    "    if not questions:\n",
    "        print(f\"‚ö†Ô∏è No questions found for mood: {mood_key}\")\n",
    "        return f\"I'm here to support you. Would you like to talk more about feeling {mood.lower()}?\"\n",
    "\n",
    "    return np.random.choice(questions)\n",
    "\n",
    "# ---- Chatbot Interface ----\n",
    "def chatbot():\n",
    "    print(\"üß† ChatBot is ready. Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \").strip()\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"üëã Take care. I'm here whenever you need support.\")\n",
    "            break\n",
    "\n",
    "        mood, confidence = predict_mood(user_input)\n",
    "        question = fetch_question(mood)\n",
    "        print(f\"\\nüß† MoodBot ({mood}): I understand you're feeling {mood.lower()}. {question}\\n\")\n",
    "        log_user_query(user_input, mood, question, confidence)\n",
    "\n",
    "# ---- Run ----\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moods code\n",
    "import json\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Load short mood dataset\n",
    "with open(\"short_mood_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Normalize and group contexts by mood\n",
    "mood_to_contexts = defaultdict(list)\n",
    "for sample in data:\n",
    "    mood = sample.get(\"Mood\", \"\").strip().title()\n",
    "    context = sample.get(\"Context\", \"\").strip().lower()\n",
    "    if mood and context:\n",
    "        mood_to_contexts[mood].append(context)\n",
    "\n",
    "# Extract keywords per mood\n",
    "mood_keywords = {}\n",
    "for mood, contexts in mood_to_contexts.items():\n",
    "    if len(contexts) < 2:\n",
    "        continue  # skip rare moods\n",
    "    all_text = \" \".join(contexts)\n",
    "    tokens = word_tokenize(all_text)\n",
    "    cleaned = [w for w in tokens if w.isalpha() and w.lower() not in stop_words]\n",
    "    top_keywords = [word.lower() for word, _ in Counter(cleaned).most_common(15)]\n",
    "    if top_keywords:\n",
    "        mood_keywords[mood] = top_keywords\n",
    "\n",
    "# Save result\n",
    "with open(\"mood_keywords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mood_keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ mood_keywords.json generated with {len(mood_keywords)} moods.\")\n",
    "\n",
    "\n",
    "# Load the old mood_keywords.json (with Mood_0, Mood_1... keys)\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "# Define mapping from generic Mood_# labels to real mood names\n",
    "mood_map = {\n",
    "    \"Mood_0\": \"Abuse\",\n",
    "    \"Mood_1\": \"Sexual Issues\",\n",
    "    \"Mood_2\": \"Erectile Dysfunction\",\n",
    "    \"Mood_3\": \"Relationship Issues\",\n",
    "    \"Mood_4\": \"Depression\",\n",
    "    \"Mood_5\": \"Child Custody\",\n",
    "    \"Mood_6\": \"Parental Issues\",\n",
    "    \"Mood_7\": \"Infidelity\",\n",
    "    \"Mood_8\": \"Marriage Problems\",\n",
    "    \"Mood_9\": \"Motherhood\",\n",
    "    \"Mood_10\": \"Gender Identity\",\n",
    "    \"Mood_11\": \"Therapist Issues\",\n",
    "    \"Mood_12\": \"Therapy Anxiety\",\n",
    "    \"Mood_13\": \"Marital Issues\",\n",
    "    \"Mood_14\": \"Unrequited Love\",\n",
    "    \"Mood_15\": \"Therapy Nervousness\",\n",
    "    \"Mood_16\": \"Health Trauma\",\n",
    "    \"Mood_17\": \"PTSD\",\n",
    "    \"Mood_18\": \"Alcoholism\",\n",
    "    \"Mood_19\": \"Breakups\",\n",
    "    \"Mood_20\": \"Sexless Marriage\",\n",
    "    \"Mood_21\": \"Hearing Voices\",\n",
    "    \"Mood_22\": \"Eating Disorders\",\n",
    "    \"Mood_23\": \"Suicidal Thoughts\",\n",
    "    \"Mood_24\": \"Panic Attacks\",\n",
    "    \"Mood_25\": \"Toxic Relationships\",\n",
    "    \"Mood_26\": \"Anxiety And Depression\",\n",
    "    \"Mood_27\": \"Daughter Stress\",\n",
    "    \"Mood_28\": \"Work And Family Stress\",\n",
    "    \"Mood_29\": \"School Stress\"\n",
    "}\n",
    "\n",
    "# Create a new dict with renamed keys\n",
    "renamed_keywords = {}\n",
    "for mood_id, keywords in mood_keywords.items():\n",
    "    readable_label = mood_map.get(mood_id, mood_id)  # fallback to original key if no match\n",
    "    renamed_keywords[readable_label] = keywords\n",
    "\n",
    "# Save the updated file\n",
    "with open(\"mood_keywords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(renamed_keywords, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ mood_keywords.json updated and saved with {len(renamed_keywords)} mood labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"combined_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Initialize sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def infer_mood(text):\n",
    "    # VADER sentiment\n",
    "    vader_scores = vader.polarity_scores(text)\n",
    "    compound = vader_scores[\"compound\"]\n",
    "\n",
    "    # TextBlob for polarity/subjectivity\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "    # Rule-based mood inference\n",
    "    if compound <= -0.5:\n",
    "        if subjectivity > 0.6:\n",
    "            return [\"Depressed\"]\n",
    "        elif polarity < -0.3:\n",
    "            return [\"Hopeless\"]\n",
    "        else:\n",
    "            return [\"Struggling\"]\n",
    "    elif -0.5 < compound < 0:\n",
    "        return [\"Anxious\"]\n",
    "    elif 0 <= compound < 0.3:\n",
    "        return [\"Neutral\"]\n",
    "    elif 0.3 <= compound <= 0.6:\n",
    "        return [\"Hopeful\"]\n",
    "    else:\n",
    "        return [\"Motivated\"]\n",
    "\n",
    "# Analyze and build output\n",
    "output_data = []\n",
    "for entry in data:\n",
    "    context = entry.get(\"Context\", \"\")\n",
    "    response = entry.get(\"Response\", \"\")\n",
    "    full_text = f\"{context} {response}\"\n",
    "\n",
    "    moods = infer_mood(full_text)\n",
    "\n",
    "    output_data.append({\n",
    "        \"Context\": context.strip(),\n",
    "        \"Detected_Moods\": moods,\n",
    "        \"Tags\": []\n",
    "    })\n",
    "\n",
    "# Save result\n",
    "with open(\"moods_keywords.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"‚úÖ moods_keywords.json generated successfully using auto NLP analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
