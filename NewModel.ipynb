{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd69415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Bidirectional, LSTM, GlobalMaxPooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- Load Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    parsed_data = [json.loads(line) for line in f if line.strip()]\n",
    "contexts = [sample[\"Context\"] for sample in parsed_data]\n",
    "\n",
    "# ---- Load Mood Keywords ----\n",
    "with open(\"mood_keywords.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    mood_keywords = json.load(f)\n",
    "\n",
    "def detect_mood(text):\n",
    "    text = text.lower()\n",
    "    for mood, keywords in mood_keywords.items():\n",
    "        if any(keyword in text for keyword in keywords):\n",
    "            return mood\n",
    "    return None\n",
    "\n",
    "X_texts, Y_labels = [], []\n",
    "for context in contexts:\n",
    "    mood = detect_mood(context)\n",
    "    if mood:\n",
    "        X_texts.append(context)\n",
    "        Y_labels.append(mood)\n",
    "\n",
    "# ---- Encode Labels (initial) ----\n",
    "label_encoder = LabelEncoder()\n",
    "Y_encoded = label_encoder.fit_transform(Y_labels)\n",
    "with open(\"mood_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "label_counts = Counter(Y_encoded)\n",
    "valid_labels = {label for label, count in label_counts.items() if count > 1}\n",
    "filtered = [(x, y) for x, y in zip(X_texts, Y_encoded) if y in valid_labels]\n",
    "X_texts_filtered = [x for x, _ in filtered]\n",
    "Y_filtered = [y for _, y in filtered]\n",
    "\n",
    "# ---- RoBERTa Token Embeddings ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\").to(device)\n",
    "roberta_model.eval()\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "def roberta_embed_token_embeddings(texts, batch_size=16, max_length=128):\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            encoded = tokenizer(batch, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "            output = roberta_model(**encoded)\n",
    "            embeddings = output.last_hidden_state.cpu().numpy()\n",
    "            all_embeddings.append(embeddings)\n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "embedding_file = \"roberta_token_embeddings.npy\"\n",
    "texts_file = \"roberta_embedding_texts.pkl\"\n",
    "labels_file = \"roberta_embedding_labels.pkl\"\n",
    "\n",
    "if os.path.exists(embedding_file) and os.path.exists(texts_file) and os.path.exists(labels_file):\n",
    "    print(\"üìÇ Loading existing RoBERTa token embeddings...\")\n",
    "    X_emb = np.load(embedding_file)\n",
    "    with open(texts_file, \"rb\") as f:\n",
    "        saved_texts = pickle.load(f)\n",
    "    with open(labels_file, \"rb\") as f:\n",
    "        saved_labels = pickle.load(f)\n",
    "    if len(saved_texts) == len(X_texts_filtered) and len(saved_labels) == len(Y_filtered):\n",
    "        print(\"‚úÖ Loaded existing embeddings successfully!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Saved data mismatch, regenerating embeddings...\")\n",
    "        X_emb = roberta_embed_token_embeddings(X_texts_filtered, max_length=max_length)\n",
    "        np.save(embedding_file, X_emb)\n",
    "        with open(texts_file, \"wb\") as f:\n",
    "            pickle.dump(X_texts_filtered, f)\n",
    "        with open(labels_file, \"wb\") as f:\n",
    "            pickle.dump(Y_filtered, f)\n",
    "else:\n",
    "    print(\"üîÑ Generating RoBERTa token embeddings...\")\n",
    "    X_emb = roberta_embed_token_embeddings(X_texts_filtered, max_length=max_length)\n",
    "    print(\"üíæ Saving RoBERTa token embeddings...\")\n",
    "    np.save(embedding_file, X_emb)\n",
    "    with open(texts_file, \"wb\") as f:\n",
    "        pickle.dump(X_texts_filtered, f)\n",
    "    with open(labels_file, \"wb\") as f:\n",
    "        pickle.dump(Y_filtered, f)\n",
    "\n",
    "print(f\"Token embedding shape: {X_emb.shape}\")\n",
    "\n",
    "# ---- Resampling with SMOTE after PCA ----\n",
    "min_samples = min(Counter(Y_filtered).values())\n",
    "\n",
    "if min_samples < 2:\n",
    "    print(\"‚ö†Ô∏è SMOTE skipped due to insufficient samples.\")\n",
    "    X_resampled = X_emb\n",
    "    Y_resampled_encoded = np.array(Y_filtered)\n",
    "else:\n",
    "    # Flatten then reduce dimensionality with PCA\n",
    "    X_flat = X_emb.reshape(X_emb.shape[0], -1)  # (samples, 128*768)\n",
    "    print(\"üîΩ Applying PCA to reduce dimensions before SMOTE...\")\n",
    "    pca = PCA(n_components=500, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_flat)  # (samples, 500)\n",
    "    \n",
    "    smote_k = max(1, min(5, min_samples - 1))\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_resampled_pca, Y_resampled_encoded = smote.fit_resample(X_pca, Y_filtered)\n",
    "\n",
    "    # Optionally map PCA output back to 3D for LSTM input\n",
    "    X_resampled = np.reshape(X_resampled_pca, (X_resampled_pca.shape[0], 10, 50))  # e.g., 10 timesteps of 50 dims\n",
    "\n",
    "# ---- Re-encode labels to zero-based continuous indices ----\n",
    "print(\"Labels before re-encoding after SMOTE:\", np.unique(Y_resampled_encoded))\n",
    "le = LabelEncoder()\n",
    "Y_resampled_encoded_zero_based = le.fit_transform(Y_resampled_encoded)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Labels after re-encoding:\", np.unique(Y_resampled_encoded_zero_based))\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "# ---- One-hot encode labels ----\n",
    "Y_resampled = to_categorical(Y_resampled_encoded_zero_based, num_classes=num_classes)\n",
    "\n",
    "# ---- Train/Test Split ----\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled,\n",
    "    test_size=0.25,\n",
    "    stratify=Y_resampled_encoded_zero_based,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "Y_train_int = np.argmax(Y_train, axis=1)\n",
    "Y_test_int = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# ---- Compute class weights ----\n",
    "class_weights = compute_class_weight(\n",
    "    \"balanced\",\n",
    "    classes=np.unique(Y_train_int),\n",
    "    y=Y_train_int\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "# ---- Model Builder ----\n",
    "def build_model(input_shape, num_classes):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ---- Train Model ----\n",
    "gc.collect()\n",
    "model = build_model(input_shape=X_train.shape[1:], num_classes=num_classes)\n",
    "\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.8, patience=7, min_lr=1e-6, verbose=1)\n",
    "\n",
    "print(\"Training data size:\", len(X_train))\n",
    "print(\"Validation data size:\", int(len(X_train)*0.1))\n",
    "print(\"Test data size:\", len(X_test))\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---- Evaluate ----\n",
    "loss, acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f\"‚úÖ Test Loss: {loss:.4f} - Test Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "Y_pred_prob = model.predict(X_test)\n",
    "Y_pred = np.argmax(Y_pred_prob, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b88f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model,save_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ---- Plot Results ----\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.title('Accuracy'); plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Loss'); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- F1 Score ----\n",
    "f1 = f1_score(Y_test_int, Y_pred, average=\"weighted\")\n",
    "print(f\"üéØ Weighted F1 Score: {f1 * 100:.2f}%\")\n",
    "\n",
    "# ---- Save Model ----\n",
    "from tensorflow.keras.models import save_model\n",
    "model_path = \"moods_classifier_bilstm_model.keras\"\n",
    "\n",
    "# Remove old model if exists\n",
    "if os.path.exists(model_path):\n",
    "    try:\n",
    "        if os.path.isfile(model_path):\n",
    "            os.remove(model_path)\n",
    "        else:\n",
    "            import shutil\n",
    "            shutil.rmtree(model_path)\n",
    "        print(f\"üßπ Removed old model: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Could not delete old model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Save model\n",
    "try:\n",
    "    save_model(model, model_path)\n",
    "    print(f\"‚úÖ Model saved in native Keras format at: {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182ebd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Devices:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338236bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Imports and Setup ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, Bidirectional, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# === Load and Clean Dataset ===\n",
    "df = pd.read_csv(\"final_cleaned_dataset.csv\")\n",
    "texts = df[\"question_text\"].fillna(\"\").astype(str).tolist()\n",
    "labels = df[\"topics\"].fillna(\"\").astype(str).str.strip().str.lower().tolist() \n",
    "\n",
    "# Ensure labels are strings and handle potential None/NaN if not done by fillna\n",
    "labels = [str(label).strip() for label in labels] # Added .strip() to remove leading/trailing whitespace\n",
    "\n",
    "# Load categories from cleaned_questions.json to ensure consistency\n",
    "# === Load categories from cleaned_questions.json to ensure consistency ===\n",
    "# === Load categories from cleaned_questions.json ===\n",
    "def get_categories_from_json(file_path=\"cleaned_questions.json\"):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        categories = set()\n",
    "        for entry in data:\n",
    "            category_name = entry.get(\"category\")\n",
    "            if category_name:\n",
    "                categories.add(str(category_name).strip().lower())  # Normalize\n",
    "        return sorted(list(categories))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: cleaned_questions.json not found at '{file_path}'\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from '{file_path}'. Check file format.\")\n",
    "        return []\n",
    "\n",
    "expected_categories = get_categories_from_json(\"cleaned_questions.json\")\n",
    "\n",
    "# === Mapping function ===\n",
    "def map_to_general_category(label_string):\n",
    "    topics = str(label_string).lower().split(\",\")\n",
    "    topics = [t.strip() for t in topics]\n",
    "    if any(t in topics for t in [\"depression\", \"anxiety\"]):\n",
    "        return \"depression & anxiety\"\n",
    "    elif any(t in topics for t in [\"stress\", \"coping\"]):\n",
    "        return \"stress & coping\"\n",
    "    elif any(t in topics for t in [\"trauma\", \"ptsd\"]):\n",
    "        return \"trauma & ptsd\"\n",
    "    else:\n",
    "        return \"personality & behaviour\"\n",
    "\n",
    "# === Apply mapping ===\n",
    "df[\"general_category\"] = df[\"topics\"].fillna(\"\").apply(map_to_general_category)\n",
    "texts = df[\"question_text\"].fillna(\"\").astype(str).tolist()\n",
    "labels = df[\"general_category\"].astype(str).str.strip().str.lower().tolist()\n",
    "\n",
    "# === Filter dataset ===\n",
    "filtered = [(text, label) for text, label in zip(texts, labels) if label in expected_categories]\n",
    "texts_filtered = [x for x, y in filtered]\n",
    "labels_filtered = [y for x, y in filtered]\n",
    "\n",
    "# === Debug ===\n",
    "print(f\"\\n--- Category Debugging ---\")\n",
    "print(f\"Categories loaded from cleaned_questions.json: {expected_categories}\")\n",
    "print(f\"Unique mapped labels found in dataset: {sorted(set(labels_filtered))}\")\n",
    "print(f\"Number of data points before filtering: {len(texts)}\")\n",
    "print(f\"Number of data points after filtering (matching JSON categories): {len(texts_filtered)}\")\n",
    "\n",
    "if not labels_filtered:\n",
    "    raise ValueError(\"No valid labels found in final_cleaned_dataset.csv that match categories in cleaned_questions.json.\")\n",
    "print(\"--- End Category Debugging ---\\n\")\n",
    "\n",
    "# === Encode Labels (Using one consistent LabelEncoder) ===\n",
    "# This is the ONE LabelEncoder that will map your string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels_filtered)\n",
    "\n",
    "# Verify the classes the encoder learned - these should be your string categories\n",
    "print(f\"Label Encoder classes (fitted on string topics): {label_encoder.classes_}\")\n",
    "\n",
    "# Save this CORRECTLY fitted encoder for inference\n",
    "with open(\"mood_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# === Load RoBERTa and Generate Embeddings ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\").to(device)\n",
    "roberta_model.eval()\n",
    "\n",
    "def roberta_embed_pooled(texts, batch_size=16, max_length=128):\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            encoded = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
    "            encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "            output = roberta_model(**encoded)\n",
    "            pooled = output.pooler_output\n",
    "            embeddings.append(pooled.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "embedding_file = \"roberta_embeddings.npy\"\n",
    "# Ensure the embedding file reflects the filtering if you're re-running from scratch\n",
    "# You might want to delete roberta_embeddings.npy if you changed filtering logic\n",
    "if os.path.exists(embedding_file):\n",
    "    print(\"üìÇ Loading cached embeddings...\")\n",
    "    try:\n",
    "        X_emb = np.load(embedding_file)\n",
    "        # Basic check to ensure loaded embeddings match filtered texts\n",
    "        if X_emb.shape[0] != len(texts_filtered):\n",
    "            print(\"Warning: Cached embeddings size mismatch. Re-generating embeddings.\")\n",
    "            os.remove(embedding_file) # Delete old file to force regeneration\n",
    "            raise FileNotFoundError # Force regeneration\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cached embeddings: {e}. Re-generating.\")\n",
    "        if os.path.exists(embedding_file):\n",
    "            os.remove(embedding_file) # Delete old file\n",
    "        X_emb = roberta_embed_pooled(texts_filtered)\n",
    "        np.save(embedding_file, X_emb)\n",
    "else:\n",
    "    print(\"üîÑ Generating embeddings...\")\n",
    "    X_emb = roberta_embed_pooled(texts_filtered)\n",
    "    np.save(embedding_file, X_emb)\n",
    "\n",
    "# === Balance Data with SMOTE ===\n",
    "# SMOTE requires at least 2 samples per class for k_neighbors\n",
    "# labels_encoded are already integer labels from the correctly fitted encoder\n",
    "label_counts_for_smote = Counter(labels_encoded)\n",
    "print(f\"Label counts before SMOTE: {label_counts_for_smote}\")\n",
    "\n",
    "# Determine appropriate k_neighbors for SMOTE\n",
    "# k_neighbors must be <= (number of samples in the smallest class - 1)\n",
    "min_samples_in_any_class = min(label_counts_for_smote.values())\n",
    "k_neighbors_val = min(min_samples_in_any_class - 1, 5) # Default to 5, but respect min_samples\n",
    "if k_neighbors_val < 1: # If a class has only 1 sample, SMOTE cannot create neighbors\n",
    "    print(\"Warning: Some classes have only one sample (or fewer than k_neighbors). SMOTE might not be effective or will fail for these classes.\")\n",
    "    print(\"Skipping SMOTE application.\")\n",
    "    X_resampled, Y_resampled = X_emb, labels_encoded\n",
    "else:\n",
    "    print(f\"Applying SMOTE with k_neighbors={k_neighbors_val}\")\n",
    "    smote = SMOTE(random_state=42, k_neighbors=k_neighbors_val)\n",
    "    X_resampled, Y_resampled = smote.fit_resample(X_emb, labels_encoded) # labels_encoded here are numbers\n",
    "    print(f\"Label counts after SMOTE: {Counter(Y_resampled)}\")\n",
    "\n",
    "\n",
    "# === One-hot encode labels ===\n",
    "# Y_resampled already contains the correct numerical labels from the first label_encoder.\n",
    "# We just need to convert them to one-hot encoding.\n",
    "# DO NOT re-fit a new LabelEncoder here on Y_resampled.\n",
    "num_classes = len(label_encoder.classes_) # Use the number of classes from the original encoder\n",
    "Y_onehot = to_categorical(Y_resampled, num_classes=num_classes)\n",
    "\n",
    "# === Split Dataset ===\n",
    "# X_resampled is (num_samples, embedding_dim)\n",
    "# Keras LSTM expects (num_samples, timesteps, features)\n",
    "# Here, timesteps = 1, features = embedding_dim\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    np.expand_dims(X_resampled, axis=1).astype(\"float32\"), # Add timestep dimension\n",
    "    Y_onehot.astype(\"float32\"),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=Y_resampled # Stratify using the numerical labels, not the one-hot\n",
    ")\n",
    "\n",
    "# === Attention Layer ===\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "# === Build Model ===\n",
    "def build_model(seq_len, embedding_dim, num_classes):\n",
    "    inp = Input(shape=(seq_len, embedding_dim))\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(1e-4)))(inp)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(1e-4))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "    return Model(inputs=inp, outputs=out)\n",
    "\n",
    "model = build_model(X_train.shape[1], X_train.shape[2], Y_train.shape[1])\n",
    "\n",
    "# === Compile Model ===\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# === Class Weights ===\n",
    "# Y_resampled contains the numerical labels after SMOTE, which are what compute_class_weight expects\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(Y_resampled), y=Y_resampled)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(f\"Computed Class Weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d35bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"mood_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "print(\"Label Encoder Classes:\", label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb5807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Compile Model ===\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=500,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "optimizer = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks WITHOUT ReduceLROnPlateau\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
    "    ModelCheckpoint(\"roberta_bilstm_attention_best.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "try:\n",
    "    history = model.fit(\n",
    "        X_train, Y_train,\n",
    "        validation_data=(X_test, Y_test),\n",
    "        epochs=30,\n",
    "        batch_size=16,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "except (tf.errors.ResourceExhaustedError, tf.errors.InternalError):\n",
    "    print(\"GPU Memory Exhausted. Switching to CPU training with adjusted batch size...\")\n",
    "    tf.keras.backend.clear_session()\n",
    "    with tf.device('/CPU:0'):\n",
    "        model = build_model(X_train.shape[1], X_train.shape[2], Y_train.shape[1])\n",
    "        model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        history = model.fit(\n",
    "            X_train, Y_train,\n",
    "            validation_data=(X_test, Y_test),\n",
    "            epochs=15,\n",
    "            batch_size=32,\n",
    "            class_weight=class_weight_dict,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "# === Save Final Model ===\n",
    "model.save(\"roberta_bilstm_attention_final.keras\")\n",
    "\n",
    "# === Evaluate Model ===\n",
    "print(\"\\n--- Evaluating Model ---\")\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# === Classification Report and Confusion Matrix ===\n",
    "Y_pred_probs = model.predict(X_test)\n",
    "Y_pred = np.argmax(Y_pred_probs, axis=1)\n",
    "Y_true = np.argmax(Y_test, axis=1)\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "target_names = label_encoder.inverse_transform(np.unique(Y_true))  # Class names sorted by index\n",
    "print(classification_report(Y_true, Y_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\n--- Confusion Matrix ---\")\n",
    "cm = confusion_matrix(Y_true, Y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8234ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label=\"Train Accuracy\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"Val Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label=\"Train Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5870d335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing 'cleaned_dataset.csv' for chatbot response bank ---\n",
      "Original shape of 'cleaned_dataset.csv': (10485, 5)\n",
      "Columns: ['question_title', 'question_text', 'topics', 'answer_text', 'split']\n",
      "Columns after renaming: ['question_title', 'Context', 'category', 'Response', 'split']\n",
      "Removed 6979 rows due to missing 'Context', 'Response', or 'category'.\n",
      "Removed 123 duplicate rows based on 'Context' and 'Response'.\n",
      "Final shape of data for chatbot: (3383, 5)\n",
      "Prepared dataset saved to: 'Final_Chatbot_Dataset.csv'\n",
      "\n",
      "Unique categories found in the prepared dataset (after mapping):\n",
      "['depression & anxiety' 'personality & behaviour' 'trauma & ptsd'\n",
      " 'stress & coping']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soham\\AppData\\Local\\Temp\\ipykernel_19876\\1482232434.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned_for_chatbot.drop_duplicates(subset=['Context', 'Response'], keep='first', inplace=True)\n",
      "C:\\Users\\soham\\AppData\\Local\\Temp\\ipykernel_19876\\1482232434.py:315: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned_for_chatbot['category'] = df_cleaned_for_chatbot['category'].apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define input and output file paths\n",
    "input_file = 'cleaned_dataset.csv'\n",
    "output_file = 'Final_Chatbot_Dataset.csv'\n",
    "\n",
    "print(f\"--- Preparing '{input_file}' for chatbot response bank ---\")\n",
    "\n",
    "try:\n",
    "    # Load the new dataset\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Original shape of '{input_file}': {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Rename columns to match chatbot's expected structure\n",
    "    df.rename(columns={\n",
    "        'question_text': 'Context',\n",
    "        'answer_text': 'Response',\n",
    "        'topics': 'category'\n",
    "    }, inplace=True)\n",
    "    print(f\"Columns after renaming: {df.columns.tolist()}\")\n",
    "\n",
    "    # Drop rows where 'Context', 'Response', or 'category' are missing\n",
    "    initial_rows = df.shape[0]\n",
    "    df_cleaned_for_chatbot = df.dropna(subset=['Context', 'Response', 'category'])\n",
    "    removed_rows_na = initial_rows - df_cleaned_for_chatbot.shape[0]\n",
    "    print(f\"Removed {removed_rows_na} rows due to missing 'Context', 'Response', or 'category'.\")\n",
    "\n",
    "    # Optionally, remove duplicates again based on 'Context' and 'Response'\n",
    "    initial_rows_after_na = df_cleaned_for_chatbot.shape[0]\n",
    "    df_cleaned_for_chatbot.drop_duplicates(subset=['Context', 'Response'], keep='first', inplace=True)\n",
    "    removed_rows_duplicates = initial_rows_after_na - df_cleaned_for_chatbot.shape[0]\n",
    "    print(f\"Removed {removed_rows_duplicates} duplicate rows based on 'Context' and 'Response'.\")\n",
    "\n",
    "    # === CRITICAL: Define the mapping for categories ===\n",
    "    # Map granular categories from your CSV's 'topics' column\n",
    "    # to the broader categories your LabelEncoder understands.\n",
    "    # Adjust this mapping as needed based on your model's training.\n",
    "    category_mapping = {\n",
    "        # Depression & Anxiety\n",
    "        'depression': 'depression & anxiety',\n",
    "        'anxiety': 'depression & anxiety',\n",
    "        'Anxiety': 'depression & anxiety', # Handle capitalization\n",
    "        'Depression': 'depression & anxiety',\n",
    "        'depression & anxiety': 'depression & anxiety', # If already combined\n",
    "        'Depression,Anxiety': 'depression & anxiety', # If comma-separated\n",
    "        'Anxiety,Depression': 'depression & anxiety',\n",
    "        'Depression,Anxiety,Relationships': 'depression & anxiety', # Prioritize core categories\n",
    "        'Anxiety,Depression,Behavioral Change': 'depression & anxiety',\n",
    "        'Anxiety,Depression,Stress,Relationships': 'depression & anxiety',\n",
    "        'Anxiety,Depression,Sleep Improvement': 'depression & anxiety',\n",
    "        'Anxiety,Depression,Self-esteem': 'depression & anxiety',\n",
    "        'Anxiety,Depression,Legal & Regulatory': 'depression & anxiety',\n",
    "        'Depression,Anxiety,Diagnosis': 'depression & anxiety',\n",
    "        'Depression,Anxiety': 'depression & anxiety', # Adding again for robustness\n",
    "        'Depression,Anxiety,Behavioral Change,Marriage': 'depression & anxiety',\n",
    "        'Depression,Anxiety,Sleep Improvement': 'depression & anxiety',\n",
    "\n",
    "\n",
    "        # Stress & Coping\n",
    "        'stress': 'stress & coping',\n",
    "        'Stress': 'stress & coping',\n",
    "        'stress & coping': 'stress & coping',\n",
    "        'Anger Management': 'stress & coping',\n",
    "        'anger-management': 'stress & coping',\n",
    "        'Sleep Improvement': 'stress & coping',\n",
    "        'sleep-improvement': 'stress & coping',\n",
    "        'Behavioral Change': 'stress & coping',\n",
    "        'behavioral-change': 'stress & coping',\n",
    "        'Stress,Eating Disorders': 'stress & coping', # Prioritize core categories\n",
    "        'Stress,Family Conflict': 'stress & coping',\n",
    "        'Stress,Workplace Relationships': 'stress & coping',\n",
    "        'Stress,Anxiety': 'stress & coping', # If linked with anxiety, map to broader\n",
    "        'Anxiety,Stress': 'stress & coping',\n",
    "        'Anger Management,Sleep Improvement': 'stress & coping',\n",
    "        'Anger Management,Behavioral Change': 'stress & coping',\n",
    "        'Behavioral Change,Anxiety': 'stress & coping',\n",
    "        'Behavioral Change,Sleep Improvement': 'stress & coping',\n",
    "        'Depression,Stress': 'stress & coping',\n",
    "\n",
    "\n",
    "        # Trauma & PTSD\n",
    "        'trauma': 'trauma & ptsd',\n",
    "        'Trauma': 'trauma & ptsd',\n",
    "        'trauma & ptsd': 'trauma & ptsd',\n",
    "        'grief-and-loss': 'trauma & ptsd',\n",
    "        'Grief and Loss': 'trauma & ptsd',\n",
    "        'domestic-violence': 'trauma & ptsd',\n",
    "        'Domestic Violence': 'trauma & ptsd',\n",
    "        'military-issues': 'trauma & ptsd',\n",
    "        'Military Issues ': 'trauma & ptsd',\n",
    "        'Trauma,Relationships': 'trauma & ptsd', # Prioritize core categories\n",
    "        'Trauma,Human Sexuality': 'trauma & ptsd',\n",
    "        'Trauma,Anxiety': 'trauma & ptsd',\n",
    "        'Trauma,Depression': 'trauma & ptsd',\n",
    "        'Trauma,Depression,Anxiety': 'trauma & ptsd',\n",
    "        'Grief and Loss,Trauma,Anxiety': 'trauma & ptsd',\n",
    "        'Trauma,Grief and Loss': 'trauma & ptsd',\n",
    "        'Domestic Violence,Anger Management': 'trauma & ptsd',\n",
    "        'Domestic Violence,Relationships': 'trauma & ptsd',\n",
    "        'Domestic Violence,Family Conflict': 'trauma & ptsd',\n",
    "        'Domestic Violence,Sleep Improvement': 'trauma & ptsd',\n",
    "        'Domestic Violence,Legal & Regulatory': 'trauma & ptsd',\n",
    "        'Trauma,Stress,Anxiety,Anger Management': 'trauma & ptsd',\n",
    "        'Trauma,Family Conflict': 'trauma & ptsd',\n",
    "        'Trauma,Self-esteem,Relationship Dissolution ': 'trauma & ptsd',\n",
    "        'Intimacy,Trauma': 'trauma & ptsd',\n",
    "        'Trauma,Military Issues ': 'trauma & ptsd',\n",
    "        'Trauma,Depression,Relationships,Intimacy': 'trauma & ptsd',\n",
    "        'Family Conflict,Trauma': 'trauma & ptsd',\n",
    "        'Grief and Loss,Substance Abuse,Trauma': 'trauma & ptsd',\n",
    "        'Human Sexuality,Trauma,Intimacy': 'trauma & ptsd',\n",
    "        'Human Sexuality,Trauma,Intimacy,Relationships': 'trauma & ptsd',\n",
    "\n",
    "\n",
    "        # Personality & Behaviour (catch-all for other related categories)\n",
    "        'personality & behaviour': 'personality & behaviour',\n",
    "        'parenting': 'personality & behaviour',\n",
    "        'self-esteem': 'personality & behaviour',\n",
    "        'relationship-dissolution': 'personality & behaviour',\n",
    "        'workplace-relationships': 'personality & behaviour',\n",
    "        'spirituality': 'personality & behaviour',\n",
    "        'intimacy': 'personality & behaviour',\n",
    "        'substance-abuse': 'personality & behaviour',\n",
    "        'family-conflict': 'personality & behaviour',\n",
    "        'marriage': 'personality & behaviour',\n",
    "        'eating-disorders': 'personality & behaviour',\n",
    "        'relationships': 'personality & behaviour',\n",
    "        'lgbtq': 'personality & behaviour',\n",
    "        'addiction': 'personality & behaviour',\n",
    "        'legal-regulatory': 'personality & behaviour',\n",
    "        'professional-ethics': 'personality & behaviour',\n",
    "        'human-sexuality': 'personality & behaviour',\n",
    "        'social-relationships': 'personality & behaviour',\n",
    "        'children-adolescents': 'personality & behaviour',\n",
    "        'diagnosis': 'personality & behaviour',\n",
    "        'counseling-fundamentals': 'personality & behaviour',\n",
    "        'Family Conflict': 'personality & behaviour', # Capitalization\n",
    "        'Self-esteem': 'personality & behaviour',\n",
    "        'Parenting': 'personality & behaviour',\n",
    "        'Relationship Dissolution ': 'personality & behaviour',\n",
    "        'Workplace Relationships': 'personality & behaviour',\n",
    "        'Spirituality': 'personality & behaviour',\n",
    "        'Intimacy': 'personality & behaviour',\n",
    "        'Substance Abuse': 'personality & behaviour',\n",
    "        'Marriage': 'personality & behaviour',\n",
    "        'Eating Disorders': 'personality & behaviour',\n",
    "        'Relationships': 'personality & behaviour',\n",
    "        'LGBTQ': 'personality & behaviour',\n",
    "        'Addiction': 'personality & behaviour',\n",
    "        'Legal & Regulatory': 'personality & behaviour',\n",
    "        'Professional Ethics': 'personality & behaviour',\n",
    "        'Human Sexuality': 'personality & behaviour',\n",
    "        'Social Relationships': 'personality & behaviour',\n",
    "        'Children & Adolescents': 'personality & behaviour',\n",
    "        'Diagnosis': 'personality & behaviour',\n",
    "        'Counseling Fundamentals ': 'personality & behaviour',\n",
    "\n",
    "        # Compound Categories (will be mapped to one of the 4 main ones)\n",
    "        'Substance Abuse,Addiction': 'personality & behaviour',\n",
    "        'Behavioral Change,Social Relationships': 'personality & behaviour',\n",
    "        'Professional Ethics,Legal & Regulatory': 'personality & behaviour',\n",
    "        'Relationships,Marriage': 'personality & behaviour',\n",
    "        'Marriage,Intimacy': 'personality & behaviour',\n",
    "        'Family Conflict,Children & Adolescents': 'personality & behaviour',\n",
    "        'Marriage,Relationship Dissolution ': 'personality & behaviour',\n",
    "        'Relationships,Intimacy': 'personality & behaviour',\n",
    "        'Anger Management,Parenting': 'personality & behaviour',\n",
    "        'Family Conflict,Self-esteem,Parenting,Anxiety': 'personality & behaviour',\n",
    "        'Human Sexuality,Marriage': 'personality & behaviour',\n",
    "        'Spirituality,Family Conflict': 'personality & behaviour',\n",
    "        'Social Relationships,Anxiety,Depression': 'personality & behaviour',\n",
    "        'Family Conflict,Relationships': 'personality & behaviour',\n",
    "        'Self-esteem,Relationships': 'personality & behaviour',\n",
    "        'Family Conflict,Marriage': 'personality & behaviour',\n",
    "        'Family Conflict,Self-esteem': 'personality & behaviour',\n",
    "        'Parenting,Relationships': 'personality & behaviour',\n",
    "        'Anxiety,Career Counseling': 'personality & behaviour',\n",
    "        'Relationships,Self-esteem': 'personality & behaviour',\n",
    "        'Relationships,Anxiety': 'personality & behaviour',\n",
    "        'Eating Disorders,Addiction': 'personality & behaviour',\n",
    "        'Workplace Relationships,Professional Ethics': 'personality & behaviour',\n",
    "        'Anxiety,Spirituality': 'personality & behaviour',\n",
    "        'Relationship Dissolution ,Relationships,Domestic Violence': 'personality & behaviour',\n",
    "        'Parenting,Substance Abuse,Spirituality': 'personality & behaviour',\n",
    "        'Self-esteem,Relationship Dissolution ': 'personality & behaviour',\n",
    "        'Relationship Dissolution ,Marriage': 'personality & behaviour',\n",
    "        'Relationship Dissolution ,Depression,Self-esteem': 'personality & behaviour',\n",
    "        'Depression,Anger Management': 'personality & behaviour',\n",
    "        'Parenting,Anger Management,Family Conflict': 'personality & behaviour',\n",
    "        'Marriage,Family Conflict,Professional Ethics,Legal & Regulatory': 'personality & behaviour',\n",
    "        'Relationships,Human Sexuality,LGBTQ': 'personality & behaviour',\n",
    "        'Relationships,Parenting,Family Conflict': 'personality & behaviour',\n",
    "        'LGBTQ,Intimacy': 'personality & behaviour',\n",
    "        'Relationship Dissolution ,Depression': 'personality & behaviour',\n",
    "        'Anger Management,Relationships': 'personality & behaviour',\n",
    "        'Substance Abuse,Family Conflict': 'personality & behaviour',\n",
    "        'Anxiety,Social Relationships,Self-esteem': 'personality & behaviour',\n",
    "        'Self-esteem,Marriage,Trauma,Intimacy': 'personality & behaviour',\n",
    "        'Marriage,Addiction': 'personality & behaviour',\n",
    "        'Relationships,Legal & Regulatory': 'personality & behaviour',\n",
    "        'Human Sexuality,Relationships': 'personality & behaviour',\n",
    "        'Family Conflict,Relationships,Marriage': 'personality & behaviour',\n",
    "        'Marriage,Anger Management': 'personality & behaviour',\n",
    "        'Relationships,Family Conflict': 'personality & behaviour',\n",
    "        'Anxiety,Behavioral Change': 'personality & behaviour',\n",
    "        'Relationships,Depression': 'personality & behaviour',\n",
    "        'Human Sexuality,Social Relationships': 'personality & behaviour',\n",
    "        'Self-esteem,Eating Disorders': 'personality & behaviour',\n",
    "        'Career Counseling,Professional Ethics': 'personality & behaviour',\n",
    "        'Marriage,Grief and Loss': 'personality & behaviour',\n",
    "        'Self-esteem,Social Relationships': 'personality & behaviour',\n",
    "        'Depression,Relationships': 'personality & behaviour',\n",
    "        'Addiction,Substance Abuse': 'personality & behaviour',\n",
    "        'Workplace Relationships,Social Relationships': 'personality & behaviour',\n",
    "        'Eating Disorders,Human Sexuality,Addiction': 'personality & behaviour',\n",
    "        'Intimacy,Relationships': 'personality & behaviour',\n",
    "        'Depression,Family Conflict': 'personality & behaviour',\n",
    "        'Depression,Social Relationships': 'personality & behaviour',\n",
    "        'Relationships,Self-esteem,Human Sexuality': 'personality & behaviour',\n",
    "        'Behavioral Change,Depression': 'personality & behaviour',\n",
    "        'Relationships,Human Sexuality': 'personality & behaviour',\n",
    "        'Marriage,Family Conflict': 'personality & behaviour',\n",
    "        'Relationships,Self-esteem,Anxiety': 'personality & behaviour',\n",
    "        'Anxiety,Self-esteem,Workplace Relationships': 'personality & behaviour',\n",
    "        'Human Sexuality,Intimacy,Marriage': 'personality & behaviour',\n",
    "        'Relationships,Parenting': 'personality & behaviour',\n",
    "        'Relationships,Family Conflict,Parenting': 'personality & behaviour',\n",
    "        'Relationships,Social Relationships,Intimacy': 'personality & behaviour',\n",
    "        'Anger Management,Domestic Violence': 'personality & behaviour',\n",
    "        'Parenting,Family Conflict': 'personality & behaviour',\n",
    "        'Anxiety,Parenting': 'personality & behaviour',\n",
    "        'Family Conflict,Legal & Regulatory': 'personality & behaviour',\n",
    "        'Domestic Violence,Marriage': 'personality & behaviour',\n",
    "        'Intimacy,Human Sexuality,Relationships': 'personality & behaviour',\n",
    "        'Intimacy,Social Relationships': 'personality & behaviour',\n",
    "        'Anger Management,Relationships,Social Relationships': 'personality & behaviour',\n",
    "        'Relationships,Relationship Dissolution ,Intimacy': 'personality & behaviour',\n",
    "        'Relationships,Behavioral Change,Anxiety': 'personality & behaviour',\n",
    "        'Family Conflict,Depression': 'personality & behaviour',\n",
    "        'Parenting,Depression,Behavioral Change,Stress': 'personality & behaviour',\n",
    "        'Family Conflict,Parenting,Marriage': 'personality & behaviour',\n",
    "        'Marriage,Intimacy,Human Sexuality': 'personality & behaviour',\n",
    "        'Self-esteem,Depression,Anxiety': 'personality & behaviour',\n",
    "        'Marriage,Relationships,Intimacy': 'personality & behaviour',\n",
    "        'Relationships,Human Sexuality,Family Conflict,Spirituality': 'personality & behaviour',\n",
    "        'Relationships,Intimacy,Human Sexuality': 'personality & behaviour',\n",
    "        'Parenting,Relationship Dissolution ,Family Conflict': 'personality & behaviour',\n",
    "        'Addiction,Marriage,Intimacy': 'personality & behaviour',\n",
    "        'Family Conflict,Social Relationships': 'personality & behaviour',\n",
    "        'Anger Management,Depression,Relationships': 'personality & behaviour',\n",
    "        'Social Relationships,Relationships,Intimacy': 'personality & behaviour',\n",
    "        'Relationship Dissolution ,Depression,Social Relationships': 'personality & behaviour',\n",
    "        'LGBTQ,Relationships,Intimacy': 'personality & behaviour',\n",
    "        'Anger Management,Social Relationships,Relationships': 'personality & behaviour',\n",
    "        'Anxiety,Social Relationships': 'personality & behaviour',\n",
    "        'Intimacy,Marriage': 'personality & behaviour',\n",
    "        'Addiction,Depression,Self-harm': 'personality & behaviour',\n",
    "        'Addiction,Substance Abuse,Anxiety': 'personality & behaviour',\n",
    "        'Depression,Grief and Loss': 'personality & behaviour',\n",
    "        'Relationships,Anxiety,Self-esteem': 'personality & behaviour',\n",
    "        'Anxiety,Relationships,Behavioral Change': 'personality & behaviour',\n",
    "        'Family Conflict,Relationships,Intimacy': 'personality & behaviour',\n",
    "        'Marriage,Relationship Dissolution ,Intimacy': 'personality & behaviour',\n",
    "        'Family Conflict,LGBTQ': 'personality & behaviour',\n",
    "        'Anxiety,Family Conflict': 'personality & behaviour',\n",
    "        'Self-esteem,Stress,Anger Management': 'personality & behaviour',\n",
    "        'Spirituality,Relationships': 'personality & behaviour',\n",
    "        'Sleep Improvement,Anxiety': 'personality & behaviour',\n",
    "        'Relationships,Workplace Relationships': 'personality & behaviour',\n",
    "        'Human Sexuality,Intimacy,Relationships': 'personality & behaviour',\n",
    "        'Human Sexuality,Intimacy': 'personality & behaviour',\n",
    "        'Grief and Loss,Depression': 'personality & behaviour',\n",
    "        'Marriage,Relationship Dissolution ,Behavioral Change': 'personality & behaviour',\n",
    "        'Professional Ethics,Legal & Regulatory,Addiction': 'personality & behaviour',\n",
    "        'Social Relationships,Substance Abuse': 'personality & behaviour',\n",
    "        'Human Sexuality,Anxiety': 'personality & behaviour',\n",
    "        'Relationships,Social Relationships': 'personality & behaviour',\n",
    "        'Intimacy,Human Sexuality,Marriage': 'personality & behaviour',\n",
    "        'Self-esteem,Sleep Improvement': 'personality & behaviour',\n",
    "        'Social Relationships,Self-esteem': 'personality & behaviour',\n",
    "        'Family Conflict,Stress,Sleep Improvement': 'personality & behaviour',\n",
    "        'LGBTQ,Family Conflict': 'personality & behaviour',\n",
    "        'Social Relationships,Children & Adolescents': 'personality & behaviour',\n",
    "        'Behavioral Change,LGBTQ': 'personality & behaviour',\n",
    "        'Grief and Loss,Family Conflict': 'personality & behaviour',\n",
    "        'Self-esteem,Behavioral Change': 'personality & behaviour',\n",
    "        'Social Relationships,Relationships,Addiction': 'personality & behaviour',\n",
    "        'Social Relationships,LGBTQ': 'personality & behaviour',\n",
    "        'Self-esteem,LGBTQ': 'personality & behaviour',\n",
    "        'Legal & Regulatory,Professional Ethics': 'personality & behaviour',\n",
    "        'Relationships,Professional Ethics,Parenting,Legal & Regulatory': 'personality & behaviour',\n",
    "        'Eric Str√∂m, JD, MA, LMHC': 'personality & behaviour', # This looks like a specific entity, not a category\n",
    "        'Relationships,Trauma': 'personality & behaviour',\n",
    "        'Domestic Violence,Sleep Improvement': 'personality & behaviour',\n",
    "        'Human Sexuality,LGBTQ': 'personality & behaviour',\n",
    "        'Social Relationships,Depression': 'personality & behaviour',\n",
    "        'Relationship Dissolution ,Social Relationships': 'personality & behaviour',\n",
    "        'Intimacy,Relationships,Human Sexuality': 'personality & behaviour',\n",
    "        'Workplace Relationships,Career Counseling': 'personality & behaviour',\n",
    "        'Behavioral Change,Relationships': 'personality & behaviour',\n",
    "        \"Alzheimer's,Family Conflict\": 'personality & behaviour',\n",
    "        'Self-esteem,Depression': 'personality & behaviour',\n",
    "        'Human Sexuality,Relationships,Intimacy': 'personality & behaviour',\n",
    "        'Social Relationships,Family Conflict': 'personality & behaviour',\n",
    "        'Family Conflict,Anger Management': 'personality & behaviour',\n",
    "        'Human Sexuality': 'personality & behaviour', # Added this if not mapped\n",
    "        ' trauma history or having a cold the time it occurred.</span><span style=\"\"line-height: 1.42857;\"\">&nbsp;&nbsp;</span>Working with someone who utilizes a behavioral approach': 'personality & behaviour', # Catch-all for very unusual entries\n",
    "\n",
    "        # Default for unmapped categories\n",
    "        'Unknown': 'personality & behaviour' # Assign unmapped to a default broad category\n",
    "    }\n",
    "\n",
    "    # Apply the mapping. Use .get() with a default value to handle unmapped categories.\n",
    "    df_cleaned_for_chatbot['category'] = df_cleaned_for_chatbot['category'].apply(\n",
    "        lambda x: category_mapping.get(x, 'personality & behaviour') # Default to personality & behaviour\n",
    "    )\n",
    "\n",
    "    print(f\"Final shape of data for chatbot: {df_cleaned_for_chatbot.shape}\")\n",
    "\n",
    "    # Save the prepared dataset\n",
    "    df_cleaned_for_chatbot.to_csv(output_file, index=False)\n",
    "    print(f\"Prepared dataset saved to: '{output_file}'\")\n",
    "\n",
    "    print(\"\\nUnique categories found in the prepared dataset (after mapping):\")\n",
    "    unique_categories_after_mapping = df_cleaned_for_chatbot['category'].unique()\n",
    "    print(unique_categories_after_mapping)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{input_file}' was not found. Please ensure it's uploaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during preparation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbbf8936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Label Encoder Information ---\n",
      "Type of Label Encoder: <class 'sklearn.preprocessing._label.LabelEncoder'>\n",
      "Classes known by the Label Encoder (these should be your category NAMES):\n",
      "['depression & anxiety' 'personality & behaviour' 'stress & coping'\n",
      " 'trauma & ptsd']\n",
      "---------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\AI ML Models\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-05-30 20:59:24 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading responses from CSV: Final_Chatbot_Dataset.csv ---\n",
      "Successfully loaded 3383 entries from 'Final_Chatbot_Dataset.csv'.\n",
      "Categories loaded from CSV: ['depression & anxiety', 'personality & behaviour', 'trauma & ptsd', 'stress & coping']\n",
      "\n",
      "--- Response Bank Categories (from your chosen source) ---\n",
      "These are the actual string categories available to the chatbot:\n",
      "['depression & anxiety', 'personality & behaviour', 'trauma & ptsd', 'stress & coping']\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 428kB [00:00, 5.45MB/s]                    \n",
      "2025-05-30 20:59:24 INFO: Downloaded file to C:\\Users\\soham\\stanza_resources\\resources.json\n",
      "2025-05-30 20:59:25 INFO: Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "2025-05-30 20:59:25 INFO: Using device: cpu\n",
      "2025-05-30 20:59:25 INFO: Loading: tokenize\n",
      "2025-05-30 20:59:25 INFO: Loading: mwt\n",
      "2025-05-30 20:59:25 INFO: Loading: pos\n",
      "2025-05-30 20:59:27 INFO: Loading: lemma\n",
      "2025-05-30 20:59:28 INFO: Loading: constituency\n",
      "2025-05-30 20:59:28 INFO: Loading: depparse\n",
      "2025-05-30 20:59:29 INFO: Loading: sentiment\n",
      "2025-05-30 20:59:29 INFO: Loading: ner\n",
      "2025-05-30 20:59:31 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot initialized. Type 'quit' to exit.\n",
      "\n",
      "--- Current Prediction Debugging ---\n",
      "Input for embedding: 'Hello '\n",
      "Model raw prediction (softmax output): [0.32846504 0.07460541 0.09264397 0.5042856 ]\n",
      "Predicted index from model output: 3\n",
      "Predicted topic from label_encoder: 'trauma & ptsd' (Type: <class 'numpy.str_'>)\n",
      "Normalized topic for lookup: 'trauma & ptsd' (Type: <class 'str'>)\n",
      "Is 'trauma & ptsd' found in response_bank keys? True\n",
      "------------------------------------\n",
      "\n",
      "Bot: üìò Detected Topic: trauma & ptsd\n",
      "üí¨ Suggested Response: I am so sorry that this happened to you and am so glad that you were able to get away. ¬†Your body is yours and yours alone. ¬†I would highly recommend that you find a therapist who specializes in treating trauma in order to help you to heal from your ordeal.\n",
      "\n",
      "Exiting chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "import stanza\n",
    "import pandas as pd\n",
    "\n",
    "# === Custom Attention Layer (for model loading) ===\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"glorot_uniform\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * inputs\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector\n",
    "\n",
    "# === Load Model and Encoder ===\n",
    "try:\n",
    "    # Ensure the custom AttentionLayer is passed when loading the model\n",
    "    model = load_model(\"roberta_bilstm_attention_final.keras\", custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "    with open(\"mood_encoder.pkl\", \"rb\") as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "\n",
    "    print(\"\\n--- Label Encoder Information ---\")\n",
    "    print(f\"Type of Label Encoder: {type(label_encoder)}\")\n",
    "    print(f\"Classes known by the Label Encoder (these should be your category NAMES):\")\n",
    "    try:\n",
    "        print(label_encoder.classes_)\n",
    "    except AttributeError:\n",
    "        print(\"    (Could not access .classes_ attribute. Is it a scikit-learn LabelEncoder?)\")\n",
    "    print(\"---------------------------------\\n\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading model or encoder: {e}. Make sure 'roberta_bilstm_attention_final.keras' and 'mood_encoder.pkl' are in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# === Load RoBERTa ===\n",
    "# Check if CUDA (GPU) is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_model = RobertaModel.from_pretrained(\"roberta-base\").to(device)\n",
    "roberta_model.eval() # Set model to evaluation mode\n",
    "\n",
    "# === Function to Embed Text ===\n",
    "def embed_text(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer([text], padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    # Move tensors to the appropriate device (CPU/GPU)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        outputs = roberta_model(**inputs)\n",
    "    pooled = outputs.pooler_output # Get the pooled output for sentence representation\n",
    "    return pooled.cpu().numpy().reshape(1, 1, -1).astype(\"float32\")\n",
    "\n",
    "# === Context Memory ===\n",
    "class ContextMemory:\n",
    "    def __init__(self, max_len=3):\n",
    "        self.window = []\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def add(self, user_input):\n",
    "        self.window.append(user_input)\n",
    "        if len(self.window) > self.max_len:\n",
    "            self.window.pop(0) # Remove oldest input if window exceeds max_len\n",
    "\n",
    "    def get_context(self):\n",
    "        return \" \".join(self.window)\n",
    "\n",
    "context_memory = ContextMemory()\n",
    "\n",
    "# === Load Response Bank from Cleaned CSV ===\n",
    "def load_responses_from_cleaned_csv(file_path=\"Final_Chatbot_Dataset.csv\"): # <<< CORRECTED FILE PATH HERE\n",
    "    \"\"\"\n",
    "    Loads responses from the prepared CSV file ('Final_Chatbot_Dataset.csv').\n",
    "    It expects 'Context', 'Response', and 'category' columns.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Loading responses from CSV: {file_path} ---\")\n",
    "    topic_to_responses = {}\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check for required columns\n",
    "        required_columns = ['Context', 'Response', 'category']\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            print(f\"Error: CSV file '{file_path}' must contain all of {required_columns}.\")\n",
    "            return {}\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            category = str(row['category']).strip()\n",
    "            response = str(row['Response']).strip()\n",
    "\n",
    "            # Skip rows if category or response is empty after stripping\n",
    "            if not category or not response:\n",
    "                continue\n",
    "\n",
    "            # In this updated version, the 'category' column in CSV is already mapped\n",
    "            # to match the LabelEncoder's classes, so no further splitting needed here.\n",
    "            primary_category = category\n",
    "\n",
    "            if primary_category not in topic_to_responses:\n",
    "                topic_to_responses[primary_category] = []\n",
    "            topic_to_responses[primary_category].append(response)\n",
    "\n",
    "        print(f\"Successfully loaded {len(df)} entries from '{file_path}'.\")\n",
    "        print(f\"Categories loaded from CSV: {list(topic_to_responses.keys())}\")\n",
    "        return topic_to_responses\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Cleaned CSV response bank file '{file_path}' not found. Ensure it's in the correct directory.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred loading CSV response bank from '{file_path}': {e}.\")\n",
    "        return {}\n",
    "\n",
    "# === Load Response Bank from JSON (Original Function - for reference/fallback) ===\n",
    "# You can uncomment the line below and comment the CSV loading line if you prefer to use JSON\n",
    "def load_responses_from_json(file_path=\"cleaned_questions.json\"):\n",
    "    \"\"\"\n",
    "    Loads responses from a JSON file in the format { \"category\": \"...\", \"questions\": [...] }\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        topic_to_responses = {}\n",
    "        for entry in data:\n",
    "            category = entry.get(\"category\", \"Unknown\")\n",
    "            questions = []\n",
    "            for section in entry.get(\"questions\", []):\n",
    "                # Assuming 'questions' here are actually the responses for the category\n",
    "                questions += section.get(\"questions\", [])\n",
    "            topic_to_responses[category] = questions\n",
    "        return topic_to_responses\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Response bank JSON file '{file_path}' not found. Using an empty response bank.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Could not decode JSON from '{file_path}'. Check file format.\")\n",
    "        return {}\n",
    "\n",
    "# === CHOOSE YOUR RESPONSE BANK SOURCE HERE ===\n",
    "# The chatbot is now configured to use the cleaned CSV by default.\n",
    "response_bank = load_responses_from_cleaned_csv(file_path=\"Final_Chatbot_Dataset.csv\")\n",
    "\n",
    "# If you wish to switch back to using the JSON file,\n",
    "# comment the line above and uncomment the line below:\n",
    "# response_bank = load_responses_from_json(file_path=\"cleaned_questions.json\")\n",
    "\n",
    "# --- DIAGNOSTIC PRINT ---\n",
    "print(\"\\n--- Response Bank Categories (from your chosen source) ---\")\n",
    "print(\"These are the actual string categories available to the chatbot:\")\n",
    "print(list(response_bank.keys()))\n",
    "print(\"----------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "# === NLP Enrichment with Stanza ===\n",
    "try:\n",
    "    # stanza.download('en') # Uncomment this line if you're running for the first time\n",
    "    # and Stanza models are not yet downloaded. This may take some time.\n",
    "    nlp = stanza.Pipeline('en')\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Stanza: {e}. NLP enrichment might not work.\")\n",
    "    nlp = None\n",
    "\n",
    "def enrich_with_nlp(text):\n",
    "    \"\"\"\n",
    "    Applies Stanza NLP to extract entities from the text.\n",
    "    \"\"\"\n",
    "    if nlp:\n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            entities = [(ent.text, ent.type) for sentence in doc.sentences for ent in sentence.ents]\n",
    "            if entities:\n",
    "                return f\"üß† NLP Insight ‚Äî Entities: {entities}\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error during Stanza NLP enrichment: {e}\")\n",
    "            return \"\"\n",
    "    return \"\"\n",
    "\n",
    "# === Select a Response ===\n",
    "def select_response(predicted_topic, response_bank):\n",
    "    \"\"\"\n",
    "    Selects a random response from the response_bank for the predicted topic.\n",
    "    \"\"\"\n",
    "    responses = response_bank.get(predicted_topic, [])\n",
    "    if responses:\n",
    "        return random.choice(responses)\n",
    "    # Fallback if no specific responses are found for the topic\n",
    "    return \"I'm here to support you. Can you share more about how you're feeling?\"\n",
    "\n",
    "# === Final Chatbot Function ===\n",
    "def chatbot_reply(user_input):\n",
    "    \"\"\"\n",
    "    Main function to generate chatbot's reply.\n",
    "    \"\"\"\n",
    "    # Store input in context memory\n",
    "    context_memory.add(user_input)\n",
    "    context = context_memory.get_context()\n",
    "\n",
    "    # Embed context and predict mood/topic\n",
    "    embedding = embed_text(context)\n",
    "    prediction = model.predict(embedding, verbose=0)[0]\n",
    "    predicted_index = np.argmax(prediction)\n",
    "    raw_predicted_topic = label_encoder.inverse_transform([predicted_index])[0]\n",
    "\n",
    "    # Normalize the predicted topic to match response_bank keys\n",
    "    predicted_topic_normalized = None\n",
    "    for key in response_bank.keys():\n",
    "        if key.lower() == str(raw_predicted_topic).lower():\n",
    "            predicted_topic_normalized = key # Use the exact key from response_bank\n",
    "            break\n",
    "\n",
    "    if predicted_topic_normalized is None:\n",
    "        predicted_topic_normalized = str(raw_predicted_topic) # Fallback to raw string\n",
    "        print(f\"Warning: Could not find a normalized key in response_bank for: '{raw_predicted_topic}'. This might lead to generic responses.\")\n",
    "\n",
    "\n",
    "    # --- DIAGNOSTIC PRINT FOR EACH PREDICTION ---\n",
    "    print(\"\\n--- Current Prediction Debugging ---\")\n",
    "    print(f\"Input for embedding: '{context}'\")\n",
    "    print(f\"Model raw prediction (softmax output): {prediction}\")\n",
    "    print(f\"Predicted index from model output: {predicted_index}\")\n",
    "    print(f\"Predicted topic from label_encoder: '{raw_predicted_topic}' (Type: {type(raw_predicted_topic)})\")\n",
    "    print(f\"Normalized topic for lookup: '{predicted_topic_normalized}' (Type: {type(predicted_topic_normalized)})\")\n",
    "    print(f\"Is '{predicted_topic_normalized}' found in response_bank keys? {predicted_topic_normalized in response_bank.keys()}\")\n",
    "    if predicted_topic_normalized not in response_bank.keys():\n",
    "        print(\"     -> WARNING: Predicted topic does NOT match any category in your response bank.\")\n",
    "        print(\"     -> This is likely why you are getting the generic fallback response.\")\n",
    "    print(\"------------------------------------\\n\")\n",
    "\n",
    "    # Get response + NLP insights using the normalized topic\n",
    "    response = select_response(predicted_topic_normalized, response_bank)\n",
    "    nlp_info = enrich_with_nlp(user_input)\n",
    "\n",
    "    return f\"üìò Detected Topic: {predicted_topic_normalized}\\nüí¨ Suggested Response: {response}\\n{nlp_info}\"\n",
    "\n",
    "# === Interactive Chat Loop ===\n",
    "print(\"Chatbot initialized. Type 'quit' to exit.\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        print(\"Exiting chatbot. Goodbye!\")\n",
    "        break\n",
    "\n",
    "    reply = chatbot_reply(user_input)\n",
    "    print(\"Bot:\", reply)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
