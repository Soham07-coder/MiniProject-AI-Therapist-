{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc732b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Code is from Collab , it is the code that has given better result and added mood keywords into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be62a87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tensorflow\n",
      "Version: 2.19.0\n",
      "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
      "Home-page: https://www.tensorflow.org/\n",
      "Author: Google Inc.\n",
      "Author-email: packages@tensorflow.org\n",
      "License: Apache 2.0\n",
      "Location: e:\\ai ml models\\.venv\\lib\\site-packages\n",
      "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n",
      "Required-by: \n",
      "---\n",
      "Name: keras\n",
      "Version: 3.9.2\n",
      "Summary: Multi-backend Keras\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Keras team <keras-users@googlegroups.com>\n",
      "License: Apache License 2.0\n",
      "Location: e:\\ai ml models\\.venv\\lib\\site-packages\n",
      "Requires: absl-py, h5py, ml-dtypes, namex, numpy, optree, packaging, rich\n",
      "Required-by: keras-tuner, tensorflow\n",
      "---\n",
      "Name: scikit-learn\n",
      "Version: 1.6.1\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: https://scikit-learn.org\n",
      "Author: \n",
      "Author-email: \n",
      "License: BSD 3-Clause License\n",
      "         \n",
      "         Copyright (c) 2007-2024 The scikit-learn developers.\n",
      "         All rights reserved.\n",
      "         \n",
      "         Redistribution and use in source and binary forms, with or without\n",
      "         modification, are permitted provided that the following conditions are met:\n",
      "         \n",
      "         * Redistributions of source code must retain the above copyright notice, this\n",
      "           list of conditions and the following disclaimer.\n",
      "         \n",
      "         * Redistributions in binary form must reproduce the above copyright notice,\n",
      "           this list of conditions and the following disclaimer in the documentation\n",
      "           and/or other materials provided with the distribution.\n",
      "         \n",
      "         * Neither the name of the copyright holder nor the names of its\n",
      "           contributors may be used to endorse or promote products derived from\n",
      "           this software without specific prior written permission.\n",
      "         \n",
      "         THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
      "         AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
      "         IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
      "         DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
      "         FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
      "         DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
      "         SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
      "         CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
      "         OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
      "         OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
      "         \n",
      "         ----\n",
      "         \n",
      "         This binary distribution of scikit-learn also bundles the following software:\n",
      "         \n",
      "         ----\n",
      "         \n",
      "         Name: Microsoft Visual C++ Runtime Files\n",
      "         Files: sklearn\\.libs\\*.dll\n",
      "         Availability: https://learn.microsoft.com/en-us/visualstudio/releases/2015/2015-redistribution-vs\n",
      "         \n",
      "         Subject to the License Terms for the software, you may copy and distribute with your\n",
      "         program any of the files within the followng folder and its subfolders except as noted\n",
      "         below. You may not modify these files.\n",
      "         \n",
      "         C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\n",
      "         \n",
      "         You may not distribute the contents of the following folders:\n",
      "         \n",
      "         C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\\debug_nonredist\n",
      "         C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\redist\\onecore\\debug_nonredist\n",
      "         \n",
      "         Subject to the License Terms for the software, you may copy and distribute the following\n",
      "         files with your program in your program’s application local folder or by deploying them\n",
      "         into the Global Assembly Cache (GAC):\n",
      "         \n",
      "         VC\\atlmfc\\lib\\mfcmifc80.dll\n",
      "         VC\\atlmfc\\lib\\amd64\\mfcmifc80.dll\n",
      "         \n",
      "Location: e:\\ai ml models\\.venv\\lib\\site-packages\n",
      "Requires: joblib, numpy, scipy, threadpoolctl\n",
      "Required-by: imbalanced-learn, sentence-transformers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show tensorflow keras scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d1cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import load_model\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import Sequential , Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, LayerNormalization, Add, MultiHeadAttention,\n",
    "    Bidirectional, LSTM, BatchNormalization, Layer\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam,AdamW\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# Ensure NLTK stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# ---- Step 1: Load Dataset ----\n",
    "with open(\"combined_dataset.json\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "parsed_data = [json.loads(line) for line in lines if line.strip()]\n",
    "\n",
    "# Extract Context (X) and Response (Y)\n",
    "X_texts = [sample[\"Context\"] for sample in parsed_data]\n",
    "Y_labels = [sample[\"Response\"] for sample in parsed_data]\n",
    "\n",
    "label_contexts = defaultdict(list)\n",
    "for context, label in zip(X_texts, Y_labels):\n",
    "    label_contexts[label].append(context)\n",
    "\n",
    "# Extract top keywords for each mood label\n",
    "mood_keywords = {}\n",
    "\n",
    "for label, texts in label_contexts.items():\n",
    "    # Filter out empty or whitespace-only texts\n",
    "    clean_texts = [text.strip() for text in texts if text.strip()]\n",
    "\n",
    "    # Skip if no valid texts\n",
    "    if not clean_texts:\n",
    "        print(f\"⚠️ Skipping label '{label}' due to empty or invalid texts.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        vectorizer = CountVectorizer(stop_words=\"english\", max_features=15)\n",
    "        X_vec = vectorizer.fit_transform(clean_texts)\n",
    "\n",
    "        # Skip if no valid vocabulary extracted\n",
    "        if not vectorizer.get_feature_names_out().size:\n",
    "            print(f\"⚠️ Skipping label '{label}' due to only stop words.\")\n",
    "            continue\n",
    "\n",
    "        keywords = vectorizer.get_feature_names_out()\n",
    "        mood_keywords[label] = list(keywords)\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"⚠️ Skipping label '{label}' due to error: {e}\")\n",
    "\n",
    "# Save to JSON file\n",
    "with open(\"mood_keywords.json\", \"w\") as f:\n",
    "    json.dump(mood_keywords, f, indent=4)\n",
    "\n",
    "print(\"✅ Saved mood-based keywords to mood_keywords.json\")\n",
    "\n",
    "# ---- Step 2: Load Sentence-BERT Model for Better Embeddings ----\n",
    "sbert_model = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "\n",
    "# Load or compute embeddings\n",
    "try:\n",
    "    X_emb = np.load(\"x_embeddings_sbert.npy\")\n",
    "    Y_encoded = np.load(\"y_encoded.npy\")\n",
    "    with open(\"response_encoder.pkl\", \"rb\") as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    print(\"✅ Loaded saved embeddings and encoder.\")\n",
    "except FileNotFoundError:\n",
    "    X_emb = sbert_model.encode(X_texts)  # Get Sentence-BERT embeddings\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    Y_encoded = label_encoder.fit_transform(Y_labels)\n",
    "\n",
    "    np.save(\"x_embeddings_sbert.npy\", X_emb)\n",
    "    np.save(\"y_encoded.npy\", Y_encoded)\n",
    "    with open(\"response_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    print(\"✅ Computed and saved embeddings and encoder.\")\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "Y_encoded = to_categorical(Y_encoded, num_classes=num_classes)  # Convert to one-hot encoding\n",
    "\n",
    "# Count instances per class\n",
    "class_counts = Counter(np.argmax(Y_encoded, axis=1))\n",
    "print(f\"📊 Original Class Distribution: {class_counts}\")\n",
    "\n",
    "# **Remove classes with only 1 sample**\n",
    "valid_classes = {cls for cls, count in class_counts.items() if count > 1}\n",
    "valid_indices = [i for i, label in enumerate(np.argmax(Y_encoded, axis=1)) if label in valid_classes]\n",
    "\n",
    "X_filtered = X_emb[valid_indices]\n",
    "Y_filtered = Y_encoded[valid_indices]\n",
    "\n",
    "# **Recalculate class distribution**\n",
    "filtered_class_counts = Counter(np.argmax(Y_filtered, axis=1))\n",
    "print(f\"📊 Filtered Class Distribution: {filtered_class_counts}\")\n",
    "\n",
    "# Ensure no class has fewer than 2 samples\n",
    "min_samples_per_class = min(filtered_class_counts.values())\n",
    "\n",
    "if min_samples_per_class < 2:\n",
    "    print(\"⚠️ Some classes still have only 1 sample after filtering. SMOTETomek will be skipped.\")\n",
    "    X_resampled, Y_resampled = X_filtered, Y_filtered  # Use filtered dataset\n",
    "else:\n",
    "    # Dynamically set k_neighbors based on min_samples_per_class\n",
    "    smote_k = min(5, min_samples_per_class - 1)  # Ensure valid k_neighbors\n",
    "    smote_k = max(smote_k, 1)  # Ensure at least 1 neighbor\n",
    "\n",
    "    print(f\"✅ Applying SMOTE with k_neighbors={smote_k} for class balancing...\")\n",
    "\n",
    "    smote = SMOTE(k_neighbors=smote_k, random_state=42)\n",
    "    X_smote, Y_smote = smote.fit_resample(X_filtered, np.argmax(Y_filtered, axis=1))\n",
    "\n",
    "    print(\"✅ Applying TomekLinks for noise reduction...\")\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_resampled, Y_resampled = smote_tomek.fit_resample(X_smote, Y_smote)\n",
    "\n",
    "    # Convert back to one-hot encoding\n",
    "    Y_resampled = to_categorical(Y_resampled, num_classes=num_classes)\n",
    "    print(f\"📊 Class Distribution After SMOTETomek: {Counter(np.argmax(Y_resampled, axis=1))}\")\n",
    "\n",
    "# ✅ Splitting into train-test sets\n",
    "num_samples = len(X_resampled)\n",
    "\n",
    "# Ensure test size is valid\n",
    "test_size = min(int(0.2 * num_samples), num_samples - num_classes)\n",
    "test_size = max(test_size, num_classes)  # Ensure at least `num_classes` samples\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_resampled, Y_resampled, test_size=test_size, random_state=42, stratify=np.argmax(Y_resampled, axis=1)\n",
    ")\n",
    "\n",
    "print(f\"✅ Training samples: {len(X_train)}, Testing samples: {len(X_test)}\")\n",
    "\n",
    "# ---- Step 5: Reshape Input for LSTM ----\n",
    "X_train = np.expand_dims(X_train, axis=1)  # Add a time step dimension\n",
    "X_test = np.expand_dims(X_test, axis=1)\n",
    "\n",
    "Y_train = np.argmax(Y_train, axis=1)  # Convert labels to integers\n",
    "Y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# ---- Step 6: Compute Class Weights ----\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(Y_train),\n",
    "    y=Y_train\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b3157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Step 7: Load or Define the Model ----\n",
    "# ---- Check for Existing Model ----\n",
    "model_path = \"optimized_lstm_model.h5\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"✅ Loading existing model for further training...\")\n",
    "    model = load_model(model_path, custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "\n",
    "    # ✅ Reset the optimizer\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=0.0013,\n",
    "        decay_steps=1150,\n",
    "        decay_rate=0.97\n",
    "    )\n",
    "    optimizer = AdamW(learning_rate=3e-4)  # Create a new optimizer\n",
    "\n",
    "    # ✅ Recompile the model with the new optimizer\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=optimizer,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "else:\n",
    "    print(\"🚀 No previous model found. Training from scratch...\")\n",
    "\n",
    "    # ✅ Define learning rate scheduler here\n",
    "    lr_schedule = ExponentialDecay(\n",
    "        initial_learning_rate=0.001,\n",
    "        decay_steps=1200,\n",
    "        decay_rate=0.9\n",
    "    )\n",
    "# Transformer Block\n",
    "    def transformer_block(inputs, num_heads=4, ff_dim=256, dropout_rate=0.4):\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n",
    "        attention_output = Dropout(dropout_rate)(attention_output)\n",
    "        out1 = LayerNormalization(epsilon=1e-6)(Add()([inputs, attention_output]))\n",
    "\n",
    "        ff_output = Dense(ff_dim, activation=\"relu\")(out1)\n",
    "        ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "        ff_output = Dropout(dropout_rate)(ff_output)\n",
    "        return LayerNormalization(epsilon=1e-6)(Add()([out1, ff_output]))\n",
    "\n",
    "    # Input shape\n",
    "    input_shape = (1, X_train.shape[-1])\n",
    "    model_input = Input(shape=input_shape)\n",
    "\n",
    "    # Transformer Block (expand to 3D)\n",
    "    x = transformer_block(model_input)\n",
    "\n",
    "    # BiLSTM Layers\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Attention Layer\n",
    "    x = AttentionLayer()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=model_input, outputs=output)\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=Adam(learning_rate=lr_schedule,clipnorm=1.0),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "# ---- Train the Model ----\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=9, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, Y_train,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    epochs=60,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# ---- Save the current model temporarily for comparison ----\n",
    "model.save(\"new_lstm_model.h5\")\n",
    "print(\"✅ New model saved temporarily as 'new_lstm_model.h5'\")\n",
    "\n",
    "# ---- Evaluate the Model ----\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "print(f\"✅ Optimized Model Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# ---- Step 9: Save the Best Model ----\n",
    "if os.path.exists(\"optimized_lstm_model.h5\"):\n",
    "    prev_model = load_model(\"optimized_lstm_model.h5\", custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "\n",
    "    _, prev_model_acc = prev_model.evaluate(X_test, Y_test, verbose=0)\n",
    "else:\n",
    "    prev_model_acc = 0.0\n",
    "\n",
    "# ---- Evaluate the new model ----\n",
    "_, new_model_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "# ---- Compare and replace if improved ----\n",
    "if new_model_acc > prev_model_acc:\n",
    "    model.save(\"optimized_lstm_model.h5\")\n",
    "    print(f\"✅ New model outperformed the previous one. Accuracy improved from {prev_model_acc*100:.2f}% → {new_model_acc*100:.2f}%\")\n",
    "else:\n",
    "    print(f\"⚠️ New model did NOT improve accuracy ({new_model_acc*100:.2f}%). Keeping previous model ({prev_model_acc*100:.2f}%).\")\n",
    "\n",
    "\n",
    "# ---- Step 10: Plot Training vs Validation Accuracy ----\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\", color='blue')\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\", color='orange')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Transformer + BiLSTM + Attention Training vs Validation Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# ---- Custom Attention Layer (Used in Model) ----\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(1,),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x * a\n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "# ---- Step 1: Load SBERT model for embeddings ----\n",
    "sbert_model = SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "\n",
    "# ---- Step 2: Load the trained mood classification model and label encoder ----\n",
    "mood_model = load_model(\"optimized_lstm_model.h5\", custom_objects={\"AttentionLayer\": AttentionLayer})\n",
    "\n",
    "with open(\"response_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# ---- Step 3: Load extracted questions JSON ----\n",
    "with open(\"extracted_questions.json\", \"r\") as f:\n",
    "    questions_by_mood = json.load(f)\n",
    "\n",
    "# ---- Step 4: Function to classify mood ----\n",
    "def classify_mood(user_input):\n",
    "    embedding = sbert_model.encode([user_input])\n",
    "    input_tensor = np.expand_dims(embedding, axis=1)  # Shape: (1, 1, embedding_dim)\n",
    "    prediction = mood_model.predict(input_tensor)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    try:\n",
    "        return label_encoder.inverse_transform([predicted_class])[0]\n",
    "    except:\n",
    "        return \"general\"\n",
    "\n",
    "# ---- Step 5: Extract all questions under the mood ----\n",
    "def get_all_questions_for_mood(mood, json_data):\n",
    "    mood = mood.capitalize()\n",
    "    if mood not in json_data:\n",
    "        return []\n",
    "    \n",
    "    mood_entries = json_data[mood]\n",
    "    all_questions = []\n",
    "\n",
    "    for entry in mood_entries:\n",
    "        for section_name, section_data in entry.items():\n",
    "            questions = section_data.get(\"questions\", [])\n",
    "            all_questions.extend(questions)\n",
    "    \n",
    "    return all_questions\n",
    "\n",
    "# ---- Step 6: Find best-matching question ----\n",
    "def fetch_best_matching_question(user_input):\n",
    "    detected_mood = classify_mood(user_input)\n",
    "    print(f\"🧠 Detected Mood: {detected_mood}\")\n",
    "\n",
    "    all_questions = get_all_questions_for_mood(detected_mood, questions_by_mood)\n",
    "\n",
    "    if not all_questions:\n",
    "        return \"❌ Sorry, I couldn't find questions for your mood.\"\n",
    "\n",
    "    # Embed input and all questions\n",
    "    input_embedding = sbert_model.encode([user_input])[0]\n",
    "    question_embeddings = sbert_model.encode(all_questions)\n",
    "\n",
    "    # Cosine similarity to find best match\n",
    "    similarities = cosine_similarity([input_embedding], question_embeddings)[0]\n",
    "    best_index = np.argmax(similarities)\n",
    "    return all_questions[best_index]\n",
    "\n",
    "# ---- Step 7: Run chatbot interaction loop ----\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"💬 Chatbot ready! Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "            print(\"Chatbot: Take care. Goodbye! 👋\")\n",
    "            break\n",
    "        response = fetch_best_matching_question(user_input)\n",
    "        print(\"Chatbot:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
